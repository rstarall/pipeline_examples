"""
title: Bocha Search LangGraph Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: A 2-stage LangGraph pipeline: 1) Web search using Bocha API, 2) AI-enhanced Q&A with search context
requirements: requests, langgraph, langchain-openai, pydantic
"""

import os
import json
import requests
from typing import List, Union, Generator, Iterator, Annotated, Literal
from typing_extensions import TypedDict
from pydantic import BaseModel

from fastapi import FastAPI
from fastapi.responses import StreamingResponse

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langgraph.config import get_stream_writer
from schemas import OpenAIChatMessage


class Pipeline:
    class Valves(BaseModel):
        # åšæŸ¥æœç´¢APIé…ç½®
        BOCHA_API_KEY: str
        BOCHA_BASE_URL: str
        BOCHA_SEARCH_COUNT: int
        BOCHA_FRESHNESS: str
        BOCHA_ENABLE_SUMMARY: bool
        BOCHA_TIMEOUT: int
        
        # å¤§æ¨¡å‹é…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool

    def __init__(self):
        self.name = "Bocha Search LangGraph Pipeline"
        
        self.valves = self.Valves(
            **{
                # åšæŸ¥æœç´¢é…ç½®
                "BOCHA_API_KEY": os.getenv("BOCHA_API_KEY", ""),
                "BOCHA_BASE_URL": os.getenv("BOCHA_BASE_URL", "https://api.bochaai.com/v1"),
                "BOCHA_SEARCH_COUNT": int(os.getenv("BOCHA_SEARCH_COUNT", "8")),
                "BOCHA_FRESHNESS": os.getenv("BOCHA_FRESHNESS", "oneYear"),  # oneDay, oneWeek, oneMonth, oneYear, all
                "BOCHA_ENABLE_SUMMARY": os.getenv("BOCHA_ENABLE_SUMMARY", "true").lower() == "true",
                "BOCHA_TIMEOUT": int(os.getenv("BOCHA_TIMEOUT", "30")),
                
                # å¤§æ¨¡å‹é…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
            }
        )
        
        # åˆå§‹åŒ–LangGraph
        self._init_langgraph()

    async def on_startup(self):
        print(f"Bocha Search LangGraph Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.BOCHA_API_KEY:
            print("âŒ ç¼ºå°‘åšæŸ¥APIå¯†é’¥ï¼Œè¯·è®¾ç½®BOCHA_API_KEYç¯å¢ƒå˜é‡")
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            
        # æµ‹è¯•åšæŸ¥APIè¿æ¥
        try:
            test_response = self._search_bocha("test query", count=1)
            if "error" not in test_response:
                print("âœ… åšæŸ¥æœç´¢APIè¿æ¥æˆåŠŸ")
            else:
                print(f"âš ï¸ åšæŸ¥æœç´¢APIæµ‹è¯•å¤±è´¥: {test_response['error']}")
        except Exception as e:
            print(f"âŒ åšæŸ¥æœç´¢APIè¿æ¥å¤±è´¥: {e}")

    async def on_shutdown(self):
        print(f"Bocha Search LangGraph Pipelineå…³é—­: {__name__}")

    def _search_bocha(self, query: str, count: int = None, freshness: str = None) -> dict:
        """è°ƒç”¨åšæŸ¥Web Search API"""
        url = f"{self.valves.BOCHA_BASE_URL}/web-search"

        # å‚æ•°éªŒè¯
        search_count = count or self.valves.BOCHA_SEARCH_COUNT
        if search_count < 1 or search_count > 20:
            search_count = min(max(search_count, 1), 20)  # é™åˆ¶åœ¨1-20ä¹‹é—´

        valid_freshness = ["oneDay", "oneWeek", "oneMonth", "oneYear", "all"]
        search_freshness = freshness or self.valves.BOCHA_FRESHNESS
        if search_freshness not in valid_freshness:
            search_freshness = "oneYear"  # é»˜è®¤å€¼

        payload = {
            "query": query.strip(),
            "count": search_count,
            "freshness": search_freshness,
            "summary": self.valves.BOCHA_ENABLE_SUMMARY
        }

        headers = {
            "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
            "Content-Type": "application/json"
        }

        try:
            if self.valves.DEBUG_MODE:
                print(f"åšæŸ¥æœç´¢è¯·æ±‚: {payload}")

            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.BOCHA_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()

            if self.valves.DEBUG_MODE:
                print(f"åšæŸ¥æœç´¢å“åº”çŠ¶æ€: {response.status_code}")

            return result
        except requests.exceptions.Timeout:
            return {"error": "åšæŸ¥æœç´¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"}
        except requests.exceptions.HTTPError as e:
            return {"error": f"åšæŸ¥æœç´¢HTTPé”™è¯¯: {e.response.status_code}"}
        except requests.exceptions.RequestException as e:
            return {"error": f"åšæŸ¥æœç´¢ç½‘ç»œé”™è¯¯: {str(e)}"}
        except Exception as e:
            return {"error": f"åšæŸ¥æœç´¢æœªçŸ¥é”™è¯¯: {str(e)}"}

    def _format_search_results(self, search_response: dict) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºæ–‡æœ¬"""
        if "error" in search_response:
            return f"æœç´¢é”™è¯¯: {search_response['error']}"

        # æ£€æŸ¥å“åº”ç»“æ„
        if not isinstance(search_response, dict):
            return "æœç´¢å“åº”æ ¼å¼é”™è¯¯"

        web_pages = search_response.get("webPages", {}).get("value", [])
        if not web_pages:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯"

        formatted_results = []
        total_results = search_response.get("webPages", {}).get("totalEstimatedMatches", 0)

        # æ·»åŠ æœç´¢ç»Ÿè®¡ä¿¡æ¯
        if total_results > 0:
            formatted_results.append(f"æ‰¾åˆ°çº¦ {total_results:,} æ¡ç›¸å…³ç»“æœ\n")

        for i, page in enumerate(web_pages[:self.valves.BOCHA_SEARCH_COUNT], 1):
            title = page.get("name", "æ— æ ‡é¢˜").strip()
            url = page.get("url", "").strip()
            snippet = page.get("snippet", "").strip()
            summary = page.get("summary", "").strip()
            site_name = page.get("siteName", "").strip()
            date_published = page.get("datePublished", "").strip()

            result_text = f"**{i}. {title}**\n"

            if site_name:
                result_text += f"   æ¥æº: {site_name}\n"

            if date_published:
                result_text += f"   å‘å¸ƒæ—¶é—´: {date_published}\n"

            if url:
                result_text += f"   é“¾æ¥: {url}\n"

            # ä¼˜å…ˆæ˜¾ç¤ºè¯¦ç»†æ‘˜è¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºæ™®é€šæ‘˜è¦
            content_to_show = summary if summary else snippet
            if content_to_show:
                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                if len(content_to_show) > 300:
                    content_to_show = content_to_show[:300] + "..."
                result_text += f"   å†…å®¹: {content_to_show}\n"

            formatted_results.append(result_text)

        return "\n".join(formatted_results)

    def _generate_custom_stream(self, type: Literal["search", "answer"], content: str):
        """ç”Ÿæˆè‡ªå®šä¹‰æµå¼è¾“å‡º"""
        if not self.valves.ENABLE_STREAMING:
            return
            
        content = f"\n{content}\n"
        custom_stream_writer = get_stream_writer()
        return custom_stream_writer({type: content})

    # LangGraphçŠ¶æ€å®šä¹‰
    class State(TypedDict):
        messages: Annotated[list, add_messages]
        query: str
        search_results: str
        final_answer: str

    def _init_langgraph(self):
        """åˆå§‹åŒ–LangGraph"""
        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model=self.valves.OPENAI_MODEL,
            api_key=self.valves.OPENAI_API_KEY,
            base_url=self.valves.OPENAI_BASE_URL
        )
        
        # å®šä¹‰å›¾
        graph_builder = StateGraph(Pipeline.State)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("search", self._search_node)
        graph_builder.add_node("answer", self._answer_node)
        
        # å®šä¹‰è¾¹
        graph_builder.add_edge(START, "search")
        graph_builder.add_edge("search", "answer")
        graph_builder.add_edge("answer", END)
        
        # ç¼–è¯‘å›¾
        self.graph = graph_builder.compile()

    def _search_node(self, state: "Pipeline.State"):
        """æœç´¢èŠ‚ç‚¹ï¼šä½¿ç”¨åšæŸ¥APIè¿›è¡Œè”ç½‘æœç´¢"""
        query = state.get("query", "").strip()

        if not query:
            error_msg = "æœç´¢æŸ¥è¯¢ä¸ºç©º"
            self._generate_custom_stream("search", f"âŒ {error_msg}")
            return {"search_results": error_msg}

        if self.valves.DEBUG_MODE:
            print(f"ğŸ” å¼€å§‹æœç´¢: {query}")

        try:
            # è°ƒç”¨åšæŸ¥æœç´¢API
            search_response = self._search_bocha(query)
            search_results = self._format_search_results(search_response)

            # æ£€æŸ¥æœç´¢æ˜¯å¦æˆåŠŸ
            if "æœç´¢é”™è¯¯" in search_results or "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ" in search_results:
                self._generate_custom_stream("search", f"âš ï¸ {search_results}")
            else:
                self._generate_custom_stream("search", f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³ä¿¡æ¯:\n{search_results}")

            return {"search_results": search_results}

        except Exception as e:
            error_msg = f"æœç´¢èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Search node error: {e}")
            self._generate_custom_stream("search", f"âŒ {error_msg}")
            return {"search_results": error_msg}

    def _answer_node(self, state: "Pipeline.State"):
        """é—®ç­”èŠ‚ç‚¹ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆå¢å¼ºå›ç­”"""
        query = state.get("query", "").strip()
        search_results = state.get("search_results", "").strip()

        if not query:
            error_msg = "ç”¨æˆ·é—®é¢˜ä¸ºç©º"
            self._generate_custom_stream("answer", f"âŒ {error_msg}")
            return {"final_answer": error_msg, "messages": []}

        if not search_results or "æœç´¢é”™è¯¯" in search_results:
            # å¦‚æœæœç´¢å¤±è´¥ï¼Œä»ç„¶å°è¯•åŸºäºé—®é¢˜æœ¬èº«å›ç­”
            fallback_prompt = f"""ç”¨æˆ·é—®é¢˜: {query}

ç”±äºæœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶è¯´æ˜è¿™æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ï¼Œå¯èƒ½ä¸åŒ…å«æœ€æ–°ä¿¡æ¯ã€‚"""

            try:
                response = self.llm.invoke([{"role": "user", "content": fallback_prompt}])
                final_answer = f"âš ï¸ æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œä»¥ä¸‹æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ï¼š\n\n{response.content}"
                self._generate_custom_stream("answer", final_answer)
                return {"final_answer": final_answer, "messages": [response]}
            except Exception as e:
                error_msg = f"é—®ç­”èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
                self._generate_custom_stream("answer", f"âŒ {error_msg}")
                return {"final_answer": error_msg, "messages": []}

        if self.valves.DEBUG_MODE:
            print(f"ğŸ’­ å¼€å§‹ç”Ÿæˆå›ç­”ï¼ŒåŸºäºæœç´¢ç»“æœ")

        try:
            # æ„å»ºå¢å¼ºæç¤º
            enhanced_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {query}

æœç´¢ç»“æœ:
{search_results}

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. åŸºäºæœç´¢ç»“æœæä¾›å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰ç”¨çš„å›ç­”
2. å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ èƒ½ç»™å‡ºçš„æœ€ä½³å»ºè®®
3. å¼•ç”¨ç›¸å…³çš„æœç´¢ç»“æœæ¥æºï¼Œå¢å¼ºå›ç­”çš„å¯ä¿¡åº¦
4. ä¿æŒå›ç­”çš„ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£
5. å¦‚æœå‘ç°æœç´¢ç»“æœä¸­æœ‰çŸ›ç›¾ä¿¡æ¯ï¼Œè¯·æŒ‡å‡ºå¹¶åˆ†æ"""

            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
            response = self.llm.invoke([{"role": "user", "content": enhanced_prompt}])
            final_answer = response.content

            # ç”Ÿæˆå›ç­”é˜¶æ®µçš„æµå¼è¾“å‡º
            self._generate_custom_stream("answer", final_answer)

            return {"final_answer": final_answer, "messages": [response]}

        except Exception as e:
            error_msg = f"é—®ç­”èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Answer node error: {e}")
            self._generate_custom_stream("answer", f"âŒ {error_msg}")
            return {"final_answer": error_msg, "messages": []}

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Union[str, Generator, Iterator]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æ–¹æ³•

        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            model_id: æ¨¡å‹IDï¼ˆåœ¨æ­¤pipelineä¸­æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼‰
            messages: æ¶ˆæ¯å†å²ï¼ˆåœ¨æ­¤pipelineä¸­æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼‰
            body: è¯·æ±‚ä½“ï¼ŒåŒ…å«æµå¼è®¾ç½®å’Œç”¨æˆ·ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²æˆ–æµå¼ç”Ÿæˆå™¨
        """
        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.valves.BOCHA_API_KEY:
            return "âŒ é”™è¯¯ï¼šç¼ºå°‘åšæŸ¥APIå¯†é’¥ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®BOCHA_API_KEY"

        if not self.valves.OPENAI_API_KEY:
            return "âŒ é”™è¯¯ï¼šç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®OPENAI_API_KEY"

        if not user_message.strip():
            return "âŒ è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹"

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if self.valves.DEBUG_MODE and "user" in body:
            print("=" * 50)
            print(f"ç”¨æˆ·: {body['user']['name']} ({body['user']['id']})")
            print(f"æŸ¥è¯¢å†…å®¹: {user_message}")
            print(f"æµå¼å“åº”: {body.get('stream', False)}")
            print(f"å¯ç”¨æµå¼: {self.valves.ENABLE_STREAMING}")
            print("=" * 50)

        # å‡†å¤‡åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [],
            "query": user_message,
            "search_results": "",
            "final_answer": ""
        }

        # æ ¹æ®æ˜¯å¦å¯ç”¨æµå¼å“åº”é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if body.get("stream", False) and self.valves.ENABLE_STREAMING:
            return self._stream_response(initial_state)
        else:
            return self._non_stream_response(initial_state)

    def _stream_response(self, initial_state: dict) -> Generator[str, None, None]:
        """æµå¼å“åº”å¤„ç†"""
        try:
            # æµå¼å¼€å§‹æ¶ˆæ¯
            yield f'data: {json.dumps({"choices": [{"delta": {}, "finish_reason": None}]})}\n\n'

            # æ‰§è¡ŒLangGraphæµå¼å¤„ç†
            for event in self.graph.stream(input=initial_state, stream_mode="custom"):
                if self.valves.DEBUG_MODE:
                    print(f"Stream event: {event}")

                search_content = event.get("search", None)
                answer_content = event.get("answer", None)

                # æœç´¢é˜¶æ®µè¾“å‡º
                if search_content:
                    search_msg = {
                        'choices': [{
                            'delta': {
                                'content': f"**ğŸ” æœç´¢é˜¶æ®µ**\n{search_content}"
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(search_msg)}\n\n"

                # å›ç­”é˜¶æ®µè¾“å‡º
                if answer_content:
                    answer_msg = {
                        'choices': [{
                            'delta': {
                                'content': f"\n**ğŸ’­ å›ç­”é˜¶æ®µ**\n{answer_content}"
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(answer_msg)}\n\n"

            # æµå¼ç»“æŸæ¶ˆæ¯
            yield f'data: {json.dumps({"choices": [{"delta": {}, "finish_reason": "stop"}]})}\n\n'

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Stream error: {e}")
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"

    def _non_stream_response(self, initial_state: dict) -> str:
        """éæµå¼å“åº”å¤„ç†"""
        try:
            # æ‰§è¡Œå®Œæ•´çš„LangGraphæµç¨‹
            final_state = self.graph.invoke(initial_state)

            search_results = final_state.get("search_results", "")
            final_answer = final_state.get("final_answer", "")

            # æ„å»ºå®Œæ•´å“åº”
            response_parts = []

            if search_results:
                response_parts.append(f"**ğŸ” æœç´¢ç»“æœ**\n{search_results}")

            if final_answer:
                response_parts.append(f"**ğŸ’­ AIå›ç­”**\n{final_answer}")

            if not response_parts:
                return "âŒ æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„æœç´¢ç»“æœæˆ–å›ç­”"

            # æ·»åŠ é…ç½®ä¿¡æ¯
            config_info = f"\n\n---\n**é…ç½®ä¿¡æ¯**\n- æœç´¢ç»“æœæ•°é‡: {self.valves.BOCHA_SEARCH_COUNT}\n- æ—¶é—´èŒƒå›´: {self.valves.BOCHA_FRESHNESS}\n- æ¨¡å‹: {self.valves.OPENAI_MODEL}"

            return "\n\n".join(response_parts) + config_info

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Non-stream error: {e}")
            return error_msg
