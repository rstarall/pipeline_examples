"""
title: LightRAG Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: A pipeline for querying LightRAG knowledge base with support for multiple query modes (local, global, hybrid, naive, mix) and conversation history
requirements: requests, sseclient-py
"""

from typing import List, Union, Generator, Iterator
import requests
import json
import os
from pydantic import BaseModel


class HistoryContextManager:
    """å†å²ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç»Ÿä¸€å¤„ç†å†å²ä¼šè¯ä¿¡æ¯"""
    
    DEFAULT_HISTORY_TURNS = 3  # é»˜è®¤3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
    
    @classmethod
    def extract_recent_context(cls, messages: List[dict], max_turns: int = None) -> str:
        """
        æå–æœ€è¿‘çš„å†å²ä¼šè¯ä¸Šä¸‹æ–‡
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_turns: æœ€å¤§è½®æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3è½®
        
        Returns:
            æ ¼å¼åŒ–çš„å†å²ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not messages or len(messages) < 2:
            return ""
            
        max_turns = max_turns or cls.DEFAULT_HISTORY_TURNS
        max_messages = max_turns * 2  # æ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
        
        recent_messages = messages[-max_messages:] if len(messages) > max_messages else messages
        context_text = ""
        
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                context_text += f"ç”¨æˆ·: {content}\n"
            elif role == "assistant":
                context_text += f"åŠ©æ‰‹: {content}\n"
                
        return context_text.strip()
    
    @classmethod
    def enhance_query_with_history(cls, user_query: str, messages: List[dict], max_turns: int = None) -> str:
        """
        ä½¿ç”¨å†å²ä¼šè¯ä¸Šä¸‹æ–‡å¢å¼ºç”¨æˆ·æŸ¥è¯¢
        
        Args:
            user_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            messages: å†å²æ¶ˆæ¯
            max_turns: æœ€å¤§å†å²è½®æ¬¡
            
        Returns:
            å¢å¼ºåçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        context_text = cls.extract_recent_context(messages, max_turns)
        
        if not context_text:
            return user_query
        
        enhanced_query = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†å²å›ç­”é—®é¢˜ï¼š

å¯¹è¯å†å²:
{context_text}

å½“å‰é—®é¢˜: {user_query}

è¯·ç»“åˆå¯¹è¯å†å²çš„ä¸Šä¸‹æ–‡ï¼Œæä¾›å‡†ç¡®ã€è¯¦ç»†ä¸”ç›¸å…³çš„å›ç­”ã€‚"""
        
        return enhanced_query


class Pipeline:
    class Valves(BaseModel):
        LIGHTRAG_BASE_URL: str
        LIGHTRAG_DEFAULT_MODE: str
        LIGHTRAG_TIMEOUT: int
        LIGHTRAG_ENABLE_STREAMING: bool
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int
        ENABLE_HISTORY_CONTEXT: bool

    def __init__(self):
        self.name = "LightRAG Pipeline"
        
        self.valves = self.Valves(
            **{
                "LIGHTRAG_BASE_URL": os.getenv("LIGHTRAG_BASE_URL", "http://localhost:9621"),
                "LIGHTRAG_DEFAULT_MODE": os.getenv("LIGHTRAG_DEFAULT_MODE", "hybrid"),
                "LIGHTRAG_TIMEOUT": int(os.getenv("LIGHTRAG_TIMEOUT", "30")),
                "LIGHTRAG_ENABLE_STREAMING": os.getenv("LIGHTRAG_ENABLE_STREAMING", "true").lower() == "true",
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
                "ENABLE_HISTORY_CONTEXT": os.getenv("ENABLE_HISTORY_CONTEXT", "true").lower() == "true",
            }
        )

    async def on_startup(self):
        print(f"LightRAG Pipelineå¯åŠ¨: {__name__}")
        # æµ‹è¯•è¿æ¥å’Œç«¯ç‚¹å¯ç”¨æ€§
        try:
            # æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹
            response = requests.get(f"{self.valves.LIGHTRAG_BASE_URL}/health", timeout=5)
            if response.status_code == 200:
                print("âœ… LightRAGæœåŠ¡è¿æ¥æˆåŠŸ")

                # æµ‹è¯•æŸ¥è¯¢ç«¯ç‚¹
                test_endpoints = [
                    ("/query", "æ ‡å‡†æŸ¥è¯¢ç«¯ç‚¹"),
                    ("/query/stream", "æµå¼æŸ¥è¯¢ç«¯ç‚¹"),
                    ("/api/chat", "Ollamaå…¼å®¹ç«¯ç‚¹")
                ]

                for endpoint, description in test_endpoints:
                    try:
                        test_response = requests.post(
                            f"{self.valves.LIGHTRAG_BASE_URL}{endpoint}",
                            json={"query": "test", "mode": "hybrid"} if endpoint != "/api/chat" else {
                                "model": "lightrag:latest",
                                "messages": [{"role": "user", "content": "test"}],
                                "stream": False
                            },
                            timeout=3
                        )
                        if test_response.status_code in [200, 422]:  # 422å¯èƒ½æ˜¯å‚æ•°éªŒè¯é”™è¯¯ï¼Œä½†ç«¯ç‚¹å­˜åœ¨
                            print(f"âœ… {description}å¯ç”¨")
                        else:
                            print(f"âš ï¸ {description}å“åº”å¼‚å¸¸: {test_response.status_code}")
                    except Exception as e:
                        print(f"âŒ {description}ä¸å¯ç”¨: {e}")

            else:
                print(f"âš ï¸ LightRAGæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°LightRAGæœåŠ¡: {e}")

    async def on_shutdown(self):
        print(f"LightRAG Pipelineå…³é—­: {__name__}")

    def _extract_query_mode(self, user_message: str) -> tuple[str, str]:
        """ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢æ¨¡å¼å’Œå®é™…æŸ¥è¯¢å†…å®¹"""
        mode_prefixes = {
            "/local": "local",
            "/global": "global", 
            "/hybrid": "hybrid",
            "/naive": "naive",
            "/mix": "mix"
        }
        
        for prefix, mode in mode_prefixes.items():
            if user_message.startswith(prefix):
                query = user_message[len(prefix):].strip()
                return mode, query
        
        return self.valves.LIGHTRAG_DEFAULT_MODE, user_message

    def _query_lightrag_standard(self, query: str, mode: str) -> dict:
        """æ ‡å‡†æŸ¥è¯¢API"""
        url = f"{self.valves.LIGHTRAG_BASE_URL}/query"
        payload = {
            "query": query,
            "mode": mode
        }
        
        headers = {"Content-Type": "application/json"}
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                headers=headers, 
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def _query_lightrag_streaming(self, query: str, mode: str) -> Generator[str, None, None]:
        """æµå¼æŸ¥è¯¢API - ä½¿ç”¨LightRAGçš„/query/streamç«¯ç‚¹"""
        url = f"{self.valves.LIGHTRAG_BASE_URL}/query/stream"

        payload = {
            "query": query,
            "mode": mode
        }

        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                stream=True,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()

            # LightRAGçš„æµå¼å“åº”æ ¼å¼æ˜¯NDJSONï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8').strip()
                    if line_text:
                        try:
                            data = json.loads(line_text)
                            # LightRAGæµå¼å“åº”æ ¼å¼: {"response": "chunk_content"}
                            if 'response' in data and data['response']:
                                chunk = data['response']
                                # è½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æµå¼å“åº”æ ¼å¼
                                yield f'data: {json.dumps({"choices": [{"delta": {"content": chunk}}]})}\n\n'
                            elif 'error' in data:
                                error_msg = data['error']
                                yield f'data: {json.dumps({"choices": [{"delta": {"content": f"é”™è¯¯: {error_msg}"}}]})}\n\n'
                        except json.JSONDecodeError:
                            # å¿½ç•¥æ— æ³•è§£æçš„è¡Œ
                            continue

        except Exception as e:
            error_msg = f"æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}"
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"

    def _query_lightrag_streaming_ollama(self, query: str, mode: str) -> Generator[str, None, None]:
        """å¤‡ç”¨æµå¼æŸ¥è¯¢API - ä½¿ç”¨LightRAGçš„/api/chatç«¯ç‚¹ï¼ˆOllamaå…¼å®¹ï¼‰"""
        url = f"{self.valves.LIGHTRAG_BASE_URL}/api/chat"

        # æ ¹æ®æ¨¡å¼å‰ç¼€æ ¼å¼åŒ–æŸ¥è¯¢
        formatted_query = f"/{mode} {query}" if mode != "hybrid" else query
        messages = [{"role": "user", "content": formatted_query}]

        payload = {
            "model": "lightrag:latest",
            "messages": messages,
            "stream": True
        }

        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                stream=True,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()

            # Ollamaå…¼å®¹APIçš„æµå¼å“åº”æ ¼å¼æ˜¯NDJSON
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8').strip()
                    if line_text:
                        try:
                            data = json.loads(line_text)
                            # Ollamaæ ¼å¼: {"message": {"content": "chunk"}, "done": false}
                            if 'message' in data and 'content' in data['message']:
                                chunk = data['message']['content']
                                if chunk:
                                    # è½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æµå¼å“åº”æ ¼å¼
                                    yield f'data: {json.dumps({"choices": [{"delta": {"content": chunk}}]})}\n\n'

                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if data.get('done', False):
                                break

                        except json.JSONDecodeError:
                            # å¿½ç•¥æ— æ³•è§£æçš„è¡Œ
                            continue

        except Exception as e:
            error_msg = f"å¤‡ç”¨æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}"
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Union[str, Generator, Iterator]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æ–¹æ³•

        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            model_id: æ¨¡å‹IDï¼ˆåœ¨æ­¤pipelineä¸­æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼‰
            messages: æ¶ˆæ¯å†å²ï¼ˆç°åœ¨ä¼šè¢«ä½¿ç”¨æ¥å¢å¼ºæŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼‰
            body: è¯·æ±‚ä½“ï¼ŒåŒ…å«æµå¼è®¾ç½®å’Œç”¨æˆ·ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²æˆ–æµå¼ç”Ÿæˆå™¨
        """
        # æå–æŸ¥è¯¢æ¨¡å¼å’Œå†…å®¹
        mode, query = self._extract_query_mode(user_message)
        
        # å¦‚æœå¯ç”¨å†å²ä¸Šä¸‹æ–‡ä¸”æœ‰å†å²æ¶ˆæ¯ï¼Œåˆ™å¢å¼ºæŸ¥è¯¢
        if self.valves.ENABLE_HISTORY_CONTEXT and messages:
            enhanced_query = HistoryContextManager.enhance_query_with_history(
                user_query=query,
                messages=messages,
                max_turns=self.valves.HISTORY_TURNS
            )
            if enhanced_query != query:  # å¦‚æœæŸ¥è¯¢è¢«å¢å¼ºäº†
                query = enhanced_query
                if body.get("user"):
                    print(f"ğŸ“œ å†å²ä¸Šä¸‹æ–‡å·²åº”ç”¨ï¼Œå¢å¼ºæŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")

        # éªŒè¯æŸ¥è¯¢å†…å®¹
        if not query.strip():
            return "è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹ã€‚"
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if "user" in body:
            print("=" * 50)
            print(f"ç”¨æˆ·: {body['user']['name']} ({body['user']['id']})")
            print(f"æŸ¥è¯¢æ¨¡å¼: {mode}")
            print(f"æŸ¥è¯¢å†…å®¹: {query}")
            print(f"æµå¼å“åº”: {body.get('stream', False)}")
            print(f"å¯ç”¨æµå¼: {self.valves.LIGHTRAG_ENABLE_STREAMING}")
            print("=" * 50)

        # æ ¹æ®æ˜¯å¦å¯ç”¨æµå¼å“åº”é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if body.get("stream", False) and self.valves.LIGHTRAG_ENABLE_STREAMING:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ä¸“ç”¨çš„æµå¼æŸ¥è¯¢ç«¯ç‚¹
            try:
                return self._query_lightrag_streaming(query, mode)
            except Exception as e:
                print(f"ä¸»æµå¼ç«¯ç‚¹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨ç«¯ç‚¹: {e}")
                # å¦‚æœä¸»ç«¯ç‚¹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Ollamaå…¼å®¹ç«¯ç‚¹
                return self._query_lightrag_streaming_ollama(query, mode)
        else:
            # éæµå¼å“åº”
            result = self._query_lightrag_standard(query, mode)
            
            if "error" in result:
                return result["error"]
            
            # è¿”å›æŸ¥è¯¢ç»“æœ
            response_content = result.get("response", "æœªè·å–åˆ°å“åº”å†…å®¹")
            
            # æ·»åŠ æ¨¡å¼ä¿¡æ¯åˆ°å“åº”ä¸­
            mode_info = f"\n\n---\n**æŸ¥è¯¢æ¨¡å¼**: {mode.upper()}\n**LightRAGæœåŠ¡**: {self.valves.LIGHTRAG_BASE_URL}"
            
            return response_content + mode_info
