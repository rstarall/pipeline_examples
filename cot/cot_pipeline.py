"""
title: Chain of Thought Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: é«˜çº§æ€ç»´é“¾æ¨ç†Pipeline - æ”¯æŒå¤šç§CoTæŠ€æœ¯ã€æµå¼è¾“å‡ºå’Œæ™ºèƒ½æ¨ç†
requirements: requests, pydantic
"""

import os
import json
import requests
import time
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel
from enum import Enum


class CoTStrategy(Enum):
    """æ€ç»´é“¾ç­–ç•¥æšä¸¾"""
    BASIC_COT = "basic_cot"              # åŸºç¡€æ€ç»´é“¾
    TREE_OF_THOUGHTS = "tree_of_thoughts" # æ ‘çŠ¶æ€ç»´
    SELF_REFLECTION = "self_reflection"   # è‡ªæˆ‘åæ€
    REVERSE_CHAIN = "reverse_chain"       # åå‘æ€ç»´é“¾
    MULTI_PERSPECTIVE = "multi_perspective" # å¤šè§†è§’åˆ†æ
    STEP_BACK = "step_back"              # æ­¥é€€åˆ†æ
    ANALOGICAL = "analogical"            # ç±»æ¯”æ¨ç†


class ThinkingDepth(Enum):
    """æ€è€ƒæ·±åº¦æšä¸¾"""
    SHALLOW = 1     # æµ…å±‚æ€è€ƒ(1-2æ­¥)
    MODERATE = 2    # ä¸­ç­‰æ·±åº¦(3-5æ­¥)
    DEEP = 3        # æ·±åº¦æ€è€ƒ(6-8æ­¥)
    EXHAUSTIVE = 4  # ç©·å°½åˆ†æ(8+æ­¥)


class HistoryContextManager:
    """å†å²ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¸“ä¸ºæ€ç»´é“¾ä¼˜åŒ–"""
    
    DEFAULT_HISTORY_TURNS = 3
    
    @classmethod
    def extract_recent_context(cls, messages: List[dict], max_turns: int = None) -> str:
        """æå–æœ€è¿‘çš„å†å²ä¼šè¯ä¸Šä¸‹æ–‡"""
        if not messages or len(messages) < 2:
            return ""
            
        max_turns = max_turns or cls.DEFAULT_HISTORY_TURNS
        max_messages = max_turns * 2
        
        recent_messages = messages[-max_messages:] if len(messages) > max_messages else messages
        context_text = ""
        
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                context_text += f"ç”¨æˆ·é—®é¢˜: {content}\n"
            elif role == "assistant":
                # æå–æ€ç»´è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
                if "ğŸ¤” **æ€ç»´è¿‡ç¨‹**:" in content:
                    thinking_part = content.split("ğŸ¤” **æ€ç»´è¿‡ç¨‹**:")[1].split("ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:")[0].strip()
                    final_part = content.split("ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:")[1].strip() if "ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:" in content else ""
                    context_text += f"åŠ©æ‰‹æ€è€ƒ: {thinking_part[:200]}...\n"
                    context_text += f"åŠ©æ‰‹å›ç­”: {final_part[:200]}...\n"
                else:
                    context_text += f"åŠ©æ‰‹: {content[:300]}...\n"
                
        return context_text.strip()

    @classmethod
    def format_thinking_prompt(cls, 
                             query: str, 
                             messages: List[dict], 
                             strategy: CoTStrategy, 
                             depth: ThinkingDepth,
                             max_turns: int = None) -> str:
        """æ ¼å¼åŒ–æ€ç»´é“¾ç”Ÿæˆæç¤º"""
        context_text = cls.extract_recent_context(messages, max_turns)
        
        # åŸºç¡€æç¤ºæ„å»º
        base_prompt = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ·±åº¦æ€è€ƒã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

        if context_text.strip():
            base_prompt = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ·±åº¦æ€è€ƒã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

        # æ ¹æ®ç­–ç•¥æ·»åŠ ç‰¹å®šæŒ‡å¯¼
        strategy_instructions = cls._get_strategy_instructions(strategy, depth)
        
        return f"""{base_prompt}

{strategy_instructions}

è¯·å±•ç°ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š
- ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒ
- é’ˆå¯¹è¿™ä¸ªå…·ä½“é—®é¢˜æ·±å…¥åˆ†æï¼Œä½“ç°å®è´¨æ€§æ€ç»´
- é¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼
- æ€è€ƒè¿‡ç¨‹è¦è¿è´¯è‡ªç„¶ï¼Œå¦‚åŒå†…å¿ƒçš„æ€ç»´æµæ·Œ
- ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®º
- è®©æ€ç»´å›´ç»•é—®é¢˜è‡ªç„¶å±•å¼€å’Œæ·±å…¥

è¯·å¼€å§‹æ€è€ƒï¼š"""

    @classmethod
    def format_answer_prompt(cls,
                           query: str,
                           thinking_process: str,
                           messages: List[dict],
                           max_turns: int = None) -> str:
        """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæç¤º"""
        context_text = cls.extract_recent_context(messages, max_turns)
        
        prompt_content = f"""è¯·åŸºäºä»¥ä¸‹åˆ†æè¿‡ç¨‹ï¼Œç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

åˆ†æè¿‡ç¨‹:
{thinking_process}

åŸå§‹é—®é¢˜: {query}"""

        if context_text.strip():
            prompt_content = f"""è¯·åŸºäºå¯¹è¯å†å²å’Œåˆ†æè¿‡ç¨‹ï¼Œç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

åˆ†æè¿‡ç¨‹:
{thinking_process}

åŸå§‹é—®é¢˜: {query}"""

        prompt_content += """

è¯·åŸºäºä¸Šè¿°åˆ†æè¿‡ç¨‹æä¾›æœ€ç»ˆç­”æ¡ˆï¼š
1. ç»¼åˆåˆ†æè¿‡ç¨‹ä¸­çš„å…³é”®æ´å¯Ÿå’Œç»“è®º
2. æä¾›ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†çš„ä¸“ä¸šå›ç­”
3. ç¡®ä¿ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå®ç”¨ä»·å€¼
4. é€‚å½“æä¾›å…·ä½“çš„å»ºè®®ã€æ–¹æ¡ˆæˆ–æŒ‡å¯¼æ„è§
5. å¦‚æœåˆ†æä¸­æ¶‰åŠå¤šä¸ªå¯èƒ½æ€§ï¼Œè¯·æ˜ç¡®è¯´æ˜å„è‡ªçš„é€‚ç”¨æ¡ä»¶

è¯·ç›´æ¥æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œä½¿ç”¨ä¸“ä¸šè§„èŒƒçš„è¡¨è¾¾æ–¹å¼ã€‚"""

        return prompt_content

    @classmethod
    def _get_strategy_instructions(cls, strategy: CoTStrategy, depth: ThinkingDepth) -> str:
        """è·å–ç‰¹å®šç­–ç•¥çš„æŒ‡å¯¼è¯´æ˜"""
        instructions = {
            CoTStrategy.BASIC_COT: """æ€ç»´æ–¹å¼ï¼šè®©æ€è€ƒæ²¿ç€é€»è¾‘çš„è„‰ç»œè‡ªç„¶æ¨è¿›ï¼Œæ¯ä¸ªæƒ³æ³•éƒ½è‡ªç„¶åœ°å¼•å‘ä¸‹ä¸€ä¸ªæƒ³æ³•ï¼Œå½¢æˆè¿è´¯çš„æ€ç»´é“¾æ¡ã€‚""",

            CoTStrategy.TREE_OF_THOUGHTS: """æ€ç»´æ–¹å¼ï¼šé¢å¯¹é—®é¢˜æ—¶è®©æ€ç»´è‡ªç„¶åœ°æ¢ç´¢ä¸åŒçš„å¯èƒ½æ–¹å‘ï¼Œåœ¨å„ç§æ€è·¯é—´æ¸¸èµ°æ¯”è¾ƒï¼Œæ ¹æ®æ€è€ƒçš„æ·±å…¥ç¨‹åº¦é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ–¹å‘ç»§ç»­æ¢ç´¢ã€‚""",

            CoTStrategy.SELF_REFLECTION: """æ€ç»´æ–¹å¼ï¼šåœ¨æ€è€ƒè¿‡ç¨‹ä¸­è‡ªç„¶åœ°å¯¹è‡ªå·±çš„æƒ³æ³•è¿›è¡Œå®¡è§†ï¼Œå¯¹å¯èƒ½çš„ç–æ¼æˆ–é”™è¯¯ä¿æŒæ•æ„Ÿï¼Œè®©æ€ç»´åœ¨è‡ªæˆ‘è´¨ç–‘ä¸­ä¸æ–­å®Œå–„ã€‚""",

            CoTStrategy.REVERSE_CHAIN: """æ€ç»´æ–¹å¼ï¼šä»æƒ³è¦è¾¾åˆ°çš„ç›®æ ‡å¼€å§‹æ€è€ƒï¼Œè‡ªç„¶åœ°è¿½æº¯å®ç°è¿™ä¸ªç›®æ ‡éœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Œå±‚å±‚é€’æ¨åœ°æ„å»ºèµ·é€šå‘ç›®æ ‡çš„æ€ç»´è·¯å¾„ã€‚""",

            CoTStrategy.MULTI_PERSPECTIVE: """æ€ç»´æ–¹å¼ï¼šè®©æ€ç»´è‡ªç„¶åœ°ä»ä¸åŒçš„è§’åº¦å»å®¡è§†é—®é¢˜ï¼Œè®¾èº«å¤„åœ°è€ƒè™‘å„æ–¹çš„ç«‹åœºå’Œè§‚ç‚¹ï¼Œåœ¨å¤šé‡è§†è§’çš„äº¤ç»‡ä¸­å½¢æˆæ›´å…¨é¢çš„ç†è§£ã€‚""",

            CoTStrategy.STEP_BACK: """æ€ç»´æ–¹å¼ï¼šå…ˆè®©æ€ç»´è·³å‡ºå…·ä½“é—®é¢˜çš„ç»†èŠ‚ï¼Œä»æ›´å®è§‚çš„å±‚é¢å»ç†è§£é—®é¢˜çš„æœ¬è´¨ï¼Œç„¶åå°†è¿™ç§å®è§‚ç†è§£è‡ªç„¶åœ°åº”ç”¨åˆ°å…·ä½“é—®é¢˜çš„æ€è€ƒä¸­ã€‚""",

            CoTStrategy.ANALOGICAL: """æ€ç»´æ–¹å¼ï¼šè®©æ€ç»´è‡ªç„¶åœ°è”æƒ³åˆ°ç›¸ä¼¼çš„æƒ…å†µå’Œæ¡ˆä¾‹ï¼Œé€šè¿‡è¿™äº›ç›¸ä¼¼æ€§æ¥å¯å‘å¯¹å½“å‰é—®é¢˜çš„æ€è€ƒï¼Œåœ¨ç±»æ¯”ä¸­å¯»æ‰¾è§£å†³é—®é¢˜çš„çµæ„Ÿã€‚"""
        }
        
        depth_instructions = {
            ThinkingDepth.SHALLOW: "æ€è€ƒæ·±åº¦ï¼šå›´ç»•æ ¸å¿ƒè¦ç´ è¿›è¡Œç²¾å‡†çš„æ€è€ƒã€‚",
            ThinkingDepth.MODERATE: "æ€è€ƒæ·±åº¦ï¼šè¿›è¡Œé€‚åº¦æ·±å…¥çš„æ€è€ƒï¼Œåœ¨å…¨é¢æ€§å’Œæ•ˆç‡é—´æ‰¾åˆ°å¹³è¡¡ã€‚", 
            ThinkingDepth.DEEP: "æ€è€ƒæ·±åº¦ï¼šè®©æ€ç»´æ·±å…¥åˆ°ç»†èŠ‚å±‚é¢ï¼Œå……åˆ†è€ƒè™‘å„ç§å½±å“å› ç´ å’Œå¯èƒ½æ€§ã€‚",
            ThinkingDepth.EXHAUSTIVE: "æ€è€ƒæ·±åº¦ï¼šè¿›è¡Œå…¨é¢æ·±å…¥çš„æ€è€ƒï¼Œå°½å¯èƒ½æ¶µç›–æ‰€æœ‰ç›¸å…³çš„ç»´åº¦å’Œç»†èŠ‚ã€‚"
        }
        
        return f"{instructions[strategy]}\n\n{depth_instructions[depth]}"


class Pipeline:
    class Valves(BaseModel):
        # OpenAI APIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # æ€ç»´é“¾ç‰¹å®šé…ç½®
        COT_STRATEGY: str
        THINKING_DEPTH: str
        THINKING_TEMPERATURE: float
        ANSWER_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int
        
        # é«˜çº§é…ç½®
        MAX_THINKING_TOKENS: int
        ENABLE_SELF_CORRECTION: bool
        USE_THINKING_CACHE: bool

    def __init__(self):
        self.name = "Chain of Thought Pipeline"
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "thinking_tokens": 0,
            "answer_tokens": 0, 
            "total_tokens": 0
        }
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # æ€ç»´é“¾ç‰¹å®šé…ç½®
                "COT_STRATEGY": os.getenv("COT_STRATEGY", "basic_cot"),
                "THINKING_DEPTH": os.getenv("THINKING_DEPTH", "moderate"),
                "THINKING_TEMPERATURE": float(os.getenv("THINKING_TEMPERATURE", "0.8")),
                "ANSWER_TEMPERATURE": float(os.getenv("ANSWER_TEMPERATURE", "0.6")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
                
                # é«˜çº§é…ç½®
                "MAX_THINKING_TOKENS": int(os.getenv("MAX_THINKING_TOKENS", "2000")),
                "ENABLE_SELF_CORRECTION": os.getenv("ENABLE_SELF_CORRECTION", "false").lower() == "true",
                "USE_THINKING_CACHE": os.getenv("USE_THINKING_CACHE", "false").lower() == "true",
            }
        )

    async def on_startup(self):
        print(f"æ€ç»´é“¾æ¨ç†Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            
        # éªŒè¯ç­–ç•¥å’Œæ·±åº¦é…ç½®
        try:
            CoTStrategy(self.valves.COT_STRATEGY)
            ThinkingDepth[self.valves.THINKING_DEPTH.upper()]
            print(f"âœ… æ€ç»´é“¾é…ç½®éªŒè¯æˆåŠŸ: ç­–ç•¥={self.valves.COT_STRATEGY}, æ·±åº¦={self.valves.THINKING_DEPTH}")
        except (ValueError, KeyError) as e:
            print(f"âŒ æ€ç»´é“¾é…ç½®é”™è¯¯: {e}")

    async def on_shutdown(self):
        print(f"æ€ç»´é“¾æ¨ç†Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_thinking_tokens(self, text: str):
        """æ·»åŠ æ€ç»´è¿‡ç¨‹tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["thinking_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_answer_tokens(self, text: str):
        """æ·»åŠ ç­”æ¡ˆtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["answer_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "thinking_tokens": 0,
            "answer_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _get_strategy_and_depth(self) -> tuple:
        """è·å–å½“å‰çš„ç­–ç•¥å’Œæ·±åº¦é…ç½®"""
        try:
            strategy = CoTStrategy(self.valves.COT_STRATEGY)
        except ValueError:
            strategy = CoTStrategy.BASIC_COT
            
        try:
            depth = ThinkingDepth[self.valves.THINKING_DEPTH.upper()]
        except KeyError:
            depth = ThinkingDepth.MODERATE
            
        return strategy, depth

    def _stage1_thinking(self, query: str, messages: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ1ï¼šç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"

        strategy, depth = self._get_strategy_and_depth()
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç°åœ¨éœ€è¦å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ·±åº¦æ€è€ƒã€‚è¯·å±•ç°ä½ çš„æ€ç»´è¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€ã€‚

æ€è€ƒè¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šçš„ä¹¦é¢è¯­è¨€ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾
- æ€è€ƒè¿‡ç¨‹è¦è‡ªç„¶è¿è´¯ï¼Œå¦‚åŒä¸“ä¸šäººå£«å†…å¿ƒçš„æ€ç»´æµæ·Œ
- ä¸è¦ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·æˆ–å…¶ä»–ç»“æ„åŒ–æ ¼å¼
- é’ˆå¯¹å…·ä½“é—®é¢˜æ·±å…¥æ€è€ƒï¼ŒåŒ…å«å®è´¨æ€§çš„åˆ†æå†…å®¹
- æ€è€ƒè¿‡ç¨‹ä¸­ä¸è¦å‡ºç°æ€»ç»“æˆ–ç»“è®ºéƒ¨åˆ†
- è®©æ€ç»´è‡ªç„¶å±•å¼€ï¼Œä½“ç°çœŸå®çš„æ¨ç†è¿‡ç¨‹

è¯·å¼€å§‹ä½ çš„æ·±åº¦æ€è€ƒï¼š"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_thinking_prompt(
            query=query,
            messages=messages or [],
            strategy=strategy,
            depth=depth,
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.MAX_THINKING_TOKENS,
            "temperature": self.valves.THINKING_TEMPERATURE,
            "stream": stream
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_thinking_tokens(system_prompt)
        self._add_thinking_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print(f"ğŸ¤” ç”Ÿæˆæ€ç»´è¿‡ç¨‹ - ç­–ç•¥: {strategy.value}, æ·±åº¦: {depth.name}")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æ¸©åº¦: {self.valves.THINKING_TEMPERATURE}")
        
        try:
            if stream:
                for chunk in self._stream_openai_response(url, headers, payload, is_thinking=True):
                    yield chunk
            else:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT
                )
                response.raise_for_status()
                result = response.json()
                
                thinking = result["choices"][0]["message"]["content"]
                self._add_thinking_tokens(thinking)
                
                if self.valves.DEBUG_MODE:
                    print(f"âœ… æ€ç»´è¿‡ç¨‹ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(thinking)}")
                    
                return thinking

        except Exception as e:
            error_msg = f"æ€ç»´é“¾ç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            if stream:
                yield error_msg
            else:
                return error_msg

    def _stage2_answer(self, query: str, thinking_process: str, messages: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ2ï¼šåŸºäºæ€ç»´é“¾ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦åŸºäºå‰è¿°æ€ç»´åˆ†æè¿‡ç¨‹ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å®ç”¨çš„æœ€ç»ˆç­”æ¡ˆã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. ç»¼åˆæ€ç»´åˆ†æä¸­çš„å…³é”®æ´å¯Ÿå’Œç»“è®º
2. æä¾›ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†çš„ä¸“ä¸šå›ç­”
3. ç¡®ä¿ç­”æ¡ˆå‡†ç¡®æ€§å’Œå®ç”¨ä»·å€¼ï¼Œç›´æ¥å›åº”ç”¨æˆ·éœ€æ±‚
4. é€‚å½“æä¾›å…·ä½“çš„å»ºè®®ã€æ–¹æ¡ˆæˆ–æŒ‡å¯¼æ„è§
5. ä¿æŒå®¢è§‚ç«‹åœºï¼Œæ˜ç¡®è¡¨è¾¾ä»»ä½•ä¸ç¡®å®šæ€§æˆ–å±€é™æ€§

è¯·ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„ä¸­æ–‡è¡¨è¾¾ï¼Œç¡®ä¿è¯­è¨€è§„èŒƒä¸”æ˜“äºç†è§£ã€‚"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_answer_prompt(
            query=query,
            thinking_process=thinking_process,
            messages=messages or [],
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.ANSWER_TEMPERATURE,
            "stream": stream
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_answer_tokens(system_prompt)
        self._add_answer_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print("ğŸ“ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æ¸©åº¦: {self.valves.ANSWER_TEMPERATURE}")
        
        try:
            if stream:
                for chunk in self._stream_openai_response(url, headers, payload, is_thinking=False):
                    yield chunk
            else:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT
                )
                response.raise_for_status()
                result = response.json()

                answer = result["choices"][0]["message"]["content"]
                self._add_answer_tokens(answer)

                if self.valves.DEBUG_MODE:
                    print(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)}")

                return answer

        except Exception as e:
            error_msg = f"ç­”æ¡ˆç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            if stream:
                yield error_msg
            else:
                return error_msg

    def _stream_openai_response(self, url, headers, payload, is_thinking=False):
        """æµå¼å¤„ç†OpenAIå“åº”"""
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                if is_thinking:
                                    self._add_thinking_tokens(delta)
                                else:
                                    self._add_answer_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            if self.valves.DEBUG_MODE:
                stage = "æ€ç»´è¿‡ç¨‹" if is_thinking else "æœ€ç»ˆç­”æ¡ˆ"
                print(f"âœ… {stage}æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(collected_content)}")
                
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°ï¼Œå¤„ç†ç”¨æˆ·è¯·æ±‚"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ’­ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        strategy, depth = self._get_strategy_and_depth()
        
        # é˜¶æ®µ1ï¼šç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹
        yield f"ğŸ¤” **æ€ç»´è¿‡ç¨‹**: æ­£åœ¨æ·±å…¥æ€è€ƒ...\n\n"
        
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING
        thinking_content = ""
        
        try:
            if stream_mode:
                # æµå¼æ¨¡å¼ - æ€ç»´è¿‡ç¨‹
                for chunk in self._stage1_thinking(user_message, messages, stream=True):
                    thinking_content += chunk
                    yield chunk
            else:
                # éæµå¼æ¨¡å¼ - æ€ç»´è¿‡ç¨‹
                thinking_result = self._stage1_thinking(user_message, messages, stream=False)
                thinking_content = thinking_result
                yield thinking_result
                
        except Exception as e:
            error_msg = f"âŒ æ€ç»´è¿‡ç¨‹ç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg
            return

        yield "\n\n"
        
        # è‡ªæˆ‘çº é”™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.valves.ENABLE_SELF_CORRECTION and "é”™è¯¯" in thinking_content.lower():
            yield "ğŸ” **è‡ªæˆ‘çº é”™**: æ£€æµ‹åˆ°å¯èƒ½çš„é”™è¯¯ï¼Œæ­£åœ¨ä¿®æ­£...\n\n"
            # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªæˆ‘çº é”™é€»è¾‘
        
        # é˜¶æ®µ2ï¼šåŸºäºæ€ç»´é“¾ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        yield "ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**: \n\n"
        
        try:
            if stream_mode:
                # æµå¼æ¨¡å¼ - æœ€ç»ˆç­”æ¡ˆ
                for chunk in self._stage2_answer(user_message, thinking_content, messages, stream=True):
                    yield chunk
                # æµå¼æ¨¡å¼ç»“æŸåæ·»åŠ tokenç»Ÿè®¡
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **åˆ†æç»Ÿè®¡**: æ€ç»´è¿‡ç¨‹ {token_info['thinking_tokens']} tokens, æœ€ç»ˆç­”æ¡ˆ {token_info['answer_tokens']} tokens, æ€»è®¡ {token_info['total_tokens']} tokens"
            else:
                # éæµå¼æ¨¡å¼ - æœ€ç»ˆç­”æ¡ˆ
                result = self._stage2_answer(user_message, thinking_content, messages, stream=False)
                yield result
                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **åˆ†æç»Ÿè®¡**: æ€ç»´è¿‡ç¨‹ {token_info['thinking_tokens']} tokens, æœ€ç»ˆç­”æ¡ˆ {token_info['answer_tokens']} tokens, æ€»è®¡ {token_info['total_tokens']} tokens"

        except Exception as e:
            error_msg = f"âŒ æœ€ç»ˆç­”æ¡ˆç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

        # æä¾›æ”¹è¿›å»ºè®®
        if self.valves.DEBUG_MODE:
            strategy_suggestions = {
                CoTStrategy.BASIC_COT: "å¦‚éœ€æ›´æ·±å…¥åˆ†æï¼Œå¯å°è¯•tree_of_thoughtsç­–ç•¥",
                CoTStrategy.TREE_OF_THOUGHTS: "å¦‚éœ€å¿«é€Ÿåˆ†æï¼Œå¯å°è¯•basic_cotç­–ç•¥", 
                CoTStrategy.SELF_REFLECTION: "å¦‚éœ€å¤šè§’åº¦åˆ†æï¼Œå¯å°è¯•multi_perspectiveç­–ç•¥"
            }
            
            if strategy in strategy_suggestions:
                yield f"\n\nğŸ’¡ **ä¼˜åŒ–å»ºè®®**: {strategy_suggestions[strategy]}"
