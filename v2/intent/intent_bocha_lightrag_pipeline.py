"""
title: ä¸“ä¸šé¢†åŸŸæ™ºèƒ½æ„å›¾è¯†åˆ«ç®¡é“
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: å¯é…ç½®çš„ä¸“ä¸šé¢†åŸŸæ™ºèƒ½æ„å›¾è¯†åˆ«5é˜¶æ®µç®¡é“ï¼š1) ä¸“ä¸šæ„å›¾è¯†åˆ«åˆ¤æ–­ï¼Œ2) ä¸“ä¸šæŸ¥è¯¢ä¼˜åŒ–ï¼Œ3) ç½‘ç»œæœç´¢ï¼Œ4) ç”Ÿæˆå¢å¼ºLightRAGæŸ¥è¯¢ï¼Œ5) LightRAGä¸“ä¸šé—®ç­”ã€‚é»˜è®¤é…ç½®ä¸ºåŒ–å¦†å“ä¸åŒ–å­¦åŸæ–™é¢†åŸŸï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰å…¶ä»–ä¸“ä¸šé¢†åŸŸã€‚
requirements: requests, pydantic
"""

import os
import json
import requests
import time
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel


class Pipeline:
    class Valves(BaseModel):
        # Bochaæœç´¢APIé…ç½®
        BOCHA_API_KEY: str
        BOCHA_BASE_URL: str
        BOCHA_SEARCH_COUNT: int
        BOCHA_FRESHNESS: str
        BOCHA_ENABLE_SUMMARY: bool
        BOCHA_TIMEOUT: int

        # OpenAIé…ç½®ï¼ˆç”¨äºæ„å›¾è¯†åˆ«ã€é—®é¢˜ä¼˜åŒ–å’ŒLightRAGæŸ¥è¯¢ç”Ÿæˆï¼‰
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # LightRAGé…ç½®
        LIGHTRAG_BASE_URL: str
        LIGHTRAG_DEFAULT_MODE: str
        LIGHTRAG_TIMEOUT: int
        LIGHTRAG_ENABLE_STREAMING: bool

        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        INTENT_CONFIDENCE_THRESHOLD: float
        PROFESSIONAL_DOMAIN: str
        DOMAIN_DESCRIPTION: str

    def __init__(self):
        # å…ˆè®¾ç½®é»˜è®¤åç§°ï¼Œç¨åä¼šåœ¨valvesåˆå§‹åŒ–åæ›´æ–°
        self.name = "ä¸“ä¸šé¢†åŸŸæ™ºèƒ½æ„å›¾è¯†åˆ«ç®¡é“"
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

        self.valves = self.Valves(
            **{
                # Bochaæœç´¢é…ç½®
                "BOCHA_API_KEY": os.getenv("BOCHA_API_KEY", ""),
                "BOCHA_BASE_URL": os.getenv("BOCHA_BASE_URL", "https://api.bochaai.com/v1"),
                "BOCHA_SEARCH_COUNT": int(os.getenv("BOCHA_SEARCH_COUNT", "20")),
                "BOCHA_FRESHNESS": os.getenv("BOCHA_FRESHNESS", "oneMonth"),  # oneDay, oneWeek, oneMonth, oneYear, all
                "BOCHA_ENABLE_SUMMARY": os.getenv("BOCHA_ENABLE_SUMMARY", "true").lower() == "true",
                "BOCHA_TIMEOUT": int(os.getenv("BOCHA_TIMEOUT", "30")),

                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "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"),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),

                # LightRAGé…ç½®
                "LIGHTRAG_BASE_URL": os.getenv("LIGHTRAG_BASE_URL", "http://117.50.252.245:9621"),
                "LIGHTRAG_DEFAULT_MODE": os.getenv("LIGHTRAG_DEFAULT_MODE", "hybrid"),
                "LIGHTRAG_TIMEOUT": int(os.getenv("LIGHTRAG_TIMEOUT", "30")),
                "LIGHTRAG_ENABLE_STREAMING": os.getenv("LIGHTRAG_ENABLE_STREAMING", "true").lower() == "true",

                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "INTENT_CONFIDENCE_THRESHOLD": float(os.getenv("INTENT_CONFIDENCE_THRESHOLD", "0.8")),
                "PROFESSIONAL_DOMAIN": os.getenv("PROFESSIONAL_DOMAIN", "åŒ–å¦†å“ã€åŸæ–™ã€é…æ–¹ã€æ³•è§„ã€åº”ç”¨"),
                "DOMAIN_DESCRIPTION": os.getenv("DOMAIN_DESCRIPTION", "åŒ–å¦†å“ã€åŒ–å­¦åŸæ–™ã€é…æ–¹å¼€å‘ã€æ³•è§„åˆè§„ã€å¸‚åœºåº”ç”¨"),
            }
        )
        
        # æ ¹æ®é…ç½®çš„ä¸“ä¸šé¢†åŸŸæ›´æ–°ç³»ç»Ÿåç§°
        self.name = f"åšæŸ¥_{self.valves.PROFESSIONAL_DOMAIN}æ™ºèƒ½æ„å›¾è¯†åˆ«ç®¡é“"

    async def on_startup(self):
        print(f"ğŸ§ª {self.valves.PROFESSIONAL_DOMAIN}æ™ºèƒ½æ„å›¾è¯†åˆ«ç®¡é“å¯åŠ¨: {__name__}")

        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.BOCHA_API_KEY:
            print("âŒ ç¼ºå°‘Bocha APIå¯†é’¥ï¼Œè¯·è®¾ç½®BOCHA_API_KEYç¯å¢ƒå˜é‡")
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")

        # æµ‹è¯•Bocha APIè¿æ¥
        if self.valves.BOCHA_API_KEY:
            try:
                print("ğŸ”§ å¼€å§‹æµ‹è¯•Bocha APIè¿æ¥...")
                test_response = self._search_bocha(f"{self.valves.PROFESSIONAL_DOMAIN}æµ‹è¯•", count=1)
                if "error" not in test_response:
                    print("âœ… Bochaæœç´¢APIè¿æ¥æˆåŠŸ")
                else:
                    print(f"âš ï¸ Bochaæœç´¢APIæµ‹è¯•å¤±è´¥: {test_response['error']}")
            except Exception as e:
                print(f"âŒ Bochaæœç´¢APIè¿æ¥å¤±è´¥: {e}")
        else:
            print("âš ï¸ è·³è¿‡Bocha APIæµ‹è¯•ï¼ˆæœªè®¾ç½®APIå¯†é’¥ï¼‰")

        # æµ‹è¯•LightRAGè¿æ¥
        try:
            response = requests.get(f"{self.valves.LIGHTRAG_BASE_URL}/health", timeout=5)
            if response.status_code == 200:
                print("âœ… LightRAGçŸ¥è¯†åº“æœåŠ¡è¿æ¥æˆåŠŸ")
            else:
                print(f"âš ï¸ LightRAGçŸ¥è¯†åº“æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°LightRAGçŸ¥è¯†åº“æœåŠ¡: {e}")

        print(f"ğŸ§ª {self.valves.PROFESSIONAL_DOMAIN}ä¸“ä¸šç®¡é“åˆå§‹åŒ–å®Œæˆ")

    async def on_shutdown(self):
        print(f"ğŸ§ª {self.valves.PROFESSIONAL_DOMAIN}æ™ºèƒ½æ„å›¾è¯†åˆ«ç®¡é“å…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """
        ç®€å•çš„tokenä¼°ç®—å‡½æ•°ï¼ŒåŸºäºå­—ç¬¦æ•°ä¼°ç®—
        ä¸­æ–‡å­—ç¬¦æŒ‰1ä¸ªtokenè®¡ç®—ï¼Œè‹±æ–‡å•è¯æŒ‰å¹³å‡1.3ä¸ªtokenè®¡ç®—
        """
        if not text:
            return 0

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')

        # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])

        # ä¼°ç®—tokenæ•°ï¼šä¸­æ–‡å­—ç¬¦1:1ï¼Œè‹±æ–‡å•è¯1:1.3
        estimated_tokens = chinese_chars + int(english_words * 1.3)

        return max(estimated_tokens, 1)  # è‡³å°‘è¿”å›1ä¸ªtoken

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _search_bocha(self, query: str, count: int = None) -> dict:
        """è°ƒç”¨Bocha Web Search API"""
        url = f"{self.valves.BOCHA_BASE_URL}/web-search"

        # å‚æ•°éªŒè¯
        search_count = count or self.valves.BOCHA_SEARCH_COUNT
        if search_count < 1 or search_count > 20:
            search_count = min(max(search_count, 1), 20)  # é™åˆ¶åœ¨1-20ä¹‹é—´

        # æ„å»ºè¯·æ±‚è½½è·
        payload = {
            "query": query.strip(),
            "count": search_count,
            "summary": self.valves.BOCHA_ENABLE_SUMMARY
        }

        headers = {
            "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
            "Content-Type": "application/json"
        }

        try:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            if self.valves.DEBUG_MODE:
                print("ğŸ” Bochaæœç´¢è°ƒè¯•ä¿¡æ¯:")
                print(f"   URL: {url}")
                print(f"   API Keyå‰ç¼€: {self.valves.BOCHA_API_KEY[:10]}..." if self.valves.BOCHA_API_KEY else "   API Key: æœªè®¾ç½®")
                print(f"   è¯·æ±‚è½½è·: {json.dumps(payload, ensure_ascii=False, indent=2)}")

            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.BOCHA_TIMEOUT
            )

            # æ·»åŠ å“åº”è°ƒè¯•ä¿¡æ¯
            if self.valves.DEBUG_MODE:
                print(f"   å“åº”çŠ¶æ€ç : {response.status_code}")
                if response.status_code != 200:
                    print(f"   å“åº”å†…å®¹: {response.text}")

            response.raise_for_status()
            result = response.json()

            if self.valves.DEBUG_MODE:
                print(f"   å“åº”æˆåŠŸï¼Œæ•°æ®é•¿åº¦: {len(str(result))}")
                # æ‰“å°å“åº”ç»“æ„ä»¥ä¾¿è°ƒè¯•
                if isinstance(result, dict):
                    print(f"   å“åº”ç»“æ„: code={result.get('code')}, msg={result.get('msg')}")
                    data = result.get('data', {})
                    if data:
                        web_pages = data.get('webPages', {})
                        if web_pages:
                            value_count = len(web_pages.get('value', []))
                            print(f"   æœç´¢ç»“æœæ•°é‡: {value_count}")

            return result

        except requests.exceptions.Timeout:
            error_msg = "Bochaæœç´¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return {"error": error_msg}
        except requests.exceptions.HTTPError as e:
            error_msg = f"Bochaæœç´¢HTTPé”™è¯¯: {e.response.status_code}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
                if hasattr(e.response, 'text'):
                    print(f"   é”™è¯¯è¯¦æƒ…: {e.response.text}")
            return {"error": error_msg}
        except requests.exceptions.RequestException as e:
            error_msg = f"Bochaæœç´¢ç½‘ç»œé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return {"error": error_msg}
        except Exception as e:
            error_msg = f"Bochaæœç´¢æœªçŸ¥é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return {"error": error_msg}

    def _format_search_results(self, search_response: dict) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºæ–‡æœ¬"""
        if "error" in search_response:
            return f"æœç´¢é”™è¯¯: {search_response['error']}"

        # æ£€æŸ¥å“åº”ç»“æ„
        if not isinstance(search_response, dict):
            return "æœç´¢å“åº”æ ¼å¼é”™è¯¯"

        # æ£€æŸ¥Bocha APIçš„å“åº”ç»“æ„ï¼šresponse.data.webPages.value
        data = search_response.get("data", {})
        if not data:
            return "æœç´¢å“åº”æ•°æ®ä¸ºç©º"

        web_pages_data = data.get("webPages", {})
        web_pages = web_pages_data.get("value", [])

        if not web_pages:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯"

        formatted_results = []
        total_results = web_pages_data.get("totalEstimatedMatches", 0)

        # æ·»åŠ æœç´¢ç»Ÿè®¡ä¿¡æ¯
        if total_results > 0:
            formatted_results.append(f"æ‰¾åˆ°çº¦ {total_results:,} æ¡ç›¸å…³ç»“æœ\n")

        for i, page in enumerate(web_pages[:self.valves.BOCHA_SEARCH_COUNT], 1):
            title = (page.get("name") or "æ— æ ‡é¢˜").strip()
            url = (page.get("url") or "").strip()
            snippet = (page.get("snippet") or "").strip()
            summary = (page.get("summary") or "").strip()
            site_name = (page.get("siteName") or "").strip()
            date_published = (page.get("datePublished") or "").strip()

            result_text = f"**{i}. {title}**\n"

            if site_name:
                result_text += f"   æ¥æº: {site_name}\n"

            if date_published:
                result_text += f"   å‘å¸ƒæ—¶é—´: {date_published}\n"

            if url:
                result_text += f"   é“¾æ¥: {url}\n"

            # ä¼˜å…ˆæ˜¾ç¤ºè¯¦ç»†æ‘˜è¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºæ™®é€šæ‘˜è¦
            content_to_show = summary if summary else snippet
            if content_to_show:
                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                if len(content_to_show) > 300:
                    content_to_show = content_to_show[:300] + "..."
                result_text += f"   å†…å®¹: {content_to_show}\n"

            formatted_results.append(result_text)

        return "\n".join(formatted_results)

    def _extract_search_links(self, search_response: dict) -> str:
        """æå–æœç´¢ç»“æœä¸­çš„é“¾æ¥ï¼Œæ ¼å¼åŒ–ä¸ºMDæ ¼å¼"""
        if "error" in search_response:
            return ""

        # æ£€æŸ¥å“åº”ç»“æ„
        if not isinstance(search_response, dict):
            return ""

        # æ£€æŸ¥Bocha APIçš„å“åº”ç»“æ„ï¼šresponse.data.webPages.value
        data = search_response.get("data", {})
        if not data:
            return ""

        web_pages_data = data.get("webPages", {})
        web_pages = web_pages_data.get("value", [])

        if not web_pages:
            return ""

        formatted_links = []
        for i, page in enumerate(web_pages[:self.valves.BOCHA_SEARCH_COUNT], 1):
            title = (page.get("name") or "æ— æ ‡é¢˜").strip()
            url = (page.get("url") or "").strip()

            if url and title:
                # æ ¼å¼åŒ–ä¸ºMDé“¾æ¥æ ¼å¼
                formatted_links.append(f"{i}. [{title}]({url})")

        return "\n".join(formatted_links)

    def _call_openai_api(self, messages: List[dict], stream: bool = False) -> Union[dict, Iterator[dict]]:
        """è°ƒç”¨OpenAI API"""
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"

        # ç»Ÿè®¡è¾“å…¥token
        input_text = ""
        for message in messages:
            input_text += message.get("content", "") + " "
        self._add_input_tokens(input_text)

        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": stream
        }

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.OPENAI_TIMEOUT,
                stream=stream
            )
            response.raise_for_status()

            if stream:
                return self._parse_stream_response_with_tokens(response)
            else:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    output_content = result['choices'][0]['message']['content']
                    self._add_output_tokens(output_content)
                return result

        except Exception as e:
            raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")

    def _parse_stream_response_with_tokens(self, response) -> Iterator[dict]:
        """è§£ææµå¼å“åº”å¹¶ç»Ÿè®¡token"""
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]
                    if data.strip() == '[DONE]':
                        break
                    try:
                        chunk = json.loads(data)
                        if 'choices' in chunk and len(chunk['choices']) > 0:
                            delta = chunk['choices'][0].get('delta', {})
                            if 'content' in delta:
                                self._add_output_tokens(delta['content'])
                        yield chunk
                    except json.JSONDecodeError:
                        continue

    def _stage1_intent_recognition(self, user_query: str, messages: List[dict]) -> tuple[str, bool, bool]:
        """ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«åˆ¤æ–­"""
        # æ„å»ºå¯¹è¯å†å²
        context_text = ""
        if messages and len(messages) > 1:
            recent_messages = messages[-6:]
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"

        context_info = f"""
**å¯¹è¯å†å²**:
{context_text.strip() if context_text.strip() else "æ— å†å²å¯¹è¯"}

**å½“å‰é—®é¢˜**: {user_query}
"""

        intent_messages = [{
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{self.valves.PROFESSIONAL_DOMAIN}é¢†åŸŸçš„æ„å›¾è¯†åˆ«ä¸“å®¶ã€‚ä½ éœ€è¦åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æœ€ä½³çš„å¤„ç†ç­–ç•¥ã€‚

**åˆ¤æ–­æ ‡å‡†**ï¼š

1. **ç›´æ¥å›ç­”ç±»**ï¼šåŸºäºä¸“ä¸šçŸ¥è¯†å¯ä»¥ç›´æ¥å›ç­”çš„é—®é¢˜
   - åŸºæœ¬æ¦‚å¿µã€å®šä¹‰è§£é‡Š
   - å¸¸è§åŸç†ã€æœºåˆ¶è¯´æ˜
   - åŸºç¡€è®¡ç®—ã€æ¢ç®—æ–¹æ³•
   - ä¸€èˆ¬æ€§æŠ€æœ¯æŒ‡å¯¼

2. **éœ€è¦æœç´¢ç±»**ï¼šéœ€è¦æœ€æ–°ã€å…·ä½“ã€å®æ—¶ä¿¡æ¯çš„é—®é¢˜
   - æœ€æ–°æ³•è§„å˜åŒ–ã€æ”¿ç­–æ›´æ–°
   - ç‰¹å®šå“ç‰Œäº§å“ä¿¡æ¯
   - æœ€æ–°å¸‚åœºè¶‹åŠ¿ã€è¡Œä¸šåŠ¨æ€
   - å…·ä½“ä¾›åº”å•†ä¿¡æ¯ã€ä»·æ ¼è¡Œæƒ…
   - æœ€æ–°ç ”ç©¶æˆæœã€æŠ€æœ¯çªç ´
   - ç‰¹å®šæ—¶é—´äº‹ä»¶ã€æ–°é—»

3. **éœ€è¦çŸ¥è¯†åº“ç±»**ï¼šéœ€è¦ä¸“ä¸šæ–‡æ¡£å’Œæ·±åº¦åˆ†æçš„é—®é¢˜
   - å¤æ‚é…æ–¹è®¾è®¡ä¸ä¼˜åŒ–
   - å¤šæˆåˆ†ç›¸äº’ä½œç”¨åˆ†æ
   - äº§å“ç¨³å®šæ€§å’Œå…¼å®¹æ€§è¯„ä¼°
   - æ·±åº¦å®‰å…¨æ€§è¯„ä»·å’Œé£é™©åˆ†æ
   - ä¸“ä¸šæŠ€æœ¯æ–‡æ¡£æŸ¥è¯¢å’Œè§£è¯»
   - å¤æ‚æ³•è§„åˆè§„æ€§åˆ†æ

**åˆ¤æ–­åŸåˆ™**ï¼šä¸“æ³¨å›ç­”ç”¨æˆ·é—®é¢˜çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
{{
    "intent": "direct_answer|need_search|need_knowledge_base",
    "confidence": 0.0-1.0,
    "reasoning": "åˆ¤æ–­åŸå› ï¼ˆè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©æ­¤ç­–ç•¥ï¼‰",
    "direct_answer": "å¦‚æœé€‰æ‹©direct_answerï¼Œè¯·æä¾›ä¸“ä¸šçš„ç­”æ¡ˆï¼›å¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²"
}}"""
        }, {
            "role": "user",
            "content": context_info
        }]

        try:
            response = self._call_openai_api(intent_messages, stream=False)
            content = response['choices'][0]['message']['content'].strip()
            
            # å°è¯•è§£æJSONå“åº”
            try:
                intent_result = json.loads(content)
                intent_type = intent_result.get("intent", "need_search")
                confidence = intent_result.get("confidence", 0.5)
                reasoning = intent_result.get("reasoning", "æ— æ³•ç¡®å®šæ„å›¾")
                direct_answer = intent_result.get("direct_answer", "")
                
                if self.valves.DEBUG_MODE:
                    print(f"ğŸ§  æ„å›¾è¯†åˆ«ç»“æœ: {intent_type} (ç½®ä¿¡åº¦: {confidence})")
                    print(f"   åŸå› : {reasoning}")
                
                # åŸºäºç½®ä¿¡åº¦å’Œæ„å›¾ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
                if intent_type == "direct_answer" and confidence >= self.valves.INTENT_CONFIDENCE_THRESHOLD:
                    return direct_answer, True, False  # ç›´æ¥å›ç­”
                elif intent_type == "need_knowledge_base":
                    return reasoning, False, True  # éœ€è¦çŸ¥è¯†åº“
                else:
                    return reasoning, False, False  # éœ€è¦æœç´¢
                    
            except json.JSONDecodeError:
                if self.valves.DEBUG_MODE:
                    print(f"âš ï¸ æ„å›¾è¯†åˆ«å“åº”è§£æå¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨æœç´¢: {content}")
                return "å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨æœç´¢ç­–ç•¥", False, False
                
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return "æ„å›¾è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨æœç´¢ç­–ç•¥", False, False

    def _stage2_optimize_query(self, user_query: str, messages: List[dict]) -> str:
        """ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå¯¹è¯å†å²çš„é—®é¢˜ä¼˜åŒ–"""
        context_messages = []

        if messages and len(messages) > 1:
            recent_messages = messages[-6:]  # é»˜è®¤3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
            context_text = ""
            conversation_topics = []

            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                    if len(content) > 10:
                        conversation_topics.append(content[:50])
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"

            if context_text.strip():
                topics_summary = "ã€".join(conversation_topics[-3:]) if conversation_topics else "æ— ç‰¹å®šä¸»é¢˜"

                context_messages.append({
                    "role": "user",
                    "content": f"""è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–çš„{self.valves.PROFESSIONAL_DOMAIN}ä¸“ä¸šæœç´¢æŸ¥è¯¢ã€‚

**å¯¹è¯å†å²**:
{context_text.strip()}

**å¯¹è¯ä¸»é¢˜æ€»ç»“**: {topics_summary}

**å½“å‰é—®é¢˜**: {user_query}

**ä¼˜åŒ–ä»»åŠ¡**:
è¯·åˆ†æå¯¹è¯å†å²ï¼Œç†è§£ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œç”Ÿæˆä¸“ä¸šçš„æœç´¢æŸ¥è¯¢ï¼š

1. **ç†è§£éœ€æ±‚**ï¼šè¯†åˆ«å…³é”®æŠ€æœ¯ç‚¹ã€ä¸“ä¸šæœ¯è¯­ã€åº”ç”¨åœºæ™¯
2. **ä¼˜åŒ–æŸ¥è¯¢**ï¼šä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ
3. **è¾“å‡ºè¦æ±‚**ï¼šç”Ÿæˆä¸“ä¸šä¸”ç²¾å‡†çš„æœç´¢æŸ¥è¯¢ï¼Œé•¿åº¦10-50å­—ï¼Œåªè¿”å›æŸ¥è¯¢æ–‡æœ¬

**ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢**:"""
                })

        if not context_messages:
            context_messages.append({
                "role": "user",
                "content": f"""è¯·ä¼˜åŒ–ä»¥ä¸‹{self.valves.PROFESSIONAL_DOMAIN}é¢†åŸŸçš„æœç´¢æŸ¥è¯¢ï¼Œä½¿å…¶æ›´ä¸“ä¸šå’Œç²¾å‡†ï¼š

**åŸå§‹é—®é¢˜**: {user_query}

**ä¼˜åŒ–è¦æ±‚**:
1. ä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ
2. åŒ…å«ç›¸å…³çš„åº”ç”¨é¢†åŸŸä¿¡æ¯
3. è€ƒè™‘å…³é”®æŠ€æœ¯ç»´åº¦
4. ä¿æŒæŸ¥è¯¢ç®€æ´ä½†ä¿¡æ¯ä¸°å¯Œ
5. åªè¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦è§£é‡Š

**ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢**:"""
            })

        try:
            response = self._call_openai_api(context_messages, stream=False)
            optimized_query = response['choices'][0]['message']['content'].strip()

            if not optimized_query or len(optimized_query) < 3:
                optimized_query = user_query

            if self.valves.DEBUG_MODE:
                print(f"ğŸ”§ æŸ¥è¯¢ä¼˜åŒ–: '{user_query}' â†’ '{optimized_query}'")

            return optimized_query

        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
            return user_query

    def _stage3_search(self, optimized_query: str) -> tuple[str, str, str]:
        """ç¬¬ä¸‰é˜¶æ®µï¼šæœç´¢"""
        search_response = self._search_bocha(optimized_query)
        search_results = self._format_search_results(search_response)
        search_links = self._extract_search_links(search_response)

        if "æœç´¢é”™è¯¯" in search_results or "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ" in search_results:
            search_status = f"âš ï¸ {search_results}"
        else:
            search_status = "âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            if search_links:
                search_status += f"\n\n**ç›¸å…³é“¾æ¥ï¼š**\n{search_links}"

        return search_results, search_status, search_links

    def _stage3_5_analyze_search_results(self, original_query: str, search_results: str, messages: List[dict]) -> str:
        """ç¬¬3.5é˜¶æ®µï¼šLLMåˆ†ææœç´¢ç»“æœï¼Œå»é™¤æ— ç”¨å†…å®¹å¹¶æ•´ç†å›ç­”"""
        # å¦‚æœæœç´¢ç»“æœåŒ…å«é”™è¯¯ï¼Œç›´æ¥è¿”å›
        if "æœç´¢é”™è¯¯" in search_results or "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ" in search_results:
            return search_results

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        context_text = ""
        if messages and len(messages) > 1:
            recent_messages = messages[-6:]
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"

        context_info = f"""
**å¯¹è¯å†å²**:
{context_text.strip() if context_text.strip() else "æ— å†å²å¯¹è¯"}

**å½“å‰é—®é¢˜**: {original_query}

**æœç´¢ç»“æœ**:
{search_results}
"""

        analyze_messages = [{
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{self.valves.PROFESSIONAL_DOMAIN}é¢†åŸŸçš„ä¿¡æ¯åˆ†æä¸“å®¶ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼šä¸“æ³¨å›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€å®Œæ•´çš„ä¸“ä¸šå›ç­”

**åˆ†æç­–ç•¥**ï¼š
1. **æœç´¢ç»“æœè¯„ä¼°**ï¼šåˆ†ææœç´¢ç»“æœæ˜¯å¦å……åˆ†å›ç­”ç”¨æˆ·é—®é¢˜
2. **ä¿¡æ¯ç­›é€‰**ï¼šå»é™¤å¹¿å‘Šã€æ— å…³å•†ä¸šä¿¡æ¯ã€é‡å¤å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
3. **çŸ¥è¯†è¡¥å……**ï¼šå½“æœç´¢ç»“æœä¸è¶³æˆ–ç¼ºä¹å…³é”®ä¿¡æ¯æ—¶ï¼Œè¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¡¥å……
4. **ç»¼åˆå›ç­”**ï¼šç»“åˆæœç´¢ä¿¡æ¯å’Œä¸“ä¸šçŸ¥è¯†ï¼Œæä¾›å®Œæ•´çš„ç­”æ¡ˆ

**å›ç­”åŸåˆ™**ï¼š
- ä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœä¸­çš„æƒå¨ä¿¡æ¯å’Œå…·ä½“æ•°æ®
- å½“æœç´¢ç»“æœä¸è¶³æ—¶ï¼Œæ˜ç¡®æŒ‡å‡ºå¹¶åŸºäºä¸“ä¸šçŸ¥è¯†è¡¥å……
- åŒºåˆ†æœç´¢ä¿¡æ¯å’Œä¸“ä¸šçŸ¥è¯†è¡¥å……çš„å†…å®¹
- ä¿æŒä¸“ä¸šå‡†ç¡®ï¼Œé€»è¾‘æ¸…æ™°
- é‡ç‚¹å†…å®¹ç”¨ç²—ä½“æ ‡è®°

**è¾“å‡ºæ ¼å¼**ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
- å¦‚æœæœç´¢ç»“æœå……åˆ†ï¼šåŸºäºæœç´¢ç»“æœå›ç­”
- å¦‚æœæœç´¢ç»“æœä¸è¶³ï¼šå…ˆè¯´æ˜æœç´¢ç»“æœçš„å±€é™æ€§ï¼Œç„¶ååŸºäºä¸“ä¸šçŸ¥è¯†è¡¥å……å›ç­”
- å¯ä»¥è¿™æ ·è¡¨è¿°ï¼š"æ ¹æ®æœç´¢ç»“æœæ˜¾ç¤º..." æˆ– "åŸºäºä¸“ä¸šçŸ¥è¯†è¡¥å……..."

è¯·ç»¼åˆæœç´¢ç»“æœå’Œä¸“ä¸šçŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"""
        }, {
            "role": "user",
            "content": context_info
        }]

        try:
            response = self._call_openai_api(analyze_messages, stream=False)
            analyzed_results = response['choices'][0]['message']['content'].strip()

            if self.valves.DEBUG_MODE:
                print(f"ğŸ” æœç´¢ç»“æœåˆ†æå®Œæˆ: {analyzed_results[:100]}...")

            return analyzed_results

        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ æœç´¢ç»“æœåˆ†æå¤±è´¥: {e}")
            return search_results  # åˆ†æå¤±è´¥æ—¶è¿”å›åŸå§‹æœç´¢ç»“æœ

    def _stage4_generate_lightrag_query(self, original_query: str, analyzed_search_results: str, messages: List[dict]) -> str:
        """ç¬¬å››é˜¶æ®µï¼šæ ¹æ®åˆ†æåçš„æœç´¢ç»“æœå’Œå¯¹è¯å†å²ç”Ÿæˆå¢å¼ºçš„LightRAGæŸ¥è¯¢"""
        context_text = ""
        conversation_summary = ""

        if messages and len(messages) > 1:
            recent_messages = messages[-6:]  # é»˜è®¤3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰

            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"

            if context_text.strip():
                conversation_summary = f"""
åŸºäºå¯¹è¯å†å²ï¼Œç”¨æˆ·å¯èƒ½å…³æ³¨çš„ä¸»é¢˜å’ŒèƒŒæ™¯ï¼š
{context_text.strip()}

è¿™è¡¨æ˜ç”¨æˆ·åœ¨æ­¤æ¬¡å¯¹è¯ä¸­çš„å…³æ³¨ç‚¹å’Œéœ€æ±‚èƒŒæ™¯ã€‚"""

        context_analysis = conversation_summary if conversation_summary else "\nè¿™æ˜¯ç”¨æˆ·çš„é¦–æ¬¡æé—®ï¼Œæ²¡æœ‰å‰åºå¯¹è¯å†å²ã€‚"

        prompt_content = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{self.valves.PROFESSIONAL_DOMAIN}é¢†åŸŸçŸ¥è¯†å›¾è°±æŸ¥è¯¢ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„LightRAGæ£€ç´¢é—®é¢˜ã€‚

**å½“å‰ç”¨æˆ·é—®é¢˜**: {original_query}

**åˆ†æåçš„æœç´¢ä¿¡æ¯**:
{analyzed_search_results}

**å¯¹è¯ä¸Šä¸‹æ–‡åˆ†æ**:{context_analysis}

**ä»»åŠ¡è¦æ±‚**:
è¯·ç»¼åˆåˆ†æä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„{self.valves.PROFESSIONAL_DOMAIN}ä¸“ä¸šLightRAGæŸ¥è¯¢ï¼š

1. **ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šè¯†åˆ«å…³é”®æŠ€æœ¯ç‚¹ã€ä¸“ä¸šæœ¯è¯­ã€åº”ç”¨åœºæ™¯
2. **è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§**ï¼šåˆ¤æ–­æœç´¢ç»“æœæ˜¯å¦å……åˆ†å›ç­”ç”¨æˆ·é—®é¢˜
3. **æ•´åˆæœ‰æ•ˆä¿¡æ¯**ï¼šæå–æœç´¢ç»“æœä¸­çš„å…³é”®æ•°æ®ã€æŠ€æœ¯è§„æ ¼ã€æ³•è§„è¦æ±‚
4. **è¡¥å……æŸ¥è¯¢é‡ç‚¹**ï¼šå¦‚æœæœç´¢ç»“æœä¸è¶³ï¼Œåœ¨æŸ¥è¯¢ä¸­æ˜ç¡®æŒ‡å‡ºéœ€è¦æ·±å…¥åˆ†æçš„ä¸“ä¸šé¢†åŸŸ
5. **æ„å»ºç»¼åˆæŸ¥è¯¢**ï¼šç”Ÿæˆèƒ½å¼•å¯¼çŸ¥è¯†å›¾è°±æ·±åº¦åˆ†æå’Œä¸“ä¸šçŸ¥è¯†è¡¥å……çš„æŸ¥è¯¢

**æŸ¥è¯¢ç­–ç•¥**ï¼š
- å¦‚æœæœç´¢ç»“æœå……åˆ†ï¼šåŸºäºæœç´¢ä¿¡æ¯æ„å»ºéªŒè¯æ€§å’Œè¡¥å……æ€§æŸ¥è¯¢
- å¦‚æœæœç´¢ç»“æœä¸è¶³ï¼šæ„å»ºæ¢ç´¢æ€§æŸ¥è¯¢ï¼Œè¦æ±‚ä»ä¸“ä¸šçŸ¥è¯†åº“ä¸­æå–æ ¸å¿ƒæ¦‚å¿µå’Œåº”ç”¨
- æŸ¥è¯¢æ ¼å¼ï¼š150-400å­—ï¼ŒåŒ…å«ä¸“ä¸šæœ¯è¯­ï¼Œé€»è¾‘æ¸…æ™°ï¼Œä¸“æ³¨å®ç”¨æ€§

**è¯·ç›´æ¥è¾“å‡ºç”Ÿæˆçš„LightRAGæŸ¥è¯¢ï¼š"""

        query_messages = [{
            "role": "user",
            "content": prompt_content
        }]

        try:
            response = self._call_openai_api(query_messages, stream=False)
            lightrag_query = response['choices'][0]['message']['content'].strip()

            if not lightrag_query or len(lightrag_query) < 20:
                lightrag_query = f"åŸºäºä»¥ä¸‹æœç´¢ä¿¡æ¯å›ç­”é—®é¢˜ï¼š{original_query}\n\næœç´¢ç»“æœï¼š{analyzed_search_results[:500]}..."

            if self.valves.DEBUG_MODE:
                print(f"ğŸ”§ LightRAGæŸ¥è¯¢ç”Ÿæˆ: {lightrag_query[:100]}...")

            return lightrag_query

        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ LightRAGæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return f"åŸºäºä»¥ä¸‹æœç´¢ä¿¡æ¯å›ç­”é—®é¢˜ï¼š{original_query}\n\næœç´¢ç»“æœï¼š{analyzed_search_results[:500]}..."

    def _stage5_query_lightrag(self, lightrag_query: str, stream: bool = False) -> Union[str, Generator[str, None, None]]:
        """ç¬¬äº”é˜¶æ®µï¼šä½¿ç”¨LightRAGè¿›è¡Œé—®ç­”"""
        mode = self.valves.LIGHTRAG_DEFAULT_MODE

        # ç»Ÿè®¡LightRAGæŸ¥è¯¢çš„è¾“å…¥token
        self._add_input_tokens(lightrag_query)

        if stream and self.valves.LIGHTRAG_ENABLE_STREAMING:
            return self._query_lightrag_streaming(lightrag_query, mode)
        else:
            result = self._query_lightrag_standard(lightrag_query, mode)
            if "error" in result:
                return f"LightRAGæŸ¥è¯¢å¤±è´¥: {result['error']}"
            response_content = result.get("response", "æœªè·å–åˆ°å“åº”å†…å®¹")
            # ç»Ÿè®¡LightRAGå“åº”çš„è¾“å‡ºtoken
            self._add_output_tokens(response_content)
            return response_content

    def _query_lightrag_standard(self, query: str, mode: str) -> dict:
        """æ ‡å‡†LightRAGæŸ¥è¯¢API"""
        url = f"{self.valves.LIGHTRAG_BASE_URL}/query"
        payload = {
            "query": query,
            "mode": mode
        }

        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def _query_lightrag_streaming(self, query: str, mode: str) -> Generator[str, None, None]:
        """æµå¼LightRAGæŸ¥è¯¢API"""
        url = f"{self.valves.LIGHTRAG_BASE_URL}/query/stream"

        payload = {
            "query": query,
            "mode": mode
        }

        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                stream=True,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()

            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8').strip()
                    if line_text:
                        try:
                            data = json.loads(line_text)
                            if 'response' in data and data['response']:
                                chunk = data['response']
                                # ç»Ÿè®¡LightRAGæµå¼è¾“å‡ºtoken
                                self._add_output_tokens(chunk)
                                yield f'data: {json.dumps({"choices": [{"delta": {"content": chunk}}]})}\n\n'
                            elif 'error' in data:
                                error_msg = data['error']
                                yield f'data: {json.dumps({"choices": [{"delta": {"content": f"é”™è¯¯: {error_msg}"}}]})}\n\n'
                        except json.JSONDecodeError:
                            continue

        except Exception as e:
            error_msg = f"LightRAGæµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}"
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict,
        __event_emitter__=None, 
        __event_call__=None,
        __user__=None
    ) -> Union[str, Generator, Iterator]:
        f"""
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æ–¹æ³• - {self.valves.PROFESSIONAL_DOMAIN}é¢†åŸŸ5é˜¶æ®µæ„å›¾è¯†åˆ«pipeline

        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            model_id: æ¨¡å‹ID
            messages: æ¶ˆæ¯å†å²
            body: è¯·æ±‚ä½“ï¼ŒåŒ…å«æµå¼è®¾ç½®å’Œç”¨æˆ·ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²æˆ–æµå¼ç”Ÿæˆå™¨
        """
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.valves.BOCHA_API_KEY:
            return "âŒ é”™è¯¯ï¼šç¼ºå°‘Bocha APIå¯†é’¥ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®BOCHA_API_KEY"
        
        if not self.valves.OPENAI_API_KEY:
            return "âŒ é”™è¯¯ï¼šç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®OPENAI_API_KEY"

        if not user_message.strip():
            return "âŒ è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹"

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if self.valves.DEBUG_MODE and "user" in body:
            print("=" * 50)
            print(f"ç”¨æˆ·: {body['user']['name']} ({body['user']['id']})")
            print(f"æŸ¥è¯¢å†…å®¹: {user_message}")
            print(f"æµå¼å“åº”: {body.get('stream', False)}")
            print(f"å¯ç”¨æµå¼: {self.valves.ENABLE_STREAMING}")
            print("=" * 50)

        # æ ¹æ®æ˜¯å¦å¯ç”¨æµå¼å“åº”é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if body.get("stream", False) and self.valves.ENABLE_STREAMING:
            return self._stream_response(user_message, messages)
        else:
            return self._non_stream_response(user_message, messages)

    def _stream_response(self, query: str, messages: List[dict]) -> Generator[str, None, None]:
        """æµå¼å“åº”å¤„ç† - 5é˜¶æ®µæ„å›¾è¯†åˆ«pipeline"""
        try:
            # æµå¼å¼€å§‹æ¶ˆæ¯
            yield f'data: {json.dumps({"choices": [{"delta": {}, "finish_reason": None}]})}\n\n'

            # ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«
            intent_msg = {
                'choices': [{
                    'delta': {
                        'content': "**ğŸ§  ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«åˆ¤æ–­**\næ­£åœ¨åˆ†æé—®é¢˜æ„å›¾..."
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(intent_msg)}\n\n"

            intent_result, can_direct_answer, need_knowledge_base = self._stage1_intent_recognition(query, messages)

            if can_direct_answer:
                # ç›´æ¥å›ç­”ï¼Œä¸éœ€è¦æœç´¢
                direct_answer_msg = {
                    'choices': [{
                        'delta': {
                            'content': f"\nâœ… æ„å›¾è¯†åˆ«ï¼šå¯ç›´æ¥å›ç­”\n\n**ğŸ“ ç›´æ¥å›ç­”ï¼š**\n{intent_result}"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(direct_answer_msg)}\n\n"

                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_stats = self._get_token_stats()
                token_info_msg = {
                    'choices': [{
                        'delta': {
                            'content': f"\n\n---\n**Tokenæ¶ˆè€—ç»Ÿè®¡**\n- è¾“å…¥Token: {token_stats['input_tokens']:,}\n- è¾“å‡ºToken: {token_stats['output_tokens']:,}\n- æ€»Token: {token_stats['total_tokens']:,}"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(token_info_msg)}\n\n"

                yield "data: [DONE]\n\n"
                return

            # éœ€è¦æœç´¢æˆ–çŸ¥è¯†åº“å¤„ç†
            intent_result_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\nâœ… æ„å›¾è¯†åˆ«å®Œæˆ\nç­–ç•¥: {'éœ€è¦çŸ¥è¯†åº“å¤„ç†' if need_knowledge_base else 'éœ€è¦ç½‘ç»œæœç´¢'}\nåŸå› : {intent_result}\n"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(intent_result_msg)}\n\n"

            # ç¬¬äºŒé˜¶æ®µï¼šé—®é¢˜ä¼˜åŒ–
            optimize_msg = {
                'choices': [{
                    'delta': {
                        'content': "\n**ğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šé—®é¢˜ä¼˜åŒ–**\næ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢é—®é¢˜..."
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(optimize_msg)}\n\n"

            optimized_query = self._stage2_optimize_query(query, messages)

            optimize_result_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\nâœ… é—®é¢˜ä¼˜åŒ–å®Œæˆ\nä¼˜åŒ–åæŸ¥è¯¢: {optimized_query}\n"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(optimize_result_msg)}\n\n"

            # ç¬¬ä¸‰é˜¶æ®µï¼šæœç´¢
            search_msg = {
                'choices': [{
                    'delta': {
                        'content': "\n**ğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šBochaæœç´¢**\næ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯..."
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(search_msg)}\n\n"

            search_results, search_status, search_links = self._stage3_search(optimized_query)

            search_result_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\n{search_status}\n"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(search_result_msg)}\n\n"

            # ç¬¬3.5é˜¶æ®µï¼šåˆ†ææœç´¢ç»“æœ
            analyze_msg = {
                'choices': [{
                    'delta': {
                        'content': "\n**ğŸ§  ç¬¬3.5é˜¶æ®µï¼šæœç´¢ç»“æœåˆ†æ**\næ­£åœ¨åˆ†ææœç´¢ç»“æœï¼Œå»é™¤æ— ç”¨ä¿¡æ¯..."
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(analyze_msg)}\n\n"

            analyzed_search_results = self._stage3_5_analyze_search_results(query, search_results, messages)

            analyze_result_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\nâœ… æœç´¢ç»“æœåˆ†æå®Œæˆ\n"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(analyze_result_msg)}\n\n"

            if need_knowledge_base:
                # éœ€è¦çŸ¥è¯†åº“å¤„ç†
                # ç¬¬å››é˜¶æ®µï¼šç”ŸæˆLightRAGæŸ¥è¯¢
                lightrag_gen_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ§  ç¬¬å››é˜¶æ®µï¼šç”ŸæˆLightRAGæŸ¥è¯¢**\næ­£åœ¨åˆ†ææœç´¢ç»“æœå¹¶ç”Ÿæˆå¢å¼ºæŸ¥è¯¢..."
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(lightrag_gen_msg)}\n\n"

                lightrag_query = self._stage4_generate_lightrag_query(query, analyzed_search_results, messages)

                lightrag_gen_result_msg = {
                    'choices': [{
                        'delta': {
                            'content': f"\nâœ… LightRAGæŸ¥è¯¢ç”Ÿæˆå®Œæˆ\nå¢å¼ºæŸ¥è¯¢: {lightrag_query[:100]}{'...' if len(lightrag_query) > 100 else ''}\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(lightrag_gen_result_msg)}\n\n"

                # ç¬¬äº”é˜¶æ®µï¼šLightRAGé—®ç­”
                lightrag_answer_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ’­ ç¬¬äº”é˜¶æ®µï¼šLightRAGé—®ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(lightrag_answer_msg)}\n\n"

                # æµå¼ç”ŸæˆLightRAGå›ç­”
                try:
                    for chunk_data in self._stage5_query_lightrag(lightrag_query, stream=True):
                        yield chunk_data
                except Exception as e:
                    error_msg = {
                        'choices': [{
                            'delta': {
                                'content': f"\nâŒ LightRAGæŸ¥è¯¢å¤±è´¥: {str(e)}"
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(error_msg)}\n\n"
            else:
                # åªéœ€è¦æœç´¢ç»“æœå›ç­”
                search_answer_msg = {
                    'choices': [{
                        'delta': {
                            'content': f"\n**ğŸ“ åŸºäºæœç´¢ç»“æœçš„å›ç­”ï¼š**\n\n{analyzed_search_results}"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(search_answer_msg)}\n\n"

            # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
            token_stats = self._get_token_stats()
            token_info_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\n\n---\n**Tokenæ¶ˆè€—ç»Ÿè®¡**\n- è¾“å…¥Token: {token_stats['input_tokens']:,}\n- è¾“å‡ºToken: {token_stats['output_tokens']:,}\n- æ€»Token: {token_stats['total_tokens']:,}"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(token_info_msg)}\n\n"

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Stream error: {e}")
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"

    def _non_stream_response(self, query: str, messages: List[dict]) -> str:
        """éæµå¼å“åº”å¤„ç† - 5é˜¶æ®µæ„å›¾è¯†åˆ«pipeline"""
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«
            intent_result, can_direct_answer, need_knowledge_base = self._stage1_intent_recognition(query, messages)

            if can_direct_answer:
                # ç›´æ¥å›ç­”ï¼Œä¸éœ€è¦æœç´¢
                response_parts = []
                response_parts.append(f"**ğŸ§  ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«åˆ¤æ–­**\nç­–ç•¥: å¯ç›´æ¥å›ç­”\nåŸå› : {intent_result}")
                response_parts.append(f"\n**ğŸ“ ç›´æ¥å›ç­”ï¼š**\n{intent_result}")

                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_stats = self._get_token_stats()
                token_info = f"\n\n---\n**Tokenæ¶ˆè€—ç»Ÿè®¡**\n- è¾“å…¥Token: {token_stats['input_tokens']:,}\n- è¾“å‡ºToken: {token_stats['output_tokens']:,}\n- æ€»Token: {token_stats['total_tokens']:,}"

                return "\n".join(response_parts) + token_info

            # ç¬¬äºŒé˜¶æ®µï¼šé—®é¢˜ä¼˜åŒ–
            optimized_query = self._stage2_optimize_query(query, messages)

            # ç¬¬ä¸‰é˜¶æ®µï¼šæœç´¢
            search_results, search_status, search_links = self._stage3_search(optimized_query)

            # ç¬¬3.5é˜¶æ®µï¼šåˆ†ææœç´¢ç»“æœ
            analyzed_search_results = self._stage3_5_analyze_search_results(query, search_results, messages)

            response_parts = []
            response_parts.append(f"**ğŸ§  ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾è¯†åˆ«åˆ¤æ–­**\nç­–ç•¥: {'éœ€è¦çŸ¥è¯†åº“å¤„ç†' if need_knowledge_base else 'éœ€è¦ç½‘ç»œæœç´¢'}\nåŸå› : {intent_result}")
            response_parts.append(f"\n**ğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šé—®é¢˜ä¼˜åŒ–**\nåŸå§‹é—®é¢˜: {query}\nä¼˜åŒ–åæŸ¥è¯¢: {optimized_query}")
            response_parts.append(f"\n**ğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šBochaæœç´¢**\n{search_status}")
            response_parts.append(f"\n**ğŸ§  ç¬¬3.5é˜¶æ®µï¼šæœç´¢ç»“æœåˆ†æ**\nâœ… æœç´¢ç»“æœåˆ†æå®Œæˆ")

            if search_results and "æœç´¢é”™è¯¯" not in search_results:
                # æ˜¾ç¤ºåˆ†æåçš„æœç´¢ç»“æœæ‘˜è¦
                lines = analyzed_search_results.split('\n')
                summary_lines = []
                for line in lines[:8]:  # åªæ˜¾ç¤ºå‰8è¡Œ
                    if line.strip():
                        summary_lines.append(line)
                if len(lines) > 8:
                    summary_lines.append("...")
                response_parts.append(f"\nåˆ†æåæœç´¢æ‘˜è¦:\n" + "\n".join(summary_lines))

            if need_knowledge_base:
                # ç¬¬å››é˜¶æ®µï¼šç”ŸæˆLightRAGæŸ¥è¯¢
                lightrag_query = self._stage4_generate_lightrag_query(query, analyzed_search_results, messages)

                # ç¬¬äº”é˜¶æ®µï¼šLightRAGé—®ç­”
                final_answer = self._stage5_query_lightrag(lightrag_query, stream=False)

                response_parts.append(f"\n**ğŸ§  ç¬¬å››é˜¶æ®µï¼šLightRAGæŸ¥è¯¢ç”Ÿæˆ**\nå¢å¼ºæŸ¥è¯¢: {lightrag_query[:150]}{'...' if len(lightrag_query) > 150 else ''}")
                response_parts.append(f"\n**ğŸ’­ ç¬¬äº”é˜¶æ®µï¼šLightRAGé—®ç­”**\n{final_answer}")
            else:
                # åªåŸºäºæœç´¢ç»“æœå›ç­”
                response_parts.append(f"\n**ğŸ“ åŸºäºæœç´¢ç»“æœçš„å›ç­”ï¼š**\n\n{analyzed_search_results}")

            # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
            token_stats = self._get_token_stats()
            token_info = f"\n\n---\n**Tokenæ¶ˆè€—ç»Ÿè®¡**\n- è¾“å…¥Token: {token_stats['input_tokens']:,}\n- è¾“å‡ºToken: {token_stats['output_tokens']:,}\n- æ€»Token: {token_stats['total_tokens']:,}"

            # æ·»åŠ é…ç½®ä¿¡æ¯
            config_info = f"\n\n**é…ç½®ä¿¡æ¯**\n- ä¸“ä¸šé¢†åŸŸ: {self.valves.PROFESSIONAL_DOMAIN}\n- æœç´¢ç»“æœæ•°é‡: {self.valves.BOCHA_SEARCH_COUNT}\n- LightRAGæ¨¡å¼: {self.valves.LIGHTRAG_DEFAULT_MODE}\n- æ„å›¾è¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼: {self.valves.INTENT_CONFIDENCE_THRESHOLD}\n- æ¨¡å‹: {self.valves.OPENAI_MODEL}"

            return "\n".join(response_parts) + token_info + config_info

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Non-stream error: {e}")
            return error_msg
