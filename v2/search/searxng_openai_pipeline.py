"""
title: SearxNG Search OpenAI Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: A 3-stage pipeline: 1) LLM query optimization, 2) Web search using SearxNG API, 3) AI-enhanced Q&A with OpenAI API
requirements: requests, pydantic
"""

import os
import json
import requests
import time
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel


class HistoryContextManager:
    """å†å²ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç»Ÿä¸€å¤„ç†å†å²ä¼šè¯ä¿¡æ¯"""
    
    DEFAULT_HISTORY_TURNS = 3  # é»˜è®¤3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
    
    @classmethod
    def extract_recent_context(cls, messages: List[dict], max_turns: int = None) -> str:
        """
        æå–æœ€è¿‘çš„å†å²ä¼šè¯ä¸Šä¸‹æ–‡
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_turns: æœ€å¤§è½®æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3è½®
        
        Returns:
            æ ¼å¼åŒ–çš„å†å²ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not messages or len(messages) < 2:
            return ""
            
        max_turns = max_turns or cls.DEFAULT_HISTORY_TURNS
        max_messages = max_turns * 2  # æ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
        
        recent_messages = messages[-max_messages:] if len(messages) > max_messages else messages
        context_text = ""
        
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                context_text += f"ç”¨æˆ·: {content}\n"
            elif role == "assistant":
                context_text += f"åŠ©æ‰‹: {content}\n"
                
        return context_text.strip()
    
    @classmethod
    def format_answer_prompt(cls,
                           original_query: str,
                           search_results: str,
                           messages: List[dict],
                           max_turns: int = None) -> str:
        """
        æ ¼å¼åŒ–ç­”æ¡ˆç”Ÿæˆæç¤ºæ¨¡æ¿
        
        Args:
            original_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            search_results: æœç´¢ç»“æœ
            messages: å†å²æ¶ˆæ¯
            max_turns: æœ€å¤§å†å²è½®æ¬¡
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºæ–‡æœ¬
        """
        context_text = cls.extract_recent_context(messages, max_turns)
        
        prompt_content = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æœç´¢ç»“æœ:
{search_results}

å½“å‰é—®é¢˜: {original_query}"""

        if context_text.strip():
            prompt_content = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†å²å’Œæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å¯¹è¯å†å²:
{context_text}

æœç´¢ç»“æœ:
{search_results}

å½“å‰é—®é¢˜: {original_query}"""

        prompt_content += """

è¯·æ ¹æ®æœç´¢ç»“æœæä¾›å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰ç”¨çš„å›ç­”ï¼š
1. ç»“åˆå¯¹è¯å†å²å’Œæœç´¢ç»“æœï¼Œç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚
2. åŸºäºæœç´¢åˆ°çš„ä¿¡æ¯æä¾›å‡†ç¡®å›ç­”
3. å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å“ªäº›éƒ¨åˆ†éœ€è¦æ›´å¤šä¿¡æ¯
4. ä¿æŒå›ç­”çš„ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨é€‚å½“çš„æ ¼å¼
5. åœ¨å¿…è¦æ—¶æä¾›ç›¸å…³çš„é“¾æ¥æˆ–å‚è€ƒèµ„æ–™"""

        return prompt_content


class Pipeline:
    class Valves(BaseModel):
        # SearxNGæœç´¢APIé…ç½®
        SEARXNG_URL: str
        SEARXNG_SEARCH_COUNT: int
        SEARXNG_LANGUAGE: str
        SEARXNG_TIMEOUT: int
        SEARXNG_CATEGORIES: str
        SEARXNG_TIME_RANGE: str
        SEARXNG_SAFESEARCH: int

        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int

    def __init__(self):
        self.name = "SearxNG Search OpenAI Pipeline"
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        self.valves = self.Valves(
            **{
                # SearxNGæœç´¢é…ç½®
                "SEARXNG_URL": os.getenv("SEARXNG_URL", "http://117.50.252.245:8081"),
                "SEARXNG_SEARCH_COUNT": int(os.getenv("SEARXNG_SEARCH_COUNT", "8")),
                "SEARXNG_LANGUAGE": os.getenv("SEARXNG_LANGUAGE", "zh-CN"),
                "SEARXNG_TIMEOUT": int(os.getenv("SEARXNG_TIMEOUT", "15")),
                "SEARXNG_CATEGORIES": os.getenv("SEARXNG_CATEGORIES", "general"),
                "SEARXNG_TIME_RANGE": os.getenv("SEARXNG_TIME_RANGE", "month"),  # day, week, month, year
                "SEARXNG_SAFESEARCH": int(os.getenv("SEARXNG_SAFESEARCH", "0")),  # 0, 1, 2
                
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
            }
        )

    async def on_startup(self):
        print(f"SearxNG Search OpenAI Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            
        # æµ‹è¯•SearxNG APIè¿æ¥
        try:
            print("ğŸ”§ å¼€å§‹æµ‹è¯•SearxNG APIè¿æ¥...")
            test_response = self._search_searxng("test query", count=1)
            if test_response and "results" in test_response:
                print("âœ… SearxNGæœç´¢APIè¿æ¥æˆåŠŸ")
            else:
                print("âš ï¸ SearxNGæœç´¢APIæµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ SearxNGæœç´¢APIè¿æ¥å¤±è´¥: {e}")

    async def on_shutdown(self):
        print(f"SearxNG Search OpenAI Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """
        ç®€å•çš„tokenä¼°ç®—å‡½æ•°ï¼ŒåŸºäºå­—ç¬¦æ•°ä¼°ç®—
        ä¸­æ–‡å­—ç¬¦æŒ‰1ä¸ªtokenè®¡ç®—ï¼Œè‹±æ–‡å•è¯æŒ‰å¹³å‡1.3ä¸ªtokenè®¡ç®—
        """
        if not text:
            return 0

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')

        # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])

        # ä¼°ç®—tokenæ•°ï¼šä¸­æ–‡å­—ç¬¦1:1ï¼Œè‹±æ–‡å•è¯1:1.3
        estimated_tokens = chinese_chars + int(english_words * 1.3)

        return max(estimated_tokens, 1)  # è‡³å°‘è¿”å›1ä¸ªtoken

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _search_searxng(self, query: str, count: int = None, max_retries: int = 2) -> dict:
        """è°ƒç”¨SearxNG APIè¿›è¡Œæœç´¢ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        url = f"{self.valves.SEARXNG_URL}/search"

        # å‚æ•°éªŒè¯
        search_count = count or self.valves.SEARXNG_SEARCH_COUNT
        if search_count < 1:
            search_count = 1

        # æ„å»ºè¯·æ±‚å‚æ•° - ä¸æŒ‡å®šç‰¹å®šæœç´¢å¼•æ“ï¼Œè®©SearXNGè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„å¼•æ“
        params = {
            'q': query.strip(),
            'format': 'json',
            'language': self.valves.SEARXNG_LANGUAGE,
            'safesearch': self.valves.SEARXNG_SAFESEARCH,
            'categories': self.valves.SEARXNG_CATEGORIES
        }

        # æ·»åŠ å¯é€‰å‚æ•°
        if self.valves.SEARXNG_TIME_RANGE:
            params['time_range'] = self.valves.SEARXNG_TIME_RANGE

        # é‡è¯•é€»è¾‘
        for attempt in range(max_retries + 1):
            try:
                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                if self.valves.DEBUG_MODE:
                    print(f"ğŸ” SearxNGæœç´¢è°ƒè¯•ä¿¡æ¯ (å°è¯• {attempt + 1}/{max_retries + 1}):")
                    print(f"   URL: {url}")
                    print(f"   è¯·æ±‚å‚æ•°: {json.dumps(params, ensure_ascii=False, indent=2)}")

                response = requests.get(
                    url,
                    params=params,
                    timeout=self.valves.SEARXNG_TIMEOUT
                )

                # æ·»åŠ å“åº”è°ƒè¯•ä¿¡æ¯
                if self.valves.DEBUG_MODE:
                    print(f"   å“åº”çŠ¶æ€ç : {response.status_code}")
                    if response.status_code != 200:
                        print(f"   å“åº”å†…å®¹: {response.text}")

                response.raise_for_status()
                result = response.json()

                if self.valves.DEBUG_MODE:
                    print(f"   å“åº”æˆåŠŸï¼Œæ•°æ®é•¿åº¦: {len(str(result))}")
                    if isinstance(result, dict):
                        print(f"   æ‰¾åˆ°ç»“æœæ•°é‡: {result.get('number_of_results', 0)}")

                return result

            except requests.exceptions.Timeout:
                if attempt < max_retries:
                    if self.valves.DEBUG_MODE:
                        print(f"âš ï¸ æœç´¢è¶…æ—¶ï¼Œ{2}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries + 1})")
                    time.sleep(2)
                    continue
                else:
                    error_msg = "SearxNGæœç´¢è¶…æ—¶ï¼Œå¯èƒ½æ˜¯æœç´¢å¼•æ“è¿æ¥é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•"
                    if self.valves.DEBUG_MODE:
                        print(f"âŒ {error_msg}")
                        print("   æç¤ºï¼šå¦‚æœæŒç»­è¶…æ—¶ï¼Œå¯èƒ½æ˜¯åç«¯æœç´¢å¼•æ“ä¸å¯ç”¨")
                    return {"error": error_msg}
            except Exception as e:
                if attempt < max_retries:
                    if self.valves.DEBUG_MODE:
                        print(f"âš ï¸ æœç´¢å‡ºé”™ï¼Œ{2}ç§’åé‡è¯•: {str(e)}")
                    time.sleep(2)
                    continue
                else:
                    # å¤„ç†å…¶ä»–å¼‚å¸¸
                    break

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯
        return {"error": "SearxNGæœç´¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"}

    def _format_search_results(self, search_response: dict) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºæ–‡æœ¬"""
        if "error" in search_response:
            return f"æœç´¢é”™è¯¯: {search_response['error']}"

        # æ£€æŸ¥å“åº”ç»“æ„
        if not isinstance(search_response, dict):
            return "æœç´¢å“åº”æ ¼å¼é”™è¯¯"

        # æ£€æŸ¥SearxNG APIçš„å“åº”ç»“æ„
        results = search_response.get("results", [])
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯"

        formatted_results = []
        total_results = search_response.get("number_of_results", 0)

        # æ·»åŠ æœç´¢å®Œæˆæç¤ºå’Œç»Ÿè®¡ä¿¡æ¯
        if total_results > 0:
            formatted_results.append("æœç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            formatted_results.append(f"æ‰¾åˆ°çº¦ {total_results:,} æ¡ç›¸å…³ç»“æœ\n")
        else:
            formatted_results.append("æœç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n")

        # é™åˆ¶æ˜¾ç¤ºçš„ç»“æœæ•°é‡
        display_count = min(len(results), self.valves.SEARXNG_SEARCH_COUNT)

        for i, result in enumerate(results[:display_count], 1):
            title = result.get("title", "æ— æ ‡é¢˜").strip()
            url = result.get("url", "").strip()
            content = result.get("content", "").strip()

            # æ¸…ç†å’Œæˆªæ–­å†…å®¹
            if content:
                # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                content = ' '.join(content.split())
                # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­å¹¶æ·»åŠ çœç•¥å·
                if len(content) > 200:
                    content = content[:200] + "..."

            # æ ¼å¼åŒ–å•ä¸ªç»“æœä¸ºmarkdownæ ¼å¼
            result_text = f"**[{i}] {title}**\n"
            result_text += f"é“¾æ¥: {url}\n"
            if content:
                result_text += f"æ‘˜è¦: {content}\n"

            formatted_results.append(result_text)

        return "\n\n".join(formatted_results)

    def _optimize_search_query(self, user_message: str, messages: List[dict]) -> str:
        """ä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼Œè€ƒè™‘å†å²å¯¹è¯ä¸Šä¸‹æ–‡"""
        # æå–å†å²ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•æ˜¾ç¤º
        context_text = HistoryContextManager.extract_recent_context(messages, self.valves.HISTORY_TURNS)
        
        if self.valves.DEBUG_MODE and context_text:
            print(f"ğŸ”„ å†å²ä¸Šä¸‹æ–‡({self.valves.HISTORY_TURNS}è½®):\n{context_text[:200]}...")

        # ç›®å‰ç®€å•è¿”å›åŸæŸ¥è¯¢ï¼Œå†å²ä¸Šä¸‹æ–‡åœ¨ç­”æ¡ˆç”Ÿæˆé˜¶æ®µä½¿ç”¨
        return user_message

    def _stage1_search(self, query: str) -> tuple:
        """ç¬¬ä¸€é˜¶æ®µï¼šæ‰§è¡Œæœç´¢å¹¶è·å–ç»“æœ"""
        if self.valves.DEBUG_MODE:
            print(f"ğŸ” æ‰§è¡Œæœç´¢: {query}")

        # éªŒè¯æŸ¥è¯¢å‚æ•°
        if not query or not query.strip():
            return "æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º", False

        search_response = self._search_searxng(query.strip())

        if "error" in search_response:
            return f"æœç´¢é”™è¯¯: {search_response['error']}", False

        # æ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦æœ‰æ•ˆ
        if not search_response.get("results"):
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯", False

        formatted_results = self._format_search_results(search_response)

        if self.valves.DEBUG_MODE:
            print(f"âœ… æœç´¢å®Œæˆï¼Œç»“æœé•¿åº¦: {len(formatted_results)}")
            print(f"   æ‰¾åˆ°ç»“æœæ•°é‡: {len(search_response.get('results', []))}")

        return formatted_results, True

    def _stage2_generate_answer(self, query: str, search_results: str, messages: List[dict] = None, stream: bool = False) -> Union[str, Generator]:
        """ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨OpenAIç”Ÿæˆå›ç­”"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŸºäºæœç´¢ç»“æœå›ç­”é—®é¢˜çš„AIåŠ©æ‰‹ã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä¸»è¦åŸºäºæä¾›çš„æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜
2. ç»“åˆå¯¹è¯å†å²ç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚å’Œä¸Šä¸‹æ–‡
3. å¦‚æœæœç´¢ç»“æœåŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”
4. å¼•ç”¨æœç´¢ç»“æœæ—¶ä½¿ç”¨ç¼–å·ï¼ˆå¦‚[1]ã€[2]ç­‰ï¼‰æ¥æ ‡æ³¨ä¿¡æ¯æ¥æº
5. å¦‚æœæœç´¢ç»“æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜å¹¶æä¾›å¯èƒ½çš„å»ºè®®
6. ä¸è¦ç¼–é€ æˆ–æ¨æµ‹æœç´¢ç»“æœä¸­æ²¡æœ‰çš„ä¿¡æ¯
7. å›ç­”è¦ç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
8. å¦‚æœå‘ç°æœç´¢ç»“æœä¸­æœ‰çŸ›ç›¾ä¿¡æ¯ï¼Œè¯·æŒ‡å‡ºå¹¶è¯´æ˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦è‡ªç„¶æµç•…ã€‚"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_answer_prompt(
            original_query=query,
            search_results=search_results,
            messages=messages or [],
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": stream
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_input_tokens(system_prompt)
        self._add_input_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print("ğŸ¤– è°ƒç”¨OpenAI APIç”Ÿæˆå›ç­”")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æµå¼å“åº”: {stream}")
        
        try:
            if stream:
                # æµå¼æ¨¡å¼ï¼šè¿”å›ç”Ÿæˆå™¨
                for chunk in self._stream_openai_response(url, headers, payload):
                    yield chunk
            else:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT
                )
                response.raise_for_status()
                result = response.json()

                answer = result["choices"][0]["message"]["content"]

                # æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡
                self._add_output_tokens(answer)

                if self.valves.DEBUG_MODE:
                    print(f"âœ… ç”Ÿæˆå›ç­”å®Œæˆï¼Œé•¿åº¦: {len(answer)}")
                    print(f"   Tokenç»Ÿè®¡: {self._get_token_stats()}")

                return answer

        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            if stream:
                yield error_msg
            else:
                return error_msg

    def _stream_openai_response(self, url, headers, payload):
        """æµå¼å¤„ç†OpenAIå“åº”"""
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_output_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            if self.valves.DEBUG_MODE:
                print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(collected_content)}")
                print(f"   Tokenç»Ÿè®¡: {self._get_token_stats()}")
                
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°ï¼Œå¤„ç†ç”¨æˆ·è¯·æ±‚"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        # é˜¶æ®µ1ï¼šæŸ¥è¯¢ä¼˜åŒ–
        yield "ğŸ”„ **é˜¶æ®µ1**: æ­£åœ¨ä¼˜åŒ–æœç´¢æŸ¥è¯¢..."
        optimized_query = self._optimize_search_query(user_message, messages)

        if optimized_query != user_message:
            yield f"âœ… æŸ¥è¯¢å·²ä¼˜åŒ–: `{optimized_query}`\n"
        else:
            yield f"âœ… ä½¿ç”¨åŸå§‹æŸ¥è¯¢: `{user_message}`\n"

        # é˜¶æ®µ2ï¼šæ‰§è¡Œæœç´¢
        yield "ğŸ” **é˜¶æ®µ2**: æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯..."
        search_results, search_success = self._stage1_search(optimized_query)

        if not search_success:
            # å¦‚æœä¼˜åŒ–åçš„æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            if optimized_query != user_message:
                yield "âš ï¸ ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•åŸå§‹æŸ¥è¯¢..."
                search_results, search_success = self._stage1_search(user_message)

            if not search_success:
                # å¦‚æœæœç´¢å®Œå…¨å¤±è´¥ï¼Œæä¾›åŸºäºOpenAIçš„å›ç­”
                if self.valves.OPENAI_API_KEY:
                    yield "âŒ æœç´¢å¤±è´¥ï¼Œå°†åŸºäºAIçŸ¥è¯†å›ç­”\n"
                    yield "ğŸ¤– **é˜¶æ®µ3**: æ­£åœ¨ç”ŸæˆåŸºäºçŸ¥è¯†çš„å›ç­”..."
                    fallback_prompt = f"ç”¨æˆ·é—®é¢˜: {user_message}\n\nç”±äºæ— æ³•è·å–æœç´¢ç»“æœï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¯´æ˜è¿™æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ï¼Œå¯èƒ½ä¸æ˜¯æœ€æ–°ä¿¡æ¯ã€‚"
                    stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING

                    if stream_mode:
                        for chunk in self._stage2_generate_answer(user_message, fallback_prompt, messages, stream=True):
                            yield chunk
                    else:
                        result = self._stage2_generate_answer(user_message, fallback_prompt, messages, stream=False)
                        yield result
                        # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                        token_info = self._get_token_stats()
                        yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
                    return
                else:
                    yield f"âŒ æœç´¢å¤±è´¥ä¸”æœªé…ç½®OpenAI APIå¯†é’¥: {search_results}"
                    return

        # æœç´¢æˆåŠŸï¼Œæ˜¾ç¤ºæœç´¢ç»“æœ
        yield "âœ… æœç´¢å®Œæˆ\n"
        yield search_results
        yield "\n"

        # é˜¶æ®µ3ï¼šç”ŸæˆAIå›ç­”
        yield "ğŸ¤– **é˜¶æ®µ3**: æ­£åœ¨åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”..."
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING

        try:
            if stream_mode:
                # æµå¼æ¨¡å¼
                for chunk in self._stage2_generate_answer(user_message, search_results, messages, stream=True):
                    yield chunk
                # æµå¼æ¨¡å¼ç»“æŸåæ·»åŠ tokenç»Ÿè®¡
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
            else:
                # éæµå¼æ¨¡å¼
                result = self._stage2_generate_answer(user_message, search_results, messages, stream=False)
                yield result
                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"

        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg