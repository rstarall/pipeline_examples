"""
ç¼–å†™åŸºäºserper APIçš„è”ç½‘æœç´¢pipeline,æ„å›¾è¯†åˆ«jsonè¿”å›
1.å®šä¹‰å¹¶è¯†åˆ«ç½‘ç«™ç±»å‹
  wiki:urlåŒ¹é…wikipedia
  ç™¾åº¦ç™¾ç§‘:urlåŒ¹é…baike.baidu
  MBAæ™ºåº“ç™¾ç§‘:wiki.mbalib
  è®ºæ–‡:urlåŒ¹é…arxiv,doi,pdf
  å…¶ä»–:urlåŒ¹é…å…¶ä»–
2.é˜¶æ®µ1:
    - æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå½“å‰é—®é¢˜è¾“å‡ºä¼˜åŒ–åçš„é—®é¢˜(jsonæ ¼å¼ï¼Œä¸­è‹±ç‰ˆæœ¬):
    {
    "optimized_question_cn": "ä¼˜åŒ–åçš„é—®é¢˜",
    "optimized_question_en": "ä¼˜åŒ–åçš„é—®é¢˜(è‹±æ–‡)",
    }
    - processå±•ç¤ºä¼˜åŒ–åçš„é—®é¢˜
3.é˜¶æ®µ2:
  - æ ¹æ®ä¼˜åŒ–åçš„é—®é¢˜è¿›è¡Œè”ç½‘æœç´¢,ä¸­æ–‡è‹±æ–‡å„10ä¸ªç»“æœ
  - æ ¹æ®ç»“æœurlè¯†åˆ«ç½‘ç«™ç±»å‹(è¯†åˆ«ä¸äº†åˆ™ä¸ºå…¶ä»–),å¤„ç†ä¸ºjsonæ ¼å¼
  - å°†20ä¸ªç»“æœçš„jsonè¾“å…¥LLMï¼Œè®©LLMæ ¹æ®é—®é¢˜å’Œç»“æœé€‰æ‹©æœ€æ°å½“çš„10(å¯é…ç½®)ä¸ªç½‘é¡µåœ°å€
  - processå±•ç¤ºä¿¡æ¯æº
4.é˜¶æ®µ4:
   - æ ¹æ®é˜¶æ®µ3çš„10ä¸ªç½‘é¡µåœ°å€è¿›è¡Œè”ç½‘å†…å®¹è·å–(åç¨‹å¹¶å‘)ï¼Œè·å–åˆ°çš„æ˜¯html
   - LLMè¿›è¡Œå†…å®¹è§£æï¼Œè¾“å‡ºjsonæ ¼å¼
   - processå±•ç¤ºä¿¡æ¯æº
5.é˜¶æ®µ5: æ ¹æ®é˜¶æ®µ4çš„å†…å®¹å’Œç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæœ€ç»ˆçš„å›ç­”ï¼Œç­”æ¡ˆè¦å¿ äºä¿¡æ¯æº,å†…å®¹ä¸°å¯Œï¼Œå‡†ç¡®

å‚è€ƒæ–‡ä»¶:
apiå‚è€ƒ:v2\search\test\serper_test.py
å¤„ç†å‚è€ƒ:v2\search\searxng_openai_pipeline.py
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
import re
from typing import List, Union, Generator, Iterator, Dict, Any
from pydantic import BaseModel
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import concurrent.futures

# åç«¯é˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "query_optimization": "é—®é¢˜ä¼˜åŒ–",
    "web_search": "ç½‘ç»œæœç´¢",
    "content_fetch": "å†…å®¹è·å–", 
    "final_answer": "ç”Ÿæˆæœ€ç»ˆå›ç­”",
}

STAGE_GROUP = {
    "query_optimization": "stage_group_1",
    "web_search": "stage_group_2", 
    "content_fetch": "stage_group_3",
    "final_answer": "stage_group_4",
}

class Pipeline:
    class Valves(BaseModel):
        # Serper APIé…ç½®
        SERPER_API_KEY: str
        SERPER_BASE_URL: str
        SERPER_SEARCH_COUNT: int
        SERPER_TIMEOUT: int
        
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        SELECTED_URLS_COUNT: int
        CONTENT_FETCH_TIMEOUT: int
        MAX_CONTENT_LENGTH: int
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int

    def __init__(self):
        self.name = "Serper Search OpenAI Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        self.valves = self.Valves(
            **{
                # Serper APIé…ç½®
                "SERPER_API_KEY": os.getenv("SERPER_API_KEY", "b981da4c22e8e472ff3840e9e975b5b9827f8795"),
                "SERPER_BASE_URL": os.getenv("SERPER_BASE_URL", "https://google.serper.dev"),
                "SERPER_SEARCH_COUNT": int(os.getenv("SERPER_SEARCH_COUNT", "10")),
                "SERPER_TIMEOUT": int(os.getenv("SERPER_TIMEOUT", "15")),
                
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "SELECTED_URLS_COUNT": int(os.getenv("SELECTED_URLS_COUNT", "10")),
                "CONTENT_FETCH_TIMEOUT": int(os.getenv("CONTENT_FETCH_TIMEOUT", "10")),
                "MAX_CONTENT_LENGTH": int(os.getenv("MAX_CONTENT_LENGTH", "5000")),
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
            }
        )

    async def on_startup(self):
        print(f"Serper Search OpenAI Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        if not self.valves.SERPER_API_KEY:
            print("âŒ ç¼ºå°‘Serper APIå¯†é’¥ï¼Œè¯·è®¾ç½®SERPER_API_KEYç¯å¢ƒå˜é‡")

    async def on_shutdown(self):
        print(f"Serper Search OpenAI Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _identify_website_type(self, url: str) -> str:
        """è¯†åˆ«ç½‘ç«™ç±»å‹"""
        url_lower = url.lower()
        
        if 'wikipedia' in url_lower:
            return 'wiki'
        elif 'baike.baidu' in url_lower:
            return 'ç™¾åº¦ç™¾ç§‘'
        elif 'wiki.mbalib' in url_lower:
            return 'MBAæ™ºåº“ç™¾ç§‘'
        elif any(keyword in url_lower for keyword in ['arxiv', 'doi', '.pdf']):
            return 'è®ºæ–‡'
        else:
            return 'å…¶ä»–'

    def _search_serper(self, query: str, gl: str = "cn", hl: str = "zh-cn") -> dict:
        """è°ƒç”¨Serper APIè¿›è¡Œæœç´¢"""
        url = f"{self.valves.SERPER_BASE_URL}/search"
        
        payload = {
            "q": query,
            "num": self.valves.SERPER_SEARCH_COUNT,
            "gl": gl,  # åœ°ç†ä½ç½®
            "hl": hl   # è¯­è¨€
        }
        
        headers = {
            'X-API-KEY': self.valves.SERPER_API_KEY,
            'Content-Type': 'application/json'
        }
        
        try:
            if self.valves.DEBUG_MODE:
                print(f"ğŸ” Serperæœç´¢: {query} (gl={gl}, hl={hl})")
            
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.SERPER_TIMEOUT
            )
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ Serperæœç´¢é”™è¯¯: {str(e)}")
            return {"error": str(e)}

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_input_tokens(system_prompt)
        self._add_input_tokens(user_prompt)
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            answer = result["choices"][0]["message"]["content"]
            self._add_output_tokens(answer)
            
            return answer
            
        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return error_msg

    def _stage1_optimize_query(self, user_message: str, messages: List[dict]) -> dict:
        """é˜¶æ®µ1: é—®é¢˜ä¼˜åŒ–"""
        # æå–å†å²ä¸Šä¸‹æ–‡
        context_text = ""
        if messages and len(messages) > 1:
            recent_messages = messages[-self.valves.HISTORY_TURNS*2:] if len(messages) > self.valves.HISTORY_TURNS*2 else messages
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼Œä¼˜åŒ–æœç´¢æŸ¥è¯¢ä»¥è·å¾—æ›´å¥½çš„æœç´¢ç»“æœã€‚

è¯·å°†ä¼˜åŒ–åçš„é—®é¢˜ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä¸­æ–‡å’Œè‹±æ–‡ç‰ˆæœ¬ï¼š
{
    "optimized_question_cn": "ä¼˜åŒ–åçš„ä¸­æ–‡é—®é¢˜",
    "optimized_question_en": "optimized English question"
}

ä¼˜åŒ–åŸåˆ™ï¼š
1. æå–æ ¸å¿ƒå…³é”®è¯
2. å»é™¤å†—ä½™è¯æ±‡
3. ä¿ç•™é‡è¦é™å®šè¯
4. ç»“åˆå†å²ä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çœŸå®æ„å›¾
5. è‹±æ–‡ç‰ˆæœ¬åº”è¯¥æ˜¯å‡†ç¡®çš„ç¿»è¯‘å¹¶é€‚åˆæœç´¢"""

        user_prompt = f"""å†å²å¯¹è¯ä¸Šä¸‹æ–‡:
{context_text if context_text else "æ— å†å²å¯¹è¯"}

å½“å‰ç”¨æˆ·é—®é¢˜: {user_message}

è¯·ä¼˜åŒ–è¿™ä¸ªé—®é¢˜ä»¥è·å¾—æ›´å¥½çš„æœç´¢ç»“æœã€‚"""

        response = self._call_openai_api(system_prompt, user_prompt, json_mode=True)
        
        try:
            return json.loads(response)
        except:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹é—®é¢˜
            return {
                "optimized_question_cn": user_message,
                "optimized_question_en": user_message
            }

    def _stage2_search_and_select(self, optimized_queries: dict) -> List[dict]:
        """é˜¶æ®µ2: æœç´¢å¹¶é€‰æ‹©æœ€ä½³ç»“æœ"""
        all_results = []
        
        # ä¸­æ–‡æœç´¢
        cn_results = self._search_serper(optimized_queries["optimized_question_cn"], gl="cn", hl="zh-cn")
        if "organic" in cn_results:
            for result in cn_results["organic"]:
                result_info = {
                    "title": result.get("title", ""),
                    "link": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "website_type": self._identify_website_type(result.get("link", "")),
                    "search_lang": "zh-cn"
                }
                all_results.append(result_info)
        
        # è‹±æ–‡æœç´¢
        en_results = self._search_serper(optimized_queries["optimized_question_en"], gl="us", hl="en")
        if "organic" in en_results:
            for result in en_results["organic"]:
                result_info = {
                    "title": result.get("title", ""),
                    "link": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "website_type": self._identify_website_type(result.get("link", "")),
                    "search_lang": "en"
                }
                all_results.append(result_info)
        
        if not all_results:
            return []
        
        # ä½¿ç”¨LLMé€‰æ‹©æœ€ä½³ç»“æœ
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯ç­›é€‰ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæœç´¢ç»“æœï¼Œé€‰æ‹©æœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„{self.valves.SELECTED_URLS_COUNT}ä¸ªç½‘é¡µé“¾æ¥ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›é€‰ä¸­çš„ç»“æœç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰ï¼š
{{
    "selected_indices": [0, 1, 2, ...]
}}

é€‰æ‹©æ ‡å‡†ï¼š
1. å†…å®¹ä¸é—®é¢˜çš„ç›¸å…³æ€§
2. ä¿¡æ¯æºçš„æƒå¨æ€§ï¼ˆwikiã€ç™¾ç§‘ç±»ä¼˜å…ˆï¼‰
3. å†…å®¹çš„ä¸°å¯Œç¨‹åº¦
4. é¿å…é‡å¤å†…å®¹"""

        results_text = ""
        for i, result in enumerate(all_results):
            results_text += f"[{i}] æ ‡é¢˜: {result['title']}\n"
            results_text += f"    é“¾æ¥: {result['link']}\n"
            results_text += f"    æ‘˜è¦: {result['snippet']}\n"
            results_text += f"    ç½‘ç«™ç±»å‹: {result['website_type']}\n"
            results_text += f"    æœç´¢è¯­è¨€: {result['search_lang']}\n\n"

        user_prompt = f"""ç”¨æˆ·é—®é¢˜: {optimized_queries["optimized_question_cn"]}

æœç´¢ç»“æœ:
{results_text}

è¯·é€‰æ‹©æœ€ç›¸å…³çš„{self.valves.SELECTED_URLS_COUNT}ä¸ªç»“æœã€‚"""

        response = self._call_openai_api(system_prompt, user_prompt, json_mode=True)
        
        try:
            selection = json.loads(response)
            selected_indices = selection.get("selected_indices", [])
            selected_results = [all_results[i] for i in selected_indices if 0 <= i < len(all_results)]
            return selected_results[:self.valves.SELECTED_URLS_COUNT]
        except:
            # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œè¿”å›å‰Nä¸ªç»“æœ
            return all_results[:self.valves.SELECTED_URLS_COUNT]

    async def _fetch_url_content(self, session: aiohttp.ClientSession, url: str) -> dict:
        """å¼‚æ­¥è·å–ç½‘é¡µå†…å®¹"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=self.valves.CONTENT_FETCH_TIMEOUT)) as response:
                if response.status == 200:
                    html_content = await response.text()
                    # ä½¿ç”¨BeautifulSoupæå–æ–‡æœ¬å†…å®¹
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–ä¸»è¦æ–‡æœ¬å†…å®¹
                    text_content = soup.get_text()
                    # æ¸…ç†æ–‡æœ¬
                    lines = (line.strip() for line in text_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    if len(text_content) > self.valves.MAX_CONTENT_LENGTH:
                        text_content = text_content[:self.valves.MAX_CONTENT_LENGTH] + "..."
                    
                    return {
                        "url": url,
                        "status": "success",
                        "content": text_content,
                        "title": soup.title.string if soup.title else ""
                    }
                else:
                    return {
                        "url": url,
                        "status": "error",
                        "content": f"HTTP {response.status}",
                        "title": ""
                    }
        except Exception as e:
            return {
                "url": url,
                "status": "error", 
                "content": str(e),
                "title": ""
            }

    async def _stage4_fetch_content(self, selected_results: List[dict]) -> List[dict]:
        """é˜¶æ®µ4: å¹¶å‘è·å–ç½‘é¡µå†…å®¹"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for result in selected_results:
                task = self._fetch_url_content(session, result["link"])
                tasks.append(task)
            
            contents = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å†…å®¹å¹¶ä¸åŸç»“æœåˆå¹¶
            enriched_results = []
            for i, content in enumerate(contents):
                if isinstance(content, dict):
                    enriched_result = selected_results[i].copy()
                    enriched_result.update(content)
                    enriched_results.append(enriched_result)
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    enriched_result = selected_results[i].copy()
                    enriched_result.update({
                        "status": "error",
                        "content": str(content),
                        "title": ""
                    })
                    enriched_results.append(enriched_result)
            
            return enriched_results

    def _stage5_generate_final_answer(self, user_message: str, enriched_results: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ5: ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        # æ„å»ºä¿¡æ¯æºæ–‡æœ¬å’Œé“¾æ¥åˆ—è¡¨
        source_content = ""
        all_sources = []  # åŒ…å«æ‰€æœ‰æ¥æºï¼ŒåŒ…æ‹¬å¤±è´¥çš„
        successful_sources = []
        
        for i, result in enumerate(enriched_results, 1):
            # è®°å½•æ‰€æœ‰æ¥æºä¿¡æ¯ç”¨äºæœ«å°¾é“¾æ¥å±•ç¤º
            source_info = {
                "index": i,
                "title": result.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                "link": result['link'],
                "website_type": result['website_type'],
                "status": result.get("status", "unknown")
            }
            all_sources.append(source_info)
            
            if result.get("status") == "success" and result.get("content"):
                source_content += f"[æ¥æº{i}] {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
                source_content += f"é“¾æ¥: {result['link']}\n"
                source_content += f"ç½‘ç«™ç±»å‹: {result['website_type']}\n"
                source_content += f"å†…å®¹æ‘˜è¦: {result['snippet']}\n"
                source_content += f"ä¸»è¦å†…å®¹: {result['content'][:8000]}...\n\n"
                successful_sources.append(i)
            else:
                # å³ä½¿è·å–å¤±è´¥ï¼Œä¹Ÿæ·»åŠ åŸºæœ¬ä¿¡æ¯
                source_content += f"[æ¥æº{i}] {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
                source_content += f"é“¾æ¥: {result['link']}\n"
                source_content += f"ç½‘ç«™ç±»å‹: {result['website_type']}\n"
                source_content += f"å†…å®¹æ‘˜è¦: {result.get('snippet', 'æ— æ‘˜è¦')}\n"
                source_content += f"çŠ¶æ€: å†…å®¹è·å–å¤±è´¥ - {result.get('content', 'æœªçŸ¥é”™è¯¯')}\n\n"
        
        if not successful_sources:
            return "æŠ±æ­‰ï¼Œæ‰€æœ‰ç½‘é¡µå†…å®¹è·å–éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æä¾›åŸºäºç½‘é¡µå†…å®¹çš„å›ç­”ã€‚"
        
        # æ„å»ºæ‰€æœ‰é“¾æ¥çš„markdownæ ¼å¼
        all_links_md = ""
        for source in all_sources:
            status_indicator = "âœ…" if source["status"] == "success" else "âŒ"
            all_links_md += f"{status_indicator} [{source['title']}]({source['link']}) ({source['website_type']})\n"
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ•´åˆä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„å¤šä¸ªä¿¡æ¯æºï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰ç”¨çš„å›ç­”ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨æ‰€æœ‰{len(enriched_results)}ä¸ªä¿¡æ¯æºçš„ä¿¡æ¯ï¼Œä¸èƒ½é—æ¼ä»»ä½•ä¸€ä¸ª
2. å›ç­”ä¸­è¦å°†å¼•ç”¨å¤„ç†æˆmarkdowné“¾æ¥æ ¼å¼ï¼š[æ ‡é¢˜](é“¾æ¥)
3. å¯¹äºå†…å®¹è·å–æˆåŠŸçš„æ¥æºï¼Œå¿…é¡»å……åˆ†åˆ©ç”¨å…¶å†…å®¹
4. å¯¹äºå†…å®¹è·å–å¤±è´¥çš„æ¥æºï¼Œè‡³å°‘è¦æåŠå…¶å­˜åœ¨å’Œç›¸å…³æ€§
5. å›ç­”å¿…é¡»å¿ äºä¿¡æ¯æºï¼Œä¸èƒ½ç¼–é€ ä¿¡æ¯
6. å†…å®¹è¦ä¸°å¯Œã€å‡†ç¡®ã€ç»“æ„æ¸…æ™°
7. å¦‚æœä¸åŒæ¥æºæœ‰çŸ›ç›¾ä¿¡æ¯ï¼Œè¯·æŒ‡å‡ºå¹¶è¯´æ˜
8. ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è‡ªç„¶æµç•…

å›ç­”ç»“æ„è¦æ±‚ï¼š
- ä¸»ä½“å›ç­”ï¼šåŸºäºæ‰€æœ‰ä¿¡æ¯æºçš„ç»¼åˆå›ç­”ï¼Œä½¿ç”¨markdowné“¾æ¥å¼•ç”¨
- æœ«å°¾å¿…é¡»åŒ…å«"## å‚è€ƒæ¥æº"éƒ¨åˆ†ï¼Œåˆ—å‡ºæ‰€æœ‰{len(enriched_results)}ä¸ªæ¥æºçš„å®Œæ•´é“¾æ¥

æ‰€æœ‰æ¥æºé“¾æ¥ï¼ˆè¯·åœ¨å›ç­”æœ«å°¾å®Œæ•´å±•ç¤ºï¼‰ï¼š
{all_links_md}

ç‰¹åˆ«æ³¨æ„ï¼šå³ä½¿æŸäº›æ¥æºå†…å®¹è·å–å¤±è´¥ï¼Œä¹Ÿè¦åœ¨å›ç­”ä¸­æåŠå…¶ç›¸å…³æ€§ï¼Œå¹¶åœ¨æœ«å°¾é“¾æ¥ä¸­åŒ…å«ã€‚"""

        user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_message}

ä¿¡æ¯æºè¯¦æƒ…:
{source_content}

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰{len(enriched_results)}ä¸ªä¿¡æ¯æºä¸ºç”¨æˆ·æä¾›è¯¦ç»†å‡†ç¡®çš„å›ç­”ï¼Œç¡®ä¿ï¼š
1. ä½¿ç”¨markdowné“¾æ¥æ ¼å¼å¼•ç”¨æ¥æº
2. ä¸é—æ¼ä»»ä½•ä¸€ä¸ªä¿¡æ¯æº
3. æœ«å°¾åŒ…å«å®Œæ•´çš„å‚è€ƒæ¥æºåˆ—è¡¨"""

        if stream:
            return self._stream_openai_response(user_prompt, system_prompt)
        else:
            return self._call_openai_api(system_prompt, user_prompt)

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_output_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
                            
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼æ¨¡å¼  
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING
        
        try:
            # é˜¶æ®µ1: é—®é¢˜ä¼˜åŒ–
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨ä¼˜åŒ–æœç´¢é—®é¢˜...", "query_optimization"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ”„ **é˜¶æ®µ1**: æ­£åœ¨ä¼˜åŒ–æœç´¢é—®é¢˜..."
            
            optimized_queries = self._stage1_optimize_query(user_message, messages)
            
            if stream_mode:
                opt_info = f"âœ… é—®é¢˜ä¼˜åŒ–å®Œæˆ\nä¸­æ–‡: {optimized_queries['optimized_question_cn']}\nè‹±æ–‡: {optimized_queries['optimized_question_en']}"
                for chunk in self._emit_processing(opt_info, "query_optimization"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield f"âœ… ä¸­æ–‡ä¼˜åŒ–é—®é¢˜: {optimized_queries['optimized_question_cn']}\n"
                yield f"âœ… è‹±æ–‡ä¼˜åŒ–é—®é¢˜: {optimized_queries['optimized_question_en']}\n"

            # é˜¶æ®µ2: æœç´¢å’Œé€‰æ‹©
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢å’Œç»“æœç­›é€‰...", "web_search"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ” **é˜¶æ®µ2**: æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢å’Œç»“æœç­›é€‰..."
            
            selected_results = self._stage2_search_and_select(optimized_queries)
            
            if not selected_results:
                yield "âŒ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯"
                return
            
            # å±•ç¤ºé€‰ä¸­çš„ä¿¡æ¯æº
            source_info = f"âœ… å·²é€‰æ‹©{len(selected_results)}ä¸ªä¿¡æ¯æº:\n"
            for i, result in enumerate(selected_results, 1):
                source_info += f"[{i}] {result['title']} ({result['website_type']})\n    {result['link']}\n"
            
            if stream_mode:
                for chunk in self._emit_processing(source_info, "web_search"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield source_info

            # é˜¶æ®µ4: è·å–ç½‘é¡µå†…å®¹ (ä½¿ç”¨å¼‚æ­¥)
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨è·å–ç½‘é¡µå†…å®¹...", "content_fetch"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ“„ **é˜¶æ®µ4**: æ­£åœ¨è·å–ç½‘é¡µå†…å®¹..."
            
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                enriched_results = loop.run_until_complete(self._stage4_fetch_content(selected_results))
            finally:
                loop.close()
            
            # ç»Ÿè®¡æˆåŠŸè·å–çš„å†…å®¹
            successful_count = sum(1 for r in enriched_results if r.get("status") == "success")
            
            content_info = f"âœ… å†…å®¹è·å–å®Œæˆï¼ŒæˆåŠŸè·å–{successful_count}/{len(enriched_results)}ä¸ªç½‘é¡µå†…å®¹"
            
            if stream_mode:
                for chunk in self._emit_processing(content_info, "content_fetch"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield content_info

            # é˜¶æ®µ5: ç”Ÿæˆæœ€ç»ˆå›ç­”
            if stream_mode:
                # æµå¼æ¨¡å¼å¼€å§‹ç”Ÿæˆå›ç­”çš„æ ‡è¯†
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ’­ ç”Ÿæˆæœ€ç»ˆå›ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
            else:
                yield "ğŸ¤– **é˜¶æ®µ5**: æ­£åœ¨åŸºäºè·å–çš„å†…å®¹ç”Ÿæˆå›ç­”..."

            # ç”Ÿæˆæœ€ç»ˆå›ç­”
            if stream_mode:
                for chunk in self._stage5_generate_final_answer(user_message, enriched_results, stream=True):
                    yield chunk
                # æµå¼æ¨¡å¼ç»“æŸåæ·»åŠ tokenç»Ÿè®¡
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
            else:
                result = self._stage5_generate_final_answer(user_message, enriched_results, stream=False)
                yield result
                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg