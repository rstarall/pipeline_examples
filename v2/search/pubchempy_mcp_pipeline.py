"""
å‚è€ƒv2\search\pubchempy_mcp\src\llm_use.pyç¼–å†™æœ¬pipeline
1.é‡‡ç”¨æµå¼è¾“å‡º,å…è®¸ä¸­é—´å·¥å…·è°ƒç”¨(æµå¼å›ç­”ç­‰å¾…ä¸è¦è¿”å›Done),è°ƒç”¨å®Œç»§ç»­æµå¼å›ç­”ï¼Œç›´åˆ°ç»“æŸè¿”å›Done
2.å·¥å…·è°ƒç”¨æ—¶ä½¿ç”¨_emit_processingè¾“å‡ºè°ƒç”¨å’Œè¿”å›ç»“æœ
3.æ³¨æ„ä½ çš„æç¤ºè¯æ˜¯ä¸€ä¸ªåŒ–åˆç‰©åŒ–å­¦å…¬å¼é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æœç´¢è¿”å›çš„jsonè¿›è¡ŒçŸ¥è¯†é—®ç­”
4.å‚è€ƒv2\search\serper_openai_pipeline.pyï¼Œä¸éœ€è¦æœç´¢ï¼Œå¯å¤šæ¬¡è°ƒç”¨å·¥å…·
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
from typing import List, Union, Generator, Iterator, Dict, Any, Optional
from pydantic import BaseModel
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åç«¯é˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "chemical_analysis": "åŒ–å­¦åˆ†æ",
    "tool_calling": "å·¥å…·è°ƒç”¨",
    "answer_generation": "ç”Ÿæˆå›ç­”",
}

STAGE_GROUP = {
    "chemical_analysis": "stage_group_1",
    "tool_calling": "stage_group_2", 
    "answer_generation": "stage_group_3",
}

class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        MAX_TOOL_CALLS: int
        
        # MCPé…ç½®
        MCP_REMOTE_URL: str
        MCP_TIMEOUT: int

    def __init__(self):
        self.name = "PubChemPy MCP Chemical Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        # MCPå®¢æˆ·ç«¯é…ç½®
        self.mcp_request_id = 1
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "MAX_TOOL_CALLS": int(os.getenv("MAX_TOOL_CALLS", "5")),
                
                # MCPé…ç½®
                "MCP_REMOTE_URL": os.getenv("MCP_REMOTE_URL", "http://localhost:8000/mcp"),
                "MCP_TIMEOUT": int(os.getenv("MCP_TIMEOUT", "30")),
            }
        )

    async def on_startup(self):
        print(f"PubChemPy MCP Chemical Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # éªŒè¯MCPæœåŠ¡å™¨åœ°å€
        print(f"ğŸ”— MCPæœåŠ¡å™¨åœ°å€: {self.valves.MCP_REMOTE_URL}")
        if not self.valves.MCP_REMOTE_URL:
            print("âŒ ç¼ºå°‘MCPæœåŠ¡å™¨åœ°å€ï¼Œè¯·è®¾ç½®MCP_REMOTE_URLç¯å¢ƒå˜é‡")

    async def on_shutdown(self):
        print(f"PubChemPy MCP Chemical Pipelineå…³é—­: {__name__}")
        print("ğŸ”š Pipelineå·²å…³é—­")



    async def _call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """é€šè¿‡HTTPè°ƒç”¨è¿œç¨‹MCPå·¥å…·"""
        if not self.valves.MCP_REMOTE_URL:
            return {"error": "MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®"}
        
        try:
            # æ„å»ºMCPè°ƒç”¨è¯·æ±‚
            request = {
                "jsonrpc": "2.0",
                "id": self.mcp_request_id,
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": arguments
                }
            }
            
            # å¢åŠ è¯·æ±‚ID
            self.mcp_request_id += 1
            
            # å‘é€HTTPè¯·æ±‚åˆ°è¿œç¨‹MCPæœåŠ¡å™¨
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥HTTPè¯·æ±‚
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.valves.MCP_REMOTE_URL,
                    headers=headers,
                    json=request,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        if "result" in result:
                            return result["result"]
                        else:
                            return {"error": result.get("error", "Unknown error")}
                    else:
                        return {"error": f"HTTP {response.status}: {await response.text()}"}
                
        except asyncio.TimeoutError:
            logger.error("MCPå·¥å…·è°ƒç”¨è¶…æ—¶")
            return {"error": "è¯·æ±‚è¶…æ—¶"}
        except aiohttp.ClientError as e:
            logger.error(f"MCP HTTPè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _search_chemical(self, query: str, search_type: str = "formula", use_fallback: bool = False) -> str:
        """æœç´¢åŒ–å­¦ç‰©è´¨"""
        result = await self._call_mcp_tool("search_chemical", {
            "query": query,
            "search_type": search_type,
            "use_fallback": use_fallback
        })
        
        if "content" in result and result["content"]:
            # æå–æ–‡æœ¬å†…å®¹
            text_content = ""
            for content in result["content"]:
                if content.get("type") == "text":
                    text_content += content.get("text", "") + "\n"
            return text_content.strip()
        elif "error" in result:
            return f"æœç´¢å¤±è´¥: {result['error']}"
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³åŒ–å­¦ä¿¡æ¯"

    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œåªæœ‰system_promptä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ systemæ¶ˆæ¯
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        if system_prompt and system_prompt.strip():
            self._add_input_tokens(system_prompt)
        self._add_input_tokens(user_prompt)
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            answer = result["choices"][0]["message"]["content"]
            self._add_output_tokens(answer)
            
            return answer
            
        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return error_msg

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œåªæœ‰system_promptä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ systemæ¶ˆæ¯
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_output_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
                            
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ–å­¦ä¿¡æ¯åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢åŒ–å­¦ç‰©è´¨çš„è¯¦ç»†ä¿¡æ¯ã€‚

å½“ç”¨æˆ·è¯¢é—®ä»¥ä¸‹å†…å®¹æ—¶ï¼Œä½ éœ€è¦è°ƒç”¨åŒ–å­¦æœç´¢å·¥å…·ï¼š
1. è¯¢é—®ç‰¹å®šåŒ–å­¦ç‰©è´¨çš„ä¿¡æ¯ï¼ˆå¦‚å’–å•¡å› ã€æ°´ã€ä¹™é†‡ç­‰ï¼‰
2. æä¾›åˆ†å­å¼å¹¶è¯¢é—®å¯¹åº”çš„åŒ–åˆç‰©ï¼ˆå¦‚H2Oã€C8H10N4O2ç­‰ï¼‰
3. æä¾›SMILESå­—ç¬¦ä¸²å¹¶è¯¢é—®åŒ–åˆç‰©ä¿¡æ¯ï¼ˆå¦‚CCOã€CN1C=NC2=C1C(=O)N(C(=O)N2C)Cç­‰ï¼‰
4. è¯¢é—®åŒ–å­¦ç‰©è´¨çš„æ€§è´¨ã€ç»“æ„ã€åŒä¹‰è¯ç­‰

âš ï¸ é‡è¦ï¼šPubChemæ•°æ®åº“ä¸»è¦ä½¿ç”¨è‹±æ–‡ï¼Œå› æ­¤queryå‚æ•°å¿…é¡»ä¸ºè‹±æ–‡åç§°ã€åˆ†å­å¼æˆ–SMILESå­—ç¬¦ä¸²ã€‚

ğŸ§  æ™ºèƒ½è½¬æ¢è§„åˆ™ï¼š
- å¦‚æœç”¨æˆ·æä¾›ä¸­æ–‡åŒ–å­¦åç§°ï¼Œè¯·æ ¹æ®ä½ çš„åŒ–å­¦çŸ¥è¯†å°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„è‹±æ–‡åç§°
- ä¼˜å…ˆä½¿ç”¨è‹±æ–‡åŒ–å­¦åç§°æœç´¢ï¼ˆæ¨èï¼Œæ›´å‡†ç¡®ï¼‰
- å¦‚æœä¸ç¡®å®šä¸­æ–‡åç§°å¯¹åº”çš„è‹±æ–‡åï¼Œå†è€ƒè™‘ä½¿ç”¨åˆ†å­å¼æœç´¢
- å¦‚æœç”¨æˆ·ç›´æ¥æä¾›äº†åˆ†å­å¼æˆ–SMILESï¼Œç›´æ¥ä½¿ç”¨

è½¬æ¢ç¤ºä¾‹ï¼š
- "å’–å•¡å› " â†’ "caffeine" (ä¸­æ–‡è½¬è‹±æ–‡å)
- "H2O" â†’ "H2O" (åˆ†å­å¼ä¿æŒä¸å˜)
- "CCO" â†’ "CCO" (SMILESä¿æŒä¸å˜)

å¦‚æœä½ åˆ¤æ–­éœ€è¦æœç´¢åŒ–å­¦ä¿¡æ¯ï¼Œè¯·å›å¤ï¼š
TOOL_CALL:search_chemical:{"query": "è½¬æ¢åçš„è‹±æ–‡/åˆ†å­å¼/SMILES", "search_type": "æœç´¢ç±»å‹"}

å…¶ä¸­search_typeå¯ä»¥æ˜¯ï¼š
- "name": æŒ‰è‹±æ–‡åŒ–å­¦åç§°æœç´¢ï¼ˆæ¨èï¼Œæ›´å‡†ç¡®ï¼‰
- "formula": æŒ‰åˆ†å­å¼æœç´¢
- "smiles": æŒ‰SMILESå­—ç¬¦ä¸²æœç´¢

è°ƒç”¨ç¤ºä¾‹ï¼š
- ç”¨æˆ·é—®"å’–å•¡å› çš„åˆ†å­å¼æ˜¯ä»€ä¹ˆ" â†’ TOOL_CALL:search_chemical:{"query": "caffeine", "search_type": "name"}
- ç”¨æˆ·é—®"H2Oæ˜¯ä»€ä¹ˆåŒ–åˆç‰©" â†’ TOOL_CALL:search_chemical:{"query": "H2O", "search_type": "formula"}
- ç”¨æˆ·é—®"CCOä»£è¡¨ä»€ä¹ˆ" â†’ TOOL_CALL:search_chemical:{"query": "CCO", "search_type": "smiles"}

å¦‚æœä¸éœ€è¦æœç´¢åŒ–å­¦ä¿¡æ¯ï¼Œè¯·æ­£å¸¸å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœéœ€è¦å¤šæ¬¡è°ƒç”¨å·¥å…·æ¥è·å–æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è°ƒç”¨ã€‚"""

    async def _process_user_message(self, user_message: str, messages: List[dict], stream_mode: bool) -> Generator:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨"""
        
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._get_system_prompt()
        
        # æ„å»ºå¯¹è¯å†å²
        conversation_history = []
        if messages and len(messages) > 1:
            # å–æœ€è¿‘çš„å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            recent_messages = messages[-6:] if len(messages) > 6 else messages
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role in ["user", "assistant"]:
                    conversation_history.append({"role": role, "content": content})
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        conversation_history.append({"role": "user", "content": user_message})
        
        # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æç¤º
        context_text = ""
        for msg in conversation_history[:-1]:  # é™¤äº†æœ€åä¸€æ¡æ¶ˆæ¯
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            context_text += f"{role}: {msg['content']}\n"
        
        full_user_prompt = f"""å†å²å¯¹è¯ä¸Šä¸‹æ–‡:
{context_text if context_text else "æ— å†å²å¯¹è¯"}

å½“å‰ç”¨æˆ·é—®é¢˜: {user_message}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡å’Œå½“å‰é—®é¢˜ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨åŒ–å­¦æœç´¢å·¥å…·ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æŒ‰ç…§æŒ‡å®šæ ¼å¼å›å¤å·¥å…·è°ƒç”¨ã€‚"""
        
        tool_call_count = 0
        collected_tool_results = []
        
        while tool_call_count < self.valves.MAX_TOOL_CALLS:
            # è·å–AIå“åº”
            ai_response = self._call_openai_api(system_prompt, full_user_prompt)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if ai_response.startswith("TOOL_CALL:search_chemical:"):
                tool_call_count += 1
                
                # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
                if stream_mode:
                    for chunk in self._emit_processing(f"ğŸ”§ æ­£åœ¨è°ƒç”¨åŒ–å­¦æœç´¢å·¥å…·ï¼ˆç¬¬{tool_call_count}æ¬¡ï¼‰...", "tool_calling"):
                        yield f'data: {json.dumps(chunk)}\n\n'
                else:
                    yield f"ğŸ”§ æ­£åœ¨è°ƒç”¨åŒ–å­¦æœç´¢å·¥å…·ï¼ˆç¬¬{tool_call_count}æ¬¡ï¼‰...\n"
                
                # è§£æå·¥å…·è°ƒç”¨å‚æ•°
                tool_args_str = ai_response.replace("TOOL_CALL:search_chemical:", "")
                try:
                    tool_args = json.loads(tool_args_str)
                    
                    # æ˜¾ç¤ºå·¥å…·è°ƒç”¨å‚æ•°
                    tool_info = f"ğŸ“ å·¥å…·è°ƒç”¨å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}"
                    if stream_mode:
                        for chunk in self._emit_processing(tool_info, "tool_calling"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield tool_info + "\n"
                    
                    # è°ƒç”¨MCPå·¥å…·
                    tool_result = await self._search_chemical(
                        query=tool_args.get("query", ""),
                        search_type=tool_args.get("search_type", "formula"),
                        use_fallback=tool_args.get("use_fallback", False)
                    )
                    
                    # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœ
                    result_info = f"âœ… å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_result[:500]}{'...' if len(tool_result) > 500 else ''}"
                    if stream_mode:
                        for chunk in self._emit_processing(result_info, "tool_calling"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield result_info + "\n"
                    
                    # æ”¶é›†å·¥å…·ç»“æœ
                    collected_tool_results.append({
                        "call": tool_args,
                        "result": tool_result
                    })
                    
                    # æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«å·¥å…·ç»“æœ
                    tool_context = f"[åŒ–å­¦æœç´¢ç»“æœ {tool_call_count}]\næŸ¥è¯¢: {tool_args}\nç»“æœ: {tool_result}\n\n"
                    full_user_prompt = f"{full_user_prompt}\n\n{tool_context}åŸºäºä»¥ä¸Šæœç´¢ç»“æœï¼Œè¯·ç»§ç»­å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è°ƒç”¨å·¥å…·ã€‚"
                    
                    # ç»§ç»­ä¸‹ä¸€è½®ï¼Œçœ‹æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·è°ƒç”¨
                    continue
                    
                except json.JSONDecodeError:
                    error_msg = "å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æå‚æ•°"
                    if stream_mode:
                        for chunk in self._emit_processing(f"âŒ {error_msg}", "tool_calling"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield f"âŒ {error_msg}\n"
                    break
            else:
                # ä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
                break
        
        # ç”Ÿæˆæœ€ç»ˆå›ç­”
        if collected_tool_results:
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ç»“æœï¼ŒåŸºäºç»“æœç”Ÿæˆå›ç­”
            final_system_prompt = "ä½ æ˜¯ä¸“ä¸šçš„åŒ–å­¦ä¿¡æ¯ä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„åŒ–å­¦æœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚"
            
            tool_summary = "åŸºäºä»¥ä¸‹åŒ–å­¦æœç´¢ç»“æœ:\n\n"
            for i, result in enumerate(collected_tool_results, 1):
                tool_summary += f"æœç´¢{i}: {json.dumps(result['call'], ensure_ascii=False)}\n"
                tool_summary += f"ç»“æœ{i}: {result['result']}\n\n"
            
            final_user_prompt = f"{tool_summary}ç”¨æˆ·é—®é¢˜: {user_message}\n\nè¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å‡†ç¡®è¯¦ç»†çš„å›ç­”ã€‚"
            
            if stream_mode:
                # æµå¼æ¨¡å¼å¼€å§‹ç”Ÿæˆå›ç­”çš„æ ‡è¯†
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ§ª åŸºäºåŒ–å­¦æ•°æ®åº“ä¿¡æ¯å›ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
                
                # æµå¼ç”Ÿæˆæœ€ç»ˆå›ç­”
                for chunk in self._stream_openai_response(final_user_prompt, final_system_prompt):
                    # åŒ…è£…æˆSSEæ ¼å¼
                    chunk_data = {
                        'choices': [{
                            'delta': {
                                'content': chunk
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
            else:
                yield "ğŸ§ª **åŸºäºåŒ–å­¦æ•°æ®åº“ä¿¡æ¯å›ç­”**\n"
                final_answer = self._call_openai_api(final_system_prompt, final_user_prompt)
                yield final_answer
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›AIå“åº”
            if stream_mode:
                # æµå¼æ¨¡å¼
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ’­ å›ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
                
                for chunk in self._stream_openai_response(full_user_prompt, system_prompt):
                    chunk_data = {
                        'choices': [{
                            'delta': {
                                'content': chunk
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
            else:
                yield "ğŸ’­ **å›ç­”**\n"
                yield ai_response

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ§ª åŒ–å­¦åŠ©æ‰‹æ”¶åˆ°æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„åŒ–å­¦é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼æ¨¡å¼  
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING
        
        try:
            # åŒ–å­¦åˆ†æé˜¶æ®µ
            if stream_mode:
                for chunk in self._emit_processing("ğŸ§ª æ­£åœ¨åˆ†æåŒ–å­¦é—®é¢˜...", "chemical_analysis"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ§ª **é˜¶æ®µ1**: æ­£åœ¨åˆ†æåŒ–å­¦é—®é¢˜...\n"
            
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œå¯èƒ½åŒ…å«å¤šæ¬¡å·¥å…·è°ƒç”¨
                async def run_process():
                    results = []
                    async for result in self._process_user_message(user_message, messages, stream_mode):
                        results.append(result)
                    return results
                
                # è·å–æ‰€æœ‰ç»“æœå¹¶é€ä¸ªyield
                results = loop.run_until_complete(run_process())
                for result in results:
                    yield result
                
                # æœ€åå‘é€å®Œæˆä¿¡æ¯
                if stream_mode:
                    # æµå¼æ¨¡å¼ç»“æŸ
                    done_msg = {
                        'choices': [{
                            'delta': {},
                            'finish_reason': 'stop'
                        }]
                    }
                    yield f"data: {json.dumps(done_msg)}\n\n"
                    yield "data: [DONE]\n\n"
                    
                    # æ·»åŠ tokenç»Ÿè®¡
                    token_info = self._get_token_stats()
                    token_msg = {
                        'choices': [{
                            'delta': {
                                'content': f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(token_msg)}\n\n"
                else:
                    # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                    token_info = self._get_token_stats()
                    yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
                    
            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            
            if stream_mode:
                error_chunk = {
                    'choices': [{
                        'delta': {
                            'content': error_msg
                        },
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield error_msg