"""
PubTator3 MCP Pipeline - åŸºäºMCPåè®®çš„å­¦æœ¯è®ºæ–‡æœç´¢ç®¡é“

åŠŸèƒ½ç‰¹æ€§:
1. é€šè¿‡MCP JSON-RPCåè®®åŠ¨æ€å‘ç°PubTator3æœåŠ¡å™¨å·¥å…·
2. ä½¿ç”¨MCP JSON-RPCåè®®è¿›è¡Œå·¥å…·è°ƒç”¨
3. æ”¯æŒæµå¼è¾“å‡ºå’Œæ™ºèƒ½å·¥å…·é€‰æ‹©
4. AIå†³ç­–é©±åŠ¨çš„å•æ¬¡å·¥å…·è°ƒç”¨æ¨¡å¼
5. åŸºäºæŠ½è±¡å†…å®¹çš„å­¦æœ¯è®ºæ–‡æœç´¢
6. å¤šæºæ‘˜è¦æ£€ç´¢åŠŸèƒ½
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
from typing import List, Union, Generator, Iterator, Dict, Any, Optional, AsyncGenerator
from pydantic import BaseModel
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åç«¯é˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "mcp_discovery": "MCPæœåŠ¡å‘ç°", 
    "tool_calling": "å·¥å…·è°ƒç”¨",
    "answer_generation": "ç”Ÿæˆå›ç­”",
}

STAGE_GROUP = {
    "mcp_discovery": "stage_group_1",
    "tool_calling": "stage_group_2", 
    "answer_generation": "stage_group_3",
}

class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        MAX_TOOL_CALLS: int
        
        # MCPé…ç½®
        MCP_SERVER_URL: str
        MCP_TIMEOUT: int
        MCP_TOOLS_EXPIRE_HOURS: int

    def __init__(self):
        self.name = "PubTator3 MCP Academic Paper Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0, 
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        # MCPå·¥å…·ç¼“å­˜
        self.mcp_tools = {}
        self.tools_loaded = False
        self.tools_loaded_time = None  # è®°å½•å·¥å…·åŠ è½½æ—¶é—´
        
        # MCPä¼šè¯IDå°†åœ¨åˆå§‹åŒ–æ—¶ä»æœåŠ¡å™¨è·å–
        self.session_id = None
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "MAX_TOOL_CALLS": int(os.getenv("MAX_TOOL_CALLS", "3")),
                
                # MCPé…ç½® - é»˜è®¤æŒ‡å‘pubtator3æœåŠ¡
                "MCP_SERVER_URL": os.getenv("MCP_SERVER_URL", "http://localhost:8991"),
                "MCP_TIMEOUT": int(os.getenv("MCP_TIMEOUT", "30")),
                "MCP_TOOLS_EXPIRE_HOURS": int(os.getenv("MCP_TOOLS_EXPIRE_HOURS", "12")),
            }
        )

    async def on_startup(self):
        print(f"PubTator3 MCP Academic Paper Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # éªŒè¯MCPæœåŠ¡å™¨åœ°å€
        print(f"ğŸ”— MCPæœåŠ¡å™¨åœ°å€: {self.valves.MCP_SERVER_URL}")
        print(f"â° å·¥å…·è¿‡æœŸæ—¶é—´: {self.valves.MCP_TOOLS_EXPIRE_HOURS} å°æ—¶")
        if not self.valves.MCP_SERVER_URL:
            print("âŒ ç¼ºå°‘MCPæœåŠ¡å™¨åœ°å€ï¼Œè¯·è®¾ç½®MCP_SERVER_URLç¯å¢ƒå˜é‡")
        
        print("ğŸ”§ MCPå·¥å…·å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨å‘ç°")

    async def on_shutdown(self):
        print(f"PubTator3 MCP Academic Paper Pipelineå…³é—­: {__name__}")
        print("ğŸ”š Pipelineå·²å…³é—­")

    async def _initialize_mcp_session(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """åˆå§‹åŒ–MCPä¼šè¯å¹¶è·å–æœåŠ¡å™¨åˆ†é…çš„session ID"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        try:
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # Step 1: å‘é€initializeè¯·æ±‚ï¼ˆä¸å¸¦session IDï¼‰
            initialize_request = {
                "jsonrpc": "2.0",
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {
                        "sampling": {},
                        "roots": {"listChanged": True}
                    },
                    "clientInfo": {
                        "name": "PubTator3 MCP Pipeline",
                        "version": "1.0.0"
                    }
                },
                "id": "init-1"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=initialize_request,
                    headers={
                        "Content-Type": "application/json",
                        "Accept": "application/json, text/event-stream"
                    },
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # æ£€æŸ¥å“åº”å¤´ä¸­çš„session ID
                        server_session_id = response.headers.get("Mcp-Session-Id")
                        if server_session_id:
                            self.session_id = server_session_id
                        
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            init_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "init-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            init_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            init_response = await response.json()
                        
                        if not init_response:
                            raise Exception("No initialize response received")
                        
                        if "error" in init_response:
                            raise Exception(f"MCP initialize error: {init_response['error']}")
                        
                        # Step 2: å‘é€initializedé€šçŸ¥
                        initialized_notification = {
                            "jsonrpc": "2.0",
                            "method": "notifications/initialized"
                        }
                        
                        headers = {
                            "Content-Type": "application/json",
                            "Accept": "application/json, text/event-stream"
                        }
                        if hasattr(self, 'session_id') and self.session_id:
                            headers["Mcp-Session-Id"] = self.session_id
                        
                        async with session.post(
                            mcp_url,
                            json=initialized_notification,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                        ) as notify_response:
                            if notify_response.status not in [200, 202]:
                                pass  # å¿½ç•¥initializedé€šçŸ¥å¤±è´¥
                        
                        init_msg = "ğŸ”§ MCPä¼šè¯åˆå§‹åŒ–å®Œæˆ"
                        if stream_mode:
                            for chunk in self._emit_processing(init_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield init_msg + "\n"
                    else:
                        error_text = await response.text()
                        raise Exception(f"Initialize failed - HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ MCPä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    async def _discover_mcp_tools(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """é€šè¿‡MCP JSON-RPCåè®®å‘ç°æœåŠ¡å™¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        start_msg = f"ğŸ” æ­£åœ¨å‘ç°PubTator3 MCPå·¥å…·..."
        if stream_mode:
            for chunk in self._emit_processing(start_msg, "mcp_discovery"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield start_msg + "\n"
        
        # é¦–å…ˆåˆå§‹åŒ–MCPä¼šè¯
        if not hasattr(self, '_session_initialized'):
            async for init_output in self._initialize_mcp_session(stream_mode):
                yield init_output
            self._session_initialized = True
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_request = {
                "jsonrpc": "2.0",
                "method": "tools/list",
                "id": "tools-list-1"
            }
            
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=mcp_request,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            mcp_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "tools-list-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            mcp_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            mcp_response = await response.json()
                        
                        if not mcp_response:
                            raise Exception("No tools/list response received")
                        
                        if "error" in mcp_response:
                            raise Exception(f"MCP error: {mcp_response['error']}")
                        
                        tools = mcp_response.get("result", {}).get("tools", [])
                        
                        # åŠ è½½æ‰€æœ‰å·¥å…·
                        for tool in tools:
                            tool_name = tool.get("name")
                            if tool_name:
                                self.mcp_tools[tool_name] = {
                                    "name": tool_name,
                                    "description": tool.get("description", ""),
                                    "input_schema": tool.get("inputSchema", {})
                                }
                        
                        self.tools_loaded = True
                        self.tools_loaded_time = time.time()  # è®°å½•å·¥å…·åŠ è½½æ—¶é—´
                        
                        final_msg = f"âœ… å‘ç° {len(self.mcp_tools)} ä¸ªPubTator3 MCPå·¥å…·"
                        if len(self.mcp_tools) > 0:
                            final_msg += f": {', '.join(self.mcp_tools.keys())}"
                        
                        if stream_mode:
                            for chunk in self._emit_processing(final_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield final_msg + "\n"
                        
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ PubTator3 MCPå·¥å…·å‘ç°å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    def _are_tools_expired(self) -> bool:
        """æ£€æŸ¥MCPå·¥å…·æ˜¯å¦å·²è¿‡æœŸ"""
        if not self.tools_loaded or self.tools_loaded_time is None:
            return True
        
        current_time = time.time()
        expire_seconds = self.valves.MCP_TOOLS_EXPIRE_HOURS * 3600  # è½¬æ¢ä¸ºç§’
        return (current_time - self.tools_loaded_time) > expire_seconds

    async def _ensure_tools_loaded(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """ç¡®ä¿MCPå·¥å…·å·²åŠ è½½ä¸”æœªè¿‡æœŸ"""
        need_reload = False
        reason = ""
        
        if not self.tools_loaded:
            need_reload = True
            reason = "å·¥å…·æœªåŠ è½½"
        elif self._are_tools_expired():
            need_reload = True
            expired_hours = (time.time() - self.tools_loaded_time) / 3600
            reason = f"å·¥å…·å·²è¿‡æœŸ ({expired_hours:.1f} å°æ—¶å‰åŠ è½½)"
        
        if need_reload:
            reload_msg = f"ğŸ”„ {reason}ï¼Œæ­£åœ¨é‡æ–°å‘ç°PubTator3 MCPå·¥å…·..."
            if stream_mode:
                for chunk in self._emit_processing(reload_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield reload_msg + "\n"
            
            # æ¸…é™¤æ—§çš„å·¥å…·å’Œä¼šè¯çŠ¶æ€
            self.mcp_tools = {}
            self.tools_loaded = False
            self.tools_loaded_time = None
            if hasattr(self, '_session_initialized'):
                delattr(self, '_session_initialized')
            self.session_id = None
            
            async for discovery_output in self._discover_mcp_tools(stream_mode):
                yield discovery_output

    async def _call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨MCP JSON-RPCåè®®è°ƒç”¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            return {"error": "MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®"}
        
        if not self.tools_loaded or self._are_tools_expired():
            try:
                async for output in self._ensure_tools_loaded(stream_mode=False):
                    # Silently consume the debug output in this context
                    pass
            except Exception as e:
                return {"error": f"å·¥å…·åŠ è½½å¤±è´¥: {str(e)}"}
        
        if tool_name not in self.mcp_tools:
            return {"error": f"å·¥å…· '{tool_name}' ä¸å¯ç”¨"}
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # MCP JSON-RPCæ ¼å¼è¯·æ±‚ä½“
            jsonrpc_payload = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": arguments
                },
                "id": f"mcp_{tool_name}_{int(time.time())}"
            }
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    headers=headers,
                    json=jsonrpc_payload,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            result = None
                            request_id = jsonrpc_payload["id"]
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == request_id:  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            result = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            result = await response.json()
                        
                        if not result:
                            return {"error": "No response received"}
                        
                        # å¤„ç†MCP JSON-RPCå“åº”
                        if "result" in result:
                            return result["result"]
                        elif "error" in result:
                            return {"error": f"MCPé”™è¯¯: {result['error'].get('message', 'Unknown error')}"}
                        else:
                            return {"error": "æ— æ•ˆçš„MCPå“åº”æ ¼å¼"}
                    else:
                        return {"error": f"HTTP {response.status}: {await response.text()}"}
                        
        except asyncio.TimeoutError:
            logger.error(f"MCPå·¥å…·è°ƒç”¨è¶…æ—¶: {tool_name}")
            return {"error": "è¯·æ±‚è¶…æ—¶"}
        except aiohttp.ClientError as e:
            logger.error(f"MCP HTTPè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _execute_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """æ‰§è¡ŒMCPå·¥å…·å¹¶è¿”å›æ ¼å¼åŒ–ç»“æœ"""
        result = await self._call_mcp_tool(tool_name, arguments)
        
        # ä¸¥æ ¼å¤„ç†è¿”å›ç»“æœ
        if "error" in result:
            return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result['error']}"
        
        # å¤„ç†pubtator3 MCPæœåŠ¡çš„è¿”å›æ ¼å¼
        if "success" in result:
            if not result["success"]:
                return f"æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
            # æˆåŠŸçš„æœç´¢ç»“æœ
            if tool_name == "search_papers_by_abstract":
                papers = result.get("results", [])
                if not papers:
                    return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡"
                
                # æ ¼å¼åŒ–è®ºæ–‡ä¿¡æ¯
                formatted_result = f"æ‰¾åˆ° {result.get('total_count', len(papers))} ç¯‡ç›¸å…³è®ºæ–‡:\n\n"
                for i, paper in enumerate(papers[:10], 1):  # åªæ˜¾ç¤ºå‰10ç¯‡
                    formatted_result += f"{i}. **{paper.get('title', 'æ— é¢˜ç›®')}**\n"
                    if paper.get('authors'):
                        authors_str = ', '.join(paper['authors']) if isinstance(paper['authors'], list) else str(paper['authors'])
                        formatted_result += f"   ä½œè€…: {authors_str}\n"
                    if paper.get('date'):
                        formatted_result += f"   å‘è¡¨æ—¥æœŸ: {paper['date']}\n"
                    if paper.get('journal'):
                        formatted_result += f"   æœŸåˆŠ: {paper['journal']}\n"
                    if paper.get('pmid'):
                        formatted_result += f"   PMID: {paper['pmid']}\n"
                    if paper.get('abstract'):
                        abstract = paper['abstract'][:400] + "..." if len(paper['abstract']) > 400 else paper['abstract']
                        formatted_result += f"   æ‘˜è¦: {abstract}\n"
                        if paper.get('abstract_source'):
                            formatted_result += f"   æ‘˜è¦æ¥æº: {paper['abstract_source']}\n"
                    
                    # æ·»åŠ é“¾æ¥
                    links = []
                    if paper.get('doi'):
                        links.append(f"DOI: https://doi.org/{paper['doi']}")
                    if paper.get('pmid'):
                        links.append(f"PubMed: https://pubmed.ncbi.nlm.nih.gov/{paper['pmid']}")
                    if links:
                        formatted_result += f"   é“¾æ¥: {', '.join(links)}\n"
                    
                    formatted_result += "\n"
                
                return formatted_result
        
        # å¦‚æœæœ‰contentå­—æ®µï¼ˆå…¼å®¹å…¶ä»–MCPæœåŠ¡ï¼‰
        if "content" in result and result["content"]:
            text_content = ""
            for content in result["content"]:
                if content.get("type") == "text":
                    text_content += content.get("text", "") + "\n"
            return text_content.strip()
        
        # é»˜è®¤æƒ…å†µï¼šç›´æ¥è¿”å›JSONæ ¼å¼çš„ç»“æœ
        return json.dumps(result, ensure_ascii=False, indent=2)

    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œåªæœ‰system_promptä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ systemæ¶ˆæ¯
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        if system_prompt and system_prompt.strip():
            self._add_input_tokens(system_prompt)
        self._add_input_tokens(user_prompt)
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            answer = result["choices"][0]["message"]["content"]
            self._add_output_tokens(answer)
            
            return answer
            
        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return error_msg

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œåªæœ‰system_promptä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ systemæ¶ˆæ¯
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_output_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
                            
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    def _get_system_prompt(self) -> str:
        """åŠ¨æ€ç”ŸæˆåŸºäºå¯ç”¨MCPå·¥å…·çš„ç³»ç»Ÿæç¤ºè¯"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æœç´¢åŠ©æ‰‹ï¼Œèƒ½å¤Ÿä½¿ç”¨PubTator3 MCPå·¥å…·æ¥æŸ¥è¯¢å­¦æœ¯è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ã€‚

ğŸ”§ å¯ç”¨å·¥å…·ï¼š
"""
        
        # åŠ¨æ€æ·»åŠ å·¥å…·ä¿¡æ¯
        if self.mcp_tools:
            for tool_name, tool_info in self.mcp_tools.items():
                base_prompt += f"""
å·¥å…·åç§°: {tool_name}
æè¿°: {tool_info.get('description', 'æ— æè¿°')}
"""
                
                # æ·»åŠ è¾“å…¥å‚æ•°ä¿¡æ¯
                input_schema = tool_info.get('input_schema', {})
                if input_schema and 'properties' in input_schema:
                    base_prompt += "å‚æ•°:\n"
                    for param_name, param_info in input_schema['properties'].items():
                        param_type = param_info.get('type', 'unknown')
                        param_desc = param_info.get('description', 'æ— æè¿°')
                        param_required = param_name in input_schema.get('required', [])
                        required_str = " (å¿…éœ€)" if param_required else " (å¯é€‰)"
                        base_prompt += f"  - {param_name} ({param_type}){required_str}: {param_desc}\n"
                    base_prompt += "\n"
        else:
            base_prompt += "âš ï¸ å½“å‰æ²¡æœ‰å¯ç”¨çš„MCPå·¥å…·\n"
        
        base_prompt += """
ğŸ§  ä½¿ç”¨æŒ‡å—ï¼š
1. å½“ç”¨æˆ·è¯¢é—®å­¦æœ¯è®ºæ–‡ç›¸å…³ä¿¡æ¯æ—¶ï¼Œåˆ†æä»–ä»¬çš„éœ€æ±‚å¹¶é€‰æ‹©åˆé€‚çš„å·¥å…·
2. search_papers_by_abstractå·¥å…·ç”¨äºåŸºäºæ‘˜è¦å†…å®¹æœç´¢è®ºæ–‡ï¼Œæ”¯æŒå…³é”®è¯æœç´¢
3. è¯¥å·¥å…·ä¼šè¿”å›åŒ…å«PMIDã€æ ‡é¢˜ã€ä½œè€…ã€æœŸåˆŠã€å‘è¡¨æ—¥æœŸã€DOIå’Œå®Œæ•´æ‘˜è¦çš„è®ºæ–‡ä¿¡æ¯
4. æœç´¢æ—¶åº”ä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼Œæ•ˆæœæ›´å¥½
5. å¯ä»¥æ ¹æ®é¡µé¢å¤§å°ã€é¡µç ç­‰å‚æ•°ä¼˜åŒ–æœç´¢ç»“æœ
6. å·¥å…·æ”¯æŒå¤šæºæ‘˜è¦æ£€ç´¢ï¼Œä¼šä»Semantic Scholarã€OpenAlexã€CrossRefç­‰æ¥æºè·å–å®Œæ•´æ‘˜è¦

ğŸ” æœç´¢å»ºè®®ï¼š
- ä½¿ç”¨å…·ä½“çš„ç ”ç©¶ä¸»é¢˜å…³é”®è¯
- å¯ä»¥ç»„åˆå¤šä¸ªç›¸å…³æœ¯è¯­
- è€ƒè™‘ä½¿ç”¨ä½œè€…å§“åæœç´¢
- åˆ©ç”¨åˆ†é¡µåŠŸèƒ½è·å–æ›´å¤šç»“æœ
- å¯ç”¨å®Œæ•´æ‘˜è¦æ£€ç´¢è·å–è¯¦ç»†ä¿¡æ¯

ğŸ”„ å·¥å…·è°ƒç”¨æ ¼å¼ï¼š
å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·å›å¤ï¼š
TOOL_CALL:<å·¥å…·åç§°>:<JSONå‚æ•°>

ç¤ºä¾‹ï¼š
- TOOL_CALL:search_papers_by_abstract:{"query": "machine learning", "page_size": 10, "include_full_abstracts": true}
- TOOL_CALL:search_papers_by_abstract:{"query": "COVID-19 vaccine", "page": 1, "page_size": 15, "max_concurrent": 3}
- TOOL_CALL:search_papers_by_abstract:{"query": "CRISPR gene editing", "include_full_abstracts": false}

å¦‚æœä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·è·å–æ›´å®Œæ•´çš„ä¿¡æ¯ã€‚
"""
        
        return base_prompt

    def _build_conversation_context(self, user_message: str, messages: List[dict]) -> str:
        """æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
        # æ„å»ºå¯¹è¯å†å²
        conversation_history = []
        if messages and len(messages) > 1:
            # å–æœ€è¿‘çš„2è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆ4æ¡æ¶ˆæ¯ï¼‰
            recent_messages = messages[-4:] if len(messages) > 4 else messages
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role in ["user", "assistant"]:
                    # å¯¹å¯¹è¯å†…å®¹è¿›è¡Œ300é•¿åº¦æˆªæ–­
                    if len(content) > 300:
                        content = content[:300] + "..."
                    conversation_history.append({"role": role, "content": content})
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        conversation_history.append({"role": "user", "content": user_message})
        
        # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æç¤º
        context_text = ""
        for msg in conversation_history[:-1]:  # é™¤äº†æœ€åä¸€æ¡æ¶ˆæ¯
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            context_text += f"{role}: {msg['content']}\n"
        
        full_user_prompt = f"""å†å²å¯¹è¯ä¸Šä¸‹æ–‡:
{context_text if context_text else "æ— å†å²å¯¹è¯"}

å½“å‰ç”¨æˆ·é—®é¢˜: {user_message}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡å’Œå½“å‰é—®é¢˜ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨PubTator3 MCPå·¥å…·ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æŒ‰ç…§æŒ‡å®šæ ¼å¼å›å¤å·¥å…·è°ƒç”¨ã€‚
å›ç­”è¦å¿ äºä¸Šä¸‹æ–‡ã€å½“å‰é—®é¢˜ã€å·¥å…·è¿”å›çš„ä¿¡æ¯ã€‚"""
        
        return full_user_prompt

    def _parse_tool_call(self, ai_response: str) -> tuple[str, dict, str]:
        """è§£æAIå“åº”ä¸­çš„å·¥å…·è°ƒç”¨
        
        Returns:
            tuple: (tool_name, tool_args, error_message)
            å¦‚æœè§£ææˆåŠŸï¼Œerror_messageä¸ºNone
            å¦‚æœè§£æå¤±è´¥ï¼Œtool_nameå’Œtool_argsä¸ºNoneï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        """
        if not ai_response.startswith("TOOL_CALL:"):
            return None, None, None
        
        try:
            # è§£æå·¥å…·è°ƒç”¨
            tool_call_str = ai_response.replace("TOOL_CALL:", "", 1)
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå†’å·åˆ†éš”å·¥å…·åç§°å’Œå‚æ•°
            if ":" in tool_call_str:
                tool_name, tool_args_str = tool_call_str.split(":", 1)
                tool_name = tool_name.strip()
                tool_args = json.loads(tool_args_str)
            else:
                raise ValueError("æ— æ•ˆçš„å·¥å…·è°ƒç”¨æ ¼å¼")
            
            # éªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨
            if tool_name not in self.mcp_tools:
                raise ValueError(f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨")
            
            return tool_name, tool_args, None
            
        except (json.JSONDecodeError, ValueError) as e:
            return None, None, str(e)

    async def _execute_tool_call_with_feedback(self, tool_name: str, tool_args: dict, stream_mode: bool) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æä¾›è¿›åº¦åé¦ˆ"""
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
        call_info = f"ğŸ”§ æ­£åœ¨è°ƒç”¨PubTator3 MCPå·¥å…· '{tool_name}'..."
        if stream_mode:
            for chunk in self._emit_processing(call_info, "tool_calling"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield call_info + "\n"
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨å‚æ•°
        tool_info = f"ğŸ“ å·¥å…·è°ƒç”¨å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}"
        if stream_mode:
            for chunk in self._emit_processing(tool_info, "tool_calling"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield tool_info + "\n"
        
        # è°ƒç”¨MCPå·¥å…·
        tool_result = await self._execute_mcp_tool(tool_name, tool_args)
        try:
            json_tool_result = json.loads(tool_result)
            tool_result = json.dumps(json_tool_result, indent=4)
        except json.JSONDecodeError:
            pass
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœ
        result_info = f"âœ… å·¥å…·è°ƒç”¨ç»“æœ:\n```json\n{tool_result}\n```"
        if stream_mode:
            for chunk in self._emit_processing(result_info, "tool_calling"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield result_info + "\n"
        
        # æœ€åè¿”å›å·¥å…·ç»“æœï¼ˆä½œä¸ºç‰¹æ®Šæ ‡è®°çš„å­—ç¬¦ä¸²ï¼‰
        yield f"TOOL_RESULT:{tool_result}"

    async def _generate_final_answer(self, user_message: str, tool_result: str, tool_name: str, tool_args: dict, full_user_prompt: str, system_prompt: str, stream_mode: bool) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        if tool_result is not None:
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ç»“æœï¼ŒåŸºäºç»“æœç”Ÿæˆå›ç­”
            final_system_prompt = "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æœç´¢ä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„PubTator3 MCPå·¥å…·è°ƒç”¨ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚è¯·åŒ…å«ä½ çš„åˆ†æå’Œè§è§£ã€‚"
            
            tool_summary = f"""åŸºäºä»¥ä¸‹PubTator3 MCPå·¥å…·è°ƒç”¨ç»“æœ:

å·¥å…·è°ƒç”¨: {tool_name}
å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}
ç»“æœ: {tool_result}

ç”¨æˆ·é—®é¢˜: {user_message}

è¯·åŸºäºä»¥ä¸Šå·¥å…·è°ƒç”¨ç»“æœä¸ºç”¨æˆ·æä¾›å‡†ç¡®è¯¦ç»†çš„å›ç­”ï¼Œå¹¶åŠ å…¥ä½ çš„ä¸“ä¸šåˆ†æå’Œè§è§£ã€‚æ³¨æ„ï¼š
1. å°½é‡è¾“å‡ºå®Œæ•´çš„ä¿¡æ¯ï¼Œä¸è¦çœç•¥é‡è¦çš„è¯¦ç»†å†…å®¹
2. æœ‰èµ„æºåœ°å€ï¼ˆå¦‚DOIã€PubMedé“¾æ¥ç­‰ï¼‰æ—¶ï¼Œä½¿ç”¨markdownæ ¼å¼è¾“å‡ºå¯ç‚¹å‡»é“¾æ¥"""
            
            if stream_mode:
                # æµå¼æ¨¡å¼å¼€å§‹ç”Ÿæˆå›ç­”çš„æ ‡è¯†
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ“š åŸºäºPubTator3å·¥å…·è°ƒç”¨ç»“æœçš„ä¸“ä¸šåˆ†æ**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
                
                # æµå¼ç”Ÿæˆæœ€ç»ˆå›ç­”
                for chunk in self._stream_openai_response(tool_summary, final_system_prompt):
                    # åŒ…è£…æˆSSEæ ¼å¼
                    chunk_data = {
                        'choices': [{
                            'delta': {
                                'content': chunk
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
            else:
                yield "ğŸ“š **åŸºäºPubTator3å·¥å…·è°ƒç”¨ç»“æœçš„ä¸“ä¸šåˆ†æ**\n"
                final_answer = self._call_openai_api(final_system_prompt, tool_summary)
                yield final_answer
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›AIå“åº”
            if stream_mode:
                # æµå¼æ¨¡å¼
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ’­ å›ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
                
                for chunk in self._stream_openai_response(full_user_prompt, system_prompt):
                    chunk_data = {
                        'choices': [{
                            'delta': {
                                'content': chunk
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
            else:
                yield "ğŸ’­ **å›ç­”**\n"
                final_answer = self._call_openai_api(system_prompt, full_user_prompt)
                yield final_answer

    async def _process_user_message(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒå¤šè½®MCPå·¥å…·è°ƒç”¨ï¼ˆé‡æ„åçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        
        # ç¡®ä¿å·¥å…·å·²åŠ è½½ä¸”æœªè¿‡æœŸ
        try:
            async for tools_output in self._ensure_tools_loaded(stream_mode):
                yield tools_output
        except Exception as e:
            error_msg = f"âŒ PubTator3 MCPå·¥å…·åŠ è½½å¤±è´¥: {str(e)}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            return
        
        # è·å–ç³»ç»Ÿæç¤ºè¯å’Œæ„å»ºä¸Šä¸‹æ–‡
        system_prompt = self._get_system_prompt()
        full_user_prompt = self._build_conversation_context(user_message, messages)
        
        # æ˜¾ç¤ºAIå†³ç­–è¿›åº¦
        decision_msg = "ğŸ¤” æ­£åœ¨åˆ†æç”¨æˆ·é—®é¢˜ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨PubTator3å·¥å…·..."
        if stream_mode:
            for chunk in self._emit_processing(decision_msg, "tool_calling"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield decision_msg + "\n"
        
        # å°è¯•è§£æå·¥å…·è°ƒç”¨ï¼Œå¦‚æœå¤±è´¥åˆ™é‡è¯•
        tool_name, tool_args = None, None
        
        for retry_count in range(self.valves.MAX_TOOL_CALLS):
            ai_response = self._call_openai_api(system_prompt, full_user_prompt)
            tool_name, tool_args, parse_error = self._parse_tool_call(ai_response)
            
            if parse_error is None:
                # è§£ææˆåŠŸæˆ–æ— éœ€å·¥å…·è°ƒç”¨
                if tool_name is None:
                    # ä¸éœ€è¦å·¥å…·è°ƒç”¨
                    no_tool_msg = "ğŸ’­ AIå†³å®šæ— éœ€è°ƒç”¨å·¥å…·ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                    if stream_mode:
                        for chunk in self._emit_processing(no_tool_msg, "answer_generation"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield no_tool_msg + "\n"
                break
            else:
                # è§£æå¤±è´¥ï¼Œé‡è¯•
                if retry_count < self.valves.MAX_TOOL_CALLS - 1:
                    retry_msg = f"âš ï¸ å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯({parse_error})ï¼Œæ­£åœ¨é‡è¯•({retry_count + 1}/{self.valves.MAX_TOOL_CALLS})..."
                    if stream_mode:
                        for chunk in self._emit_processing(retry_msg, "tool_calling"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield retry_msg + "\n"
                    continue
                else:
                    # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    error_msg = f"âŒ å·¥å…·è°ƒç”¨è§£æå¤±è´¥ï¼Œå·²é‡è¯•{self.valves.MAX_TOOL_CALLS}æ¬¡: {parse_error}"
                    if stream_mode:
                        for chunk in self._emit_processing(error_msg, "tool_calling"):
                            yield f'data: {json.dumps(chunk)}\n\n'
                    else:
                        yield error_msg + "\n"
                    return
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        tool_result = None
        if tool_name and tool_args:
            async for output in self._execute_tool_call_with_feedback(tool_name, tool_args, stream_mode):
                if isinstance(output, str) and output.startswith('TOOL_RESULT:'):
                    # è¿™æ˜¯æœ€ç»ˆçš„å·¥å…·ç»“æœ
                    tool_result = output.replace('TOOL_RESULT:', '', 1)
                else:
                    # è¿™æ˜¯è¿›åº¦åé¦ˆ
                    yield output
        
        # ç”Ÿæˆæœ€ç»ˆå›ç­”
        async for output in self._generate_final_answer(
            user_message, tool_result, tool_name, tool_args, 
            full_user_prompt, system_prompt, stream_mode
        ):
            yield output

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ“š PubTator3 MCPå­¦æœ¯è®ºæ–‡åŠ©æ‰‹æ”¶åˆ°æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")
            print(f"ğŸ”— MCPæœåŠ¡å™¨: {self.valves.MCP_SERVER_URL}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å­¦æœ¯è®ºæ–‡æœç´¢é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼æ¨¡å¼  
        stream_mode = self.valves.ENABLE_STREAMING
        
        try:
            # MCPæœåŠ¡å‘ç°é˜¶æ®µ
            if stream_mode:
                for chunk in self._emit_processing("ğŸ” æ­£åœ¨å‡†å¤‡PubTator3 MCPæœåŠ¡...", "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ” **é˜¶æ®µ1**: æ­£åœ¨å‡†å¤‡PubTator3 MCPæœåŠ¡...\n"
            
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç  - çœŸæ­£çš„æµå¼å¤„ç†
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨
                async_gen = self._process_user_message(user_message, messages, stream_mode)
                
                # æµå¼å¤„ç†æ¯ä¸ªç»“æœ
                while True:
                    try:
                        result = loop.run_until_complete(async_gen.__anext__())
                        yield result
                    except StopAsyncIteration:
                        break
                
                # æœ€åå‘é€å®Œæˆä¿¡æ¯
                if stream_mode:
                    # æµå¼æ¨¡å¼ç»“æŸ
                    done_msg = {
                        'choices': [{
                            'delta': {},
                            'finish_reason': 'stop'
                        }]
                    }
                    yield f"data: {json.dumps(done_msg)}\n\n"
                    yield "data: [DONE]\n\n"
                    
                    # æ·»åŠ tokenç»Ÿè®¡
                    token_info = self._get_token_stats()
                    token_msg = {
                        'choices': [{
                            'delta': {
                                'content': f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
                            },
                            'finish_reason': None
                        }]
                    }
                    yield f"data: {json.dumps(token_msg)}\n\n"
                else:
                    # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                    token_info = self._get_token_stats()
                    yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
                    
            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            
            if stream_mode:
                error_chunk = {
                    'choices': [{
                        'delta': {
                            'content': error_msg
                        },
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield error_msg
