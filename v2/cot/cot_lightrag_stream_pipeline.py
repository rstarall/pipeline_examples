"""
title: COT LightRAG Stream Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: ä¸€ä¸ªåŒç®¡çº¿CoTç³»ç»Ÿæµå¼ç‰ˆæœ¬ï¼šç®¡çº¿1ç›´æ¥ç”ŸæˆåŸå§‹å›ç­”(å¯¹æ¯”)ï¼Œç®¡çº¿2å¤šæ­¥éª¤é€’è¿›æ£€ç´¢(3é˜¶æ®µLightRAGæµå¼)ï¼Œä¸¤æ¡ç®¡çº¿å¹¶è¡Œæ‰§è¡Œ
requirements: requests, pydantic, openai
"""

import os
import json
import requests
import time
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel

# ===========================================
# æç¤ºè¯æ¨¡æ¿å®šä¹‰
# ===========================================

# é—®é¢˜ä¼˜åŒ–é˜¶æ®µ - æ•´åˆå†å²é—®é¢˜è¿›è¡Œæ„å›¾åˆ¤æ–­å’Œæ‰©å†™ä¼˜åŒ–
QUESTION_OPTIMIZATION_PROMPT = """ä½ æ˜¯é—®é¢˜åˆ†æä¼˜åŒ–ä¸“å®¶ã€‚è¯·åŸºäºç”¨æˆ·çš„å†å²é—®é¢˜å’Œå½“å‰é—®é¢˜ï¼Œåˆ¤æ–­ç”¨æˆ·çš„çœŸå®æ„å›¾å¹¶ä¼˜åŒ–å½“å‰é—®é¢˜ã€‚

ç”¨æˆ·å†å²é—®é¢˜ï¼š
{history_questions}

ç”¨æˆ·å½“å‰é—®é¢˜ï¼š{current_query}

è¯·åˆ†æï¼š
1. ç”¨æˆ·çš„é—®é¢˜èƒŒæ™¯å’Œä¸Šä¸‹æ–‡
2. å½“å‰é—®é¢˜ä¸å†å²é—®é¢˜çš„å…³è”
3. ç”¨æˆ·çš„çœŸå®æ„å›¾å’Œéœ€æ±‚

åŸºäºåˆ†æç»“æœï¼Œè¾“å‡ºä¸€ä¸ªä¼˜åŒ–åçš„é—®é¢˜ï¼Œè¦æ±‚ï¼š
- æ•´åˆç›¸å…³çš„å†å²é—®é¢˜ä¿¡æ¯
- æ˜ç¡®é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
- è¡¥å……å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- ä¿æŒç®€æ´æ˜ç¡®

ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„é—®é¢˜ï¼ˆçº¦100å­—ä»¥å†…ï¼‰ï¼š"""

# ç®¡çº¿1 - ç›´æ¥å›ç­”æç¤ºè¯æ¨¡æ¿
PIPELINE1_DIRECT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ã€‚è¯·ç›´æ¥æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›ä¸€ä¸ªå…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

å¯¹è¯å†å²å‚è€ƒï¼š
{context}

è¯·æä¾›ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œæ¶µç›–é—®é¢˜çš„å„ä¸ªæ–¹é¢ã€‚å›ç­”åº”è¯¥åŸºäºä½ çš„çŸ¥è¯†å‚¨å¤‡ï¼Œä¿æŒå®¢è§‚ã€å‡†ç¡®ã€‚"""

# ç®¡çº¿2 - ç¬¬ä¸€é˜¶æ®µæ£€ç´¢é—®é¢˜ä¼˜åŒ–æ¨¡æ¿
STAGE1_QUERY_OPTIMIZATION_PROMPT = """ä½ æ˜¯æ£€ç´¢é—®é¢˜åˆ†æä¸“å®¶ã€‚è¯·åŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆç¬¬ä¸€é˜¶æ®µçš„å…·ä½“æ£€ç´¢æŸ¥è¯¢ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_query}
å¯¹è¯ä¸Šä¸‹æ–‡: {context}

è¯·æå–ç”¨æˆ·é—®é¢˜ä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€å…³é”®å®ä½“å’Œæ½œåœ¨éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªå…·ä½“çš„åˆå§‹æ£€ç´¢æŸ¥è¯¢ã€‚æŸ¥è¯¢åº”è¯¥é’ˆå¯¹é—®é¢˜çš„åŸºç¡€ç»´åº¦ï¼ŒåŒ…å«å…·ä½“çš„åè¯ã€æœ¯è¯­æˆ–æ¦‚å¿µã€‚

ç›´æ¥è¾“å‡ºçº¦150å­—çš„å…·ä½“æ£€ç´¢æŸ¥è¯¢ï¼š"""

# ç®¡çº¿2 - ç¬¬äºŒé˜¶æ®µé—®é¢˜ä¼˜åŒ–æ¨¡æ¿
STAGE2_QUERY_REFINEMENT_PROMPT = """ä½ æ˜¯æ£€ç´¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·ä»ç¬¬ä¸€é˜¶æ®µçš„æ£€ç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç¬¬äºŒé˜¶æ®µçš„æ·±åº¦æŸ¥è¯¢ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_query}
ç¬¬ä¸€é˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage1_query}
ç¬¬ä¸€é˜¶æ®µæ£€ç´¢ç»“æœ: {stage1_results}

è¯·ä»”ç»†åˆ†æç¬¬ä¸€é˜¶æ®µçš„æ£€ç´¢ç»“æœï¼Œæå–å…¶ä¸­çš„ï¼šå…³é”®åè¯ã€å­¦æœ¯æœ¯è¯­ã€é‡è¦æ¦‚å¿µã€å®ä½“å…³ç³»ã€æ•°æ®æŒ‡æ ‡ç­‰å…·ä½“å†…å®¹ã€‚åŸºäºè¿™äº›æå–çš„ä¿¡æ¯ï¼Œæ€è€ƒåŸå§‹é—®é¢˜ä¸­å“ªäº›æ–¹é¢éœ€è¦è¿›ä¸€æ­¥æ·±å…¥ï¼Ÿä»€ä¹ˆå…³é”®ä¿¡æ¯è¿˜ç¼ºå¤±ï¼Ÿç„¶åç”Ÿæˆä¸€ä¸ªæ›´å…·ä½“ã€æ›´æ·±å…¥çš„ç¬¬äºŒé˜¶æ®µæ£€ç´¢æŸ¥è¯¢ã€‚

ç›´æ¥è¾“å‡ºçº¦150å­—çš„ä¼˜åŒ–æ£€ç´¢æŸ¥è¯¢ï¼š"""

# ç®¡çº¿2 - ç¬¬ä¸‰é˜¶æ®µç»¼åˆåˆ†ææ¨¡æ¿
STAGE3_COMPREHENSIVE_PROMPT = """ä½ æ˜¯ä¿¡æ¯ç»¼åˆåˆ†æä¸“å®¶ã€‚è¯·æ•´åˆå‰ä¸¤é˜¶æ®µçš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆç¬¬ä¸‰é˜¶æ®µçš„ç»¼åˆæŸ¥è¯¢ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_query}

ç¬¬ä¸€é˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage1_query}
ç¬¬ä¸€é˜¶æ®µæ£€ç´¢ç»“æœ: {stage1_results}

ç¬¬äºŒé˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage2_query}
ç¬¬äºŒé˜¶æ®µæ£€ç´¢ç»“æœ: {stage2_results}

è¯·æ·±å…¥åˆ†æå‰ä¸¤é˜¶æ®µæ£€ç´¢ç»“æœä¸­çš„ï¼šå…·ä½“æ•°æ®ã€å…¬å¼ç¬¦å·ã€æŠ€æœ¯ç»†èŠ‚ã€æ¡ˆä¾‹å®ä¾‹ã€å› æœå…³ç³»ã€å¯¹æ¯”åˆ†æç­‰å†…å®¹ã€‚æ€è€ƒè¿™äº›ä¿¡æ¯ä¹‹é—´çš„å…³è”æ€§ï¼Œè¯†åˆ«çŸ¥è¯†ä½“ç³»ä¸­çš„å…³é”®ç¯èŠ‚ã€‚åŸºäºè¿™äº›å…·ä½“ä¿¡æ¯ï¼Œä»åŸå§‹é—®é¢˜çš„è§’åº¦æ€è€ƒï¼šè¿˜éœ€è¦å“ªäº›å…·ä½“çš„è¡¥å……ä¿¡æ¯æ‰èƒ½å®Œæ•´å›ç­”ï¼Ÿç”Ÿæˆä¸€ä¸ªé’ˆå¯¹æ€§å¼ºçš„ç¬¬ä¸‰é˜¶æ®µæŸ¥è¯¢ã€‚

ç›´æ¥è¾“å‡ºçº¦150å­—çš„ç»¼åˆæ£€ç´¢æŸ¥è¯¢ï¼š"""

# ç®¡çº¿2 - æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæ¨¡æ¿
FINAL_ANSWER_GENERATION_PROMPT = """ä½ æ˜¯çŸ¥è¯†æ•´åˆä¸“å®¶ã€‚è¯·åŸºäºä¸‰ä¸ªé˜¶æ®µçš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„æœ€ç»ˆå›ç­”ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_query}

ç¬¬ä¸€é˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage1_query}
ç¬¬ä¸€é˜¶æ®µæ£€ç´¢ç»“æœ: {stage1_results}

ç¬¬äºŒé˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage2_query}
ç¬¬äºŒé˜¶æ®µæ£€ç´¢ç»“æœ: {stage2_results}

ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢æŸ¥è¯¢: {stage3_query}
ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢ç»“æœ: {stage3_results}

è¯·ç»¼åˆä¸‰ä¸ªé˜¶æ®µçš„æ£€ç´¢ä¿¡æ¯ï¼Œå¤§é‡ç›´æ¥å¼•ç”¨æ£€ç´¢ç»“æœä¸­çš„åŸå§‹æ–‡å­—ã€æ•°æ®ã€æ¡ˆä¾‹ã€å…¬å¼ç­‰å…·ä½“å†…å®¹ã€‚ä¿æŒä¿¡æ¯çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼ŒæŒ‰é€»è¾‘å±‚æ¬¡ç»„ç»‡ç­”æ¡ˆï¼Œç¡®ä¿å†…å®¹ä¸°å¯Œè¯¦å®ã€‚ä¼˜å…ˆä½¿ç”¨æ£€ç´¢åˆ°çš„å…·ä½“ä¿¡æ¯è€Œéæ³›æ³›è€Œè°ˆã€‚

è¯·ç”Ÿæˆä¸€ä¸ªè¯¦å®å®Œæ•´çš„æœ€ç»ˆå›ç­”ï¼š"""

# ===========================================
# é˜¶æ®µé…ç½®å®šä¹‰
# ===========================================

# åç«¯é˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    # é—®é¢˜ä¼˜åŒ–é˜¶æ®µ
    "question_optimization": "é—®é¢˜æ„å›¾åˆ†æä¸ä¼˜åŒ–",
    
    # ç®¡çº¿1é˜¶æ®µ
    "direct_answer": "åŸå§‹å›ç­”ç”Ÿæˆ",
    
    # ç®¡çº¿2é˜¶æ®µ
    "stage1_retrieval": "ç¬¬ä¸€é˜¶æ®µæ£€ç´¢",
    "stage2_retrieval": "ç¬¬äºŒé˜¶æ®µæ£€ç´¢", 
    "stage3_retrieval": "ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢",
    "final_synthesis": "æœ€ç»ˆç­”æ¡ˆç»¼åˆ",
}

# é˜¶æ®µç»„é…ç½® - ç”¨äºåŒºåˆ†ä¸¤æ¡å¹¶è¡Œç®¡çº¿
STAGE_GROUP = {
    # é—®é¢˜ä¼˜åŒ–é˜¶æ®µ
    "question_optimization": "preprocessing",
    
    # ç®¡çº¿1 - ç›´æ¥å›ç­”ç®¡çº¿
    "direct_answer": "pipeline_1_direct",
    
    # ç®¡çº¿2 - å¤šé˜¶æ®µæ£€ç´¢ç®¡çº¿
    "stage1_retrieval": "pipeline_2_stage1",
    "stage2_retrieval": "pipeline_2_stage2", 
    "stage3_retrieval": "pipeline_2_stage3",
    "final_synthesis": "pipeline_2_final",
}


class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®ï¼ˆç”¨äºé—®é¢˜ä¼˜åŒ–å’Œç›´æ¥å›ç­”ï¼‰
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # LightRAGé…ç½®
        LIGHTRAG_BASE_URL: str
        LIGHTRAG_DEFAULT_MODE: str
        LIGHTRAG_TIMEOUT: int
        LIGHTRAG_ENABLE_STREAMING: bool

        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        ENABLE_PARALLEL_EXECUTION: bool

    def __init__(self):
        self.name = "COT LightRAG Stream Pipeline"
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),

                # LightRAGé…ç½®
                "LIGHTRAG_BASE_URL": os.getenv("LIGHTRAG_BASE_URL", "http://117.50.252.245:9621"),
                "LIGHTRAG_DEFAULT_MODE": os.getenv("LIGHTRAG_DEFAULT_MODE", "hybrid"),
                "LIGHTRAG_TIMEOUT": int(os.getenv("LIGHTRAG_TIMEOUT", "30")),
                "LIGHTRAG_ENABLE_STREAMING": os.getenv("LIGHTRAG_ENABLE_STREAMING", "true").lower() == "true",

                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "ENABLE_PARALLEL_EXECUTION": os.getenv("ENABLE_PARALLEL_EXECUTION", "true").lower() == "true",
            }
        )

    async def on_startup(self):
        print(f"COT LightRAG Stream Pipelineå¯åŠ¨: {__name__}")

        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")

        # æµ‹è¯•LightRAGè¿æ¥
        try:
            response = requests.get(f"{self.valves.LIGHTRAG_BASE_URL}/health", timeout=5)
            if response.status_code == 200:
                print("âœ… LightRAGæœåŠ¡è¿æ¥æˆåŠŸ")
            else:
                print(f"âš ï¸ LightRAGæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°LightRAGæœåŠ¡: {e}")

    async def on_shutdown(self):
        print(f"COT LightRAG Stream Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """
        ç®€å•çš„tokenä¼°ç®—å‡½æ•°ï¼ŒåŸºäºå­—ç¬¦æ•°ä¼°ç®—
        ä¸­æ–‡å­—ç¬¦æŒ‰1ä¸ªtokenè®¡ç®—ï¼Œè‹±æ–‡å•è¯æŒ‰å¹³å‡1.3ä¸ªtokenè®¡ç®—
        """
        if not text:
            return 0

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')

        # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])

        # ä¼°ç®—tokenæ•°ï¼šä¸­æ–‡å­—ç¬¦1:1ï¼Œè‹±æ–‡å•è¯1:1.3
        estimated_tokens = chinese_chars + int(english_words * 1.3)

        return max(estimated_tokens, 1)  # è‡³å°‘è¿”å›1ä¸ªtoken

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _get_history_questions(self, messages: List[dict], max_questions: int = 5) -> str:
        """ä»æ¶ˆæ¯å†å²ä¸­æå–ç”¨æˆ·çš„å†å²é—®é¢˜ï¼ˆä¸åŒ…æ‹¬AIå›ç­”ï¼‰"""
        if not messages:
            return "æ— å†å²é—®é¢˜"

        history_questions = []
        
        # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ï¼Œå€’åºæå–ç”¨æˆ·é—®é¢˜
        for message in reversed(messages[-10:]):  # æœ€å¤šå–æœ€è¿‘10æ¡æ¶ˆæ¯
            role = message.get("role", "user")
            content = message.get("content", "").strip()

            # åªæå–ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸åŒ…æ‹¬AIçš„å›ç­”
            if content and role == "user":
                history_questions.append(content)
                
                if len(history_questions) >= max_questions:
                    break

        # æ¢å¤æ­£åºå¹¶æ ¼å¼åŒ–
        history_questions.reverse()
        if history_questions:
            formatted_questions = []
            for i, question in enumerate(history_questions, 1):
                formatted_questions.append(f"{i}. {question}")
            return "\n".join(formatted_questions)
        else:
            return "æ— å†å²é—®é¢˜"

    def _get_conversation_context(self, messages: List[dict], max_context_length: int = 500) -> str:
        """ä»æ¶ˆæ¯å†å²ä¸­æå–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if not messages:
            return "æ— å†å²å¯¹è¯"

        context_parts = []
        total_length = 0

        # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ï¼Œå€’åºæå–ä¸Šä¸‹æ–‡
        for message in reversed(messages[-5:]):  # æœ€å¤šå–æœ€è¿‘5æ¡æ¶ˆæ¯
            role = message.get("role", "user")
            content = message.get("content", "").strip()

            if content and role in ["user", "assistant"]:
                role_text = "ç”¨æˆ·" if role == "user" else "åŠ©æ‰‹"
                msg_text = f"{role_text}: {content}"

                if total_length + len(msg_text) > max_context_length:
                    break

                context_parts.append(msg_text)
                total_length += len(msg_text)

        # æ¢å¤æ­£åº
        context_parts.reverse()
        return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"

    def _call_openai_api(self, messages: List[dict], stream: bool = False, max_retries: int = 2) -> dict:
        """è°ƒç”¨OpenAI APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": stream
        }

        # ç»Ÿè®¡è¾“å…¥token
        for message in messages:
            self._add_input_tokens(message.get("content", ""))

        for attempt in range(max_retries + 1):
            try:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT,
                    stream=stream
                )
                response.raise_for_status()
                
                if stream:
                    return response
                else:
                    result = response.json()
                    # ç»Ÿè®¡è¾“å‡ºtoken
                    if "choices" in result and result["choices"]:
                        content = result["choices"][0].get("message", {}).get("content", "")
                        self._add_output_tokens(content)
                    return result

            except Exception as e:
                if attempt < max_retries:
                    if self.valves.DEBUG_MODE:
                        print(f"âš ï¸ OpenAI APIè°ƒç”¨å¤±è´¥ï¼Œ{2}ç§’åé‡è¯•: {str(e)}")
                    time.sleep(2)
                    continue
                else:
                    return {"error": f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}"}

    def _query_lightrag_stream(self, query: str, mode: str = None, stage: str = "retrieval", stream_callback=None) -> dict:
        """è°ƒç”¨LightRAGè¿›è¡Œæµå¼æ£€ç´¢ï¼Œæ”¯æŒå®æ—¶æµå¼è¾“å‡º"""
        if not mode:
            mode = self.valves.LIGHTRAG_DEFAULT_MODE

        url = f"{self.valves.LIGHTRAG_BASE_URL}/query/stream"
        payload = {
            "query": query,
            "mode": mode
        }

        headers = {
            "Content-Type": "application/json",
            "Accept": "application/x-ndjson",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }

        # ç»Ÿè®¡è¾“å…¥token
        self._add_input_tokens(query)

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.LIGHTRAG_TIMEOUT,
                stream=True
            )
            response.raise_for_status()
            
            collected_content = ""
            buffer = ""
            
            # ä½¿ç”¨ç¼“å†²åŒºå¤„ç†NDJSONæµå¼å“åº”
            for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):
                if chunk:
                    buffer += chunk
                    
                    # æŒ‰è¡Œåˆ†å‰²å¤„ç†NDJSON
                    while '\n' in buffer:
                        line, buffer = buffer.split('\n', 1)
                        line = line.strip()
                        
                        if line:
                            try:
                                json_data = json.loads(line)
                                
                                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                                if "error" in json_data:
                                    return {"error": f"LightRAGæŸ¥è¯¢å¤±è´¥: {json_data['error']}"}
                                
                                # å¤„ç†æ­£å¸¸å“åº”
                                if "response" in json_data:
                                    response_content = json_data["response"]
                                    collected_content += response_content
                                    
                                    # ç«‹å³è¿›è¡Œæµå¼è¾“å‡ºï¼ˆå¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼‰
                                    if stream_callback:
                                        for chunk_emit in self._emit_processing(response_content, stage):
                                            stream_callback(f"data: {json.dumps(chunk_emit)}\n\n")
                                    
                            except json.JSONDecodeError as e:
                                if self.valves.DEBUG_MODE:
                                    print(f"è­¦å‘Š: JSONè§£æé”™è¯¯: {line} - {str(e)}")
                                continue
            
            # å¤„ç†æœ€åçš„ç¼“å†²åŒºå†…å®¹
            if buffer.strip():
                try:
                    json_data = json.loads(buffer.strip())
                    if "response" in json_data:
                        response_content = json_data["response"]
                        collected_content += response_content
                        if stream_callback:
                            for chunk_emit in self._emit_processing(response_content, stage):
                                stream_callback(f"data: {json.dumps(chunk_emit)}\n\n")
                    elif "error" in json_data:
                        return {"error": f"LightRAGæŸ¥è¯¢å¤±è´¥: {json_data['error']}"}
                except json.JSONDecodeError:
                    if self.valves.DEBUG_MODE:
                        print(f"è­¦å‘Š: æ— æ³•è§£ææœ€åçš„å“åº”ç‰‡æ®µ: {buffer}")
                        
            # ç»Ÿè®¡è¾“å‡ºtoken
            if collected_content:
                self._add_output_tokens(collected_content)
                
            return {"response": collected_content} if collected_content else {"error": "æœªè·å–åˆ°æ£€ç´¢ç»“æœ"}
                
        except Exception as e:
            return {"error": f"LightRAGæµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}"}



    def _query_lightrag(self, query: str, mode: str = None) -> dict:
        """è°ƒç”¨LightRAGè¿›è¡Œæ£€ç´¢ï¼ˆéæµå¼ç‰ˆæœ¬ï¼‰"""
        if not mode:
            mode = self.valves.LIGHTRAG_DEFAULT_MODE

        url = f"{self.valves.LIGHTRAG_BASE_URL}/query"
        payload = {
            "query": query,
            "mode": mode
        }

        headers = {"Content-Type": "application/json"}

        # ç»Ÿè®¡è¾“å…¥token
        self._add_input_tokens(query)

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            # ç»Ÿè®¡è¾“å‡ºtoken
            if "response" in result:
                self._add_output_tokens(result["response"])
                
            return result
        except Exception as e:
            return {"error": f"LightRAGæŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def _emit_processing(self, message: str, stage: str) -> Generator[dict, None, None]:
        """
        å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹ - ä½¿ç”¨processing_contentå­—æ®µå®ç°æŠ˜å æ˜¾ç¤º

        Args:
            message: å¤„ç†å†…å®¹
            stage: å¤„ç†é˜¶æ®µ

        Yields:
            å¤„ç†äº‹ä»¶
        """
        yield {
            'choices': [{
                'delta': {
                    'processing_content': message,
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")  # æ·»åŠ stageä¿¡æ¯ç”¨äºç»„ä»¶åŒºåˆ†
                },
                'finish_reason': None
            }]
        }

    def _emit_status(self, description: str, done: bool = False) -> Generator[dict, None, None]:
        """
        å‘é€çŠ¶æ€äº‹ä»¶ - ä¸æŠ˜å æ˜¾ç¤ºçš„çŠ¶æ€ä¿¡æ¯

        Args:
            description: çŠ¶æ€æè¿°
            done: æ˜¯å¦å®Œæˆ

        Yields:
            çŠ¶æ€äº‹ä»¶
        """
        yield {
            "event": {
                "type": "status",
                "data": {
                    "description": description,
                    "done": done,
                },
            }
        }

    # ===========================================
    # é—®é¢˜ä¼˜åŒ–é˜¶æ®µ
    # ===========================================

    def _question_optimization(self, messages: List[dict], current_query: str) -> str:
        """é—®é¢˜ä¼˜åŒ–é˜¶æ®µï¼šæ•´åˆå†å²é—®é¢˜è¿›è¡Œæ„å›¾åˆ¤æ–­å’Œæ‰©å†™ä¼˜åŒ–"""
        try:
            history_questions = self._get_history_questions(messages)
            
            prompt = QUESTION_OPTIMIZATION_PROMPT.format(
                history_questions=history_questions,
                current_query=current_query
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            response = self._call_openai_api(messages_for_api, stream=False)
            
            if "error" in response:
                return current_query  # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹é—®é¢˜
                
            optimized_query = response.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            # å¦‚æœä¼˜åŒ–ç»“æœä¸ºç©ºï¼Œè¿”å›åŸå§‹é—®é¢˜
            return optimized_query if optimized_query else current_query
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ é—®é¢˜ä¼˜åŒ–å¤±è´¥: {e}")
            return current_query  # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œè¿”å›åŸå§‹é—®é¢˜

    # ===========================================
    # ç®¡çº¿1 - ç›´æ¥å›ç­”ç”Ÿæˆ
    # ===========================================

    def _pipeline1_direct_answer_stream(self, optimized_query: str, messages: List[dict]) -> Generator[str, None, None]:
        """ç®¡çº¿1ï¼šç›´æ¥ç”ŸæˆåŸå§‹å›ç­”ï¼Œç”¨äºå¯¹æ¯” - æ”¯æŒæµå¼è¾“å‡º"""
        try:
            context = self._get_conversation_context(messages)
            
            prompt = PIPELINE1_DIRECT_PROMPT.format(
                query=optimized_query,
                context=context
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            
            # ä½¿ç”¨æµå¼APIè°ƒç”¨
            response = self._call_openai_api(messages_for_api, stream=True)
            
            if isinstance(response, dict) and "error" in response:
                error_msg = f"ç›´æ¥å›ç­”ç”Ÿæˆå¤±è´¥: {response['error']}"
                for chunk in self._emit_processing(error_msg, "direct_answer"):
                    yield f"data: {json.dumps(chunk)}\n\n"
                return
            
            # å¤„ç†æµå¼å“åº”
            collected_content = ""
            for line in response.iter_lines():
                if not line or not line.strip():
                    continue

                line = line.decode('utf-8')    
                if line.startswith("data: "):
                    data_content = line[6:].strip()
                    
                    if data_content == "[DONE]":
                        break
                        
                    try:
                        data = json.loads(data_content)
                        if "choices" in data and data["choices"]:
                            delta = data["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            
                            if content:
                                collected_content += content
                                # ä½¿ç”¨_emit_processingæµå¼è¾“å‡ºå†…å®¹
                                for chunk in self._emit_processing(content, "direct_answer"):
                                    yield f"data: {json.dumps(chunk)}\n\n"
                                    
                    except (json.JSONDecodeError, KeyError):
                        continue
            
            # ç»Ÿè®¡è¾“å‡ºtoken
            self._add_output_tokens(collected_content)
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ ç®¡çº¿1ç›´æ¥å›ç­”å¤±è´¥: {e}")
            error_msg = f"ç›´æ¥å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            for chunk in self._emit_processing(error_msg, "direct_answer"):
                yield f"data: {json.dumps(chunk)}\n\n"

    # ===========================================
    # ç®¡çº¿2 - å¤šé˜¶æ®µæ£€ç´¢ç®¡çº¿ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
    # ===========================================

    def _pipeline2_stage1_retrieval_stream(self, optimized_query: str, messages: List[dict], stream_callback=None) -> tuple:
        """ç®¡çº¿2é˜¶æ®µ1ï¼šåˆå§‹æ£€ç´¢ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        try:
            context = self._get_conversation_context(messages)
            
            # ç”Ÿæˆç¬¬ä¸€é˜¶æ®µæ£€ç´¢æŸ¥è¯¢
            prompt = STAGE1_QUERY_OPTIMIZATION_PROMPT.format(
                original_query=optimized_query,
                context=context
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            response = self._call_openai_api(messages_for_api, stream=False)
            
            if "error" in response:
                return f"ç¬¬ä¸€é˜¶æ®µæŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {response['error']}", ""
                
            stage1_query = response.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            # æ‰§è¡ŒLightRAGæ£€ç´¢ï¼ˆä½¿ç”¨æµå¼æŸ¥è¯¢è·å–ç»“æœï¼‰
            lightrag_result = self._query_lightrag_stream(stage1_query, stage="stage1_retrieval", stream_callback=stream_callback)
            
            if "error" in lightrag_result:
                return stage1_query, f"ç¬¬ä¸€é˜¶æ®µæ£€ç´¢å¤±è´¥: {lightrag_result['error']}"
                
            stage1_results = lightrag_result.get("response", "æœªè·å–åˆ°æ£€ç´¢ç»“æœ")
            
            return stage1_query, stage1_results
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ ç®¡çº¿2é˜¶æ®µ1å¤±è´¥: {e}")
            return "", f"ç¬¬ä¸€é˜¶æ®µæ£€ç´¢å¤±è´¥: {str(e)}"



    def _pipeline2_stage2_retrieval_stream(self, optimized_query: str, stage1_query: str, stage1_results: str, stream_callback=None) -> tuple:
        """ç®¡çº¿2é˜¶æ®µ2ï¼šåŸºäºç¬¬ä¸€é˜¶æ®µç»“æœçš„æ·±åº¦æ£€ç´¢ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        try:
            # ç”Ÿæˆç¬¬äºŒé˜¶æ®µæ£€ç´¢æŸ¥è¯¢
            prompt = STAGE2_QUERY_REFINEMENT_PROMPT.format(
                original_query=optimized_query,
                stage1_query=stage1_query,
                stage1_results=stage1_results
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            response = self._call_openai_api(messages_for_api, stream=False)
            
            if "error" in response:
                return f"ç¬¬äºŒé˜¶æ®µæŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {response['error']}", ""
                
            stage2_query = response.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            # æ‰§è¡ŒLightRAGæ£€ç´¢ï¼ˆä½¿ç”¨æµå¼ç‰ˆæœ¬è·å–ç»“æœï¼‰
            lightrag_result = self._query_lightrag_stream(stage2_query, stage="stage2_retrieval", stream_callback=stream_callback)
            
            if "error" in lightrag_result:
                return stage2_query, f"ç¬¬äºŒé˜¶æ®µæ£€ç´¢å¤±è´¥: {lightrag_result['error']}"
                
            stage2_results = lightrag_result.get("response", "æœªè·å–åˆ°æ£€ç´¢ç»“æœ")
            
            return stage2_query, stage2_results
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ ç®¡çº¿2é˜¶æ®µ2å¤±è´¥: {e}")
            return "", f"ç¬¬äºŒé˜¶æ®µæ£€ç´¢å¤±è´¥: {str(e)}"

    def _pipeline2_stage3_retrieval_stream(self, optimized_query: str, stage1_query: str, stage1_results: str, 
                                   stage2_query: str, stage2_results: str, stream_callback=None) -> tuple:
        """ç®¡çº¿2é˜¶æ®µ3ï¼šç»¼åˆæ€§æ£€ç´¢ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        try:
            # ç”Ÿæˆç¬¬ä¸‰é˜¶æ®µæ£€ç´¢æŸ¥è¯¢
            prompt = STAGE3_COMPREHENSIVE_PROMPT.format(
                original_query=optimized_query,
                stage1_query=stage1_query,
                stage1_results=stage1_results,
                stage2_query=stage2_query,
                stage2_results=stage2_results
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            response = self._call_openai_api(messages_for_api, stream=False)
            
            if "error" in response:
                return f"ç¬¬ä¸‰é˜¶æ®µæŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {response['error']}", ""
                
            stage3_query = response.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            # æ‰§è¡ŒLightRAGæ£€ç´¢ï¼ˆä½¿ç”¨æµå¼ç‰ˆæœ¬è·å–ç»“æœï¼‰
            lightrag_result = self._query_lightrag_stream(stage3_query, stage="stage3_retrieval", stream_callback=stream_callback)
            
            if "error" in lightrag_result:
                return stage3_query, f"ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢å¤±è´¥: {lightrag_result['error']}"
                
            stage3_results = lightrag_result.get("response", "æœªè·å–åˆ°æ£€ç´¢ç»“æœ")
            
            return stage3_query, stage3_results
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ ç®¡çº¿2é˜¶æ®µ3å¤±è´¥: {e}")
            return "", f"ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢å¤±è´¥: {str(e)}"

    def _pipeline2_final_synthesis_stream(self, optimized_query: str, stage1_query: str, stage1_results: str,
                                  stage2_query: str, stage2_results: str, stage3_query: str, stage3_results: str) -> str:
        """ç®¡çº¿2æœ€ç»ˆé˜¶æ®µï¼šç»¼åˆä¸‰æ¬¡æ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        try:
            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            prompt = FINAL_ANSWER_GENERATION_PROMPT.format(
                original_query=optimized_query,
                stage1_query=stage1_query,
                stage1_results=stage1_results,
                stage2_query=stage2_query,
                stage2_results=stage2_results,
                stage3_query=stage3_query,
                stage3_results=stage3_results
            )
            
            messages_for_api = [{"role": "user", "content": prompt}]
            response = self._call_openai_api(messages_for_api, stream=False)
            
            if "error" in response:
                return f"æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {response['error']}"
                
            final_answer = response.get("choices", [{}])[0].get("message", {}).get("content", "æœªè·å–åˆ°æœ€ç»ˆç­”æ¡ˆ")
            
            return final_answer
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ ç®¡çº¿2æœ€ç»ˆç»¼åˆå¤±è´¥: {e}")
            return f"æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"

    # ===========================================
    # ä¸»ç®¡çº¿æ‰§è¡Œæ–¹æ³•
    # ===========================================

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict,
        __event_emitter__=None,
        __event_call__=None,
        __user__=None
    ) -> Union[str, Generator, Iterator]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æ–¹æ³• - åŒç®¡çº¿CoTç³»ç»Ÿï¼ˆæµå¼ç‰ˆæœ¬ï¼‰

        ç®¡çº¿1ï¼šç›´æ¥ç”ŸæˆåŸå§‹å›ç­”ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        ç®¡çº¿2ï¼š3é˜¶æ®µé€’è¿›æ£€ç´¢ï¼Œæœ€åç»¼åˆç”Ÿæˆç­”æ¡ˆ

        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            model_id: æ¨¡å‹ID
            messages: æ¶ˆæ¯å†å²
            body: è¯·æ±‚ä½“ï¼ŒåŒ…å«æµå¼è®¾ç½®å’Œç”¨æˆ·ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²æˆ–æµå¼ç”Ÿæˆå™¨
        """
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.valves.OPENAI_API_KEY:
            return "âŒ é”™è¯¯ï¼šç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®OPENAI_API_KEY"

        if not user_message.strip():
            return "âŒ è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹"

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if self.valves.DEBUG_MODE and "user" in body:
            print("=" * 50)
            print(f"ç”¨æˆ·: {body['user']['name']} ({body['user']['id']})")
            print(f"æŸ¥è¯¢å†…å®¹: {user_message}")
            print(f"æµå¼å“åº”: {body.get('stream', False)}")
            print(f"å¯ç”¨æµå¼: {self.valves.ENABLE_STREAMING}")
            print("=" * 50)

        # æ€»æ˜¯è¿”å›æµå¼å“åº”
        return self._stream_response(user_message, messages)

    def _stream_response(self, query: str, messages: List[dict]) -> Generator[str, None, None]:
        """æµå¼å“åº”å¤„ç† - åŒç®¡çº¿å¹¶è¡Œæ‰§è¡Œ"""
        try:
            # æµå¼å¼€å§‹æ¶ˆæ¯
            yield f'data: {json.dumps({"choices": [{"delta": {}, "finish_reason": None}]})}\n\n'

            # é—®é¢˜ä¼˜åŒ–é˜¶æ®µ
            for chunk in self._emit_processing("æ­£åœ¨åˆ†æç”¨æˆ·æ„å›¾å¹¶ä¼˜åŒ–é—®é¢˜...\n", "question_optimization"):
                yield f"data: {json.dumps(chunk)}\n\n"

            optimized_query = self._question_optimization(messages, query)

            for chunk in self._emit_processing(f"âœ… é—®é¢˜ä¼˜åŒ–å®Œæˆ\n\n**åŸå§‹é—®é¢˜:**\n{query}\n\n**ä¼˜åŒ–åçš„é—®é¢˜:**\n{optimized_query}", "question_optimization"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # æµå¼å¤„ç†ç®¡çº¿1çš„ç›´æ¥å›ç­”
            for chunk in self._pipeline1_direct_answer_stream(optimized_query, messages):
                yield chunk

            for chunk in self._emit_processing("\nâœ… ç›´æ¥å›ç­”ç”Ÿæˆå®Œæˆ", "direct_answer"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # ç®¡çº¿2ï¼šå¤šé˜¶æ®µæ£€ç´¢
            # ç¬¬ä¸€é˜¶æ®µ
            for chunk in self._emit_processing("æ­£åœ¨è¿›è¡Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢...\n", "stage1_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # åˆ›å»ºstream_callbackæ¥æ”¶é›†å®æ—¶è¾“å‡º
            stream_buffer = []
            def stream_callback(data):
                stream_buffer.append(data)

            stage1_query, stage1_results = self._pipeline2_stage1_retrieval_stream(optimized_query, messages, stream_callback)
            
            # è¾“å‡ºæ”¶é›†åˆ°çš„æµå¼æ•°æ®
            for buffered_data in stream_buffer:
                yield buffered_data
            stream_buffer.clear()

            for chunk in self._emit_processing(f"âœ… ç¬¬ä¸€é˜¶æ®µæ£€ç´¢å®Œæˆ\n\n**æ£€ç´¢æŸ¥è¯¢:**\n{stage1_query}\n\n**æ£€ç´¢ç»“æœé•¿åº¦:** {len(stage1_results)} å­—ç¬¦", "stage1_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # ç¬¬äºŒé˜¶æ®µ
            for chunk in self._emit_processing("æ­£åœ¨è¿›è¡Œç¬¬äºŒé˜¶æ®µæ·±åº¦æ£€ç´¢...\n", "stage2_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            stage2_query, stage2_results = self._pipeline2_stage2_retrieval_stream(optimized_query, stage1_query, stage1_results, stream_callback)
            
            # è¾“å‡ºæ”¶é›†åˆ°çš„æµå¼æ•°æ®
            for buffered_data in stream_buffer:
                yield buffered_data
            stream_buffer.clear()

            for chunk in self._emit_processing(f"âœ… ç¬¬äºŒé˜¶æ®µæ£€ç´¢å®Œæˆ\n\n**æ£€ç´¢æŸ¥è¯¢:**\n{stage2_query}\n\n**æ£€ç´¢ç»“æœé•¿åº¦:** {len(stage2_results)} å­—ç¬¦", "stage2_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # ç¬¬ä¸‰é˜¶æ®µ
            for chunk in self._emit_processing("æ­£åœ¨è¿›è¡Œç¬¬ä¸‰é˜¶æ®µç»¼åˆæ£€ç´¢...\n", "stage3_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            stage3_query, stage3_results = self._pipeline2_stage3_retrieval_stream(
                optimized_query, stage1_query, stage1_results, stage2_query, stage2_results, stream_callback
            )
            
            # è¾“å‡ºæ”¶é›†åˆ°çš„æµå¼æ•°æ®
            for buffered_data in stream_buffer:
                yield buffered_data
            stream_buffer.clear()

            for chunk in self._emit_processing(f"âœ… ç¬¬ä¸‰é˜¶æ®µæ£€ç´¢å®Œæˆ\n\n**æ£€ç´¢æŸ¥è¯¢:**\n{stage3_query}\n\n**æ£€ç´¢ç»“æœé•¿åº¦:** {len(stage3_results)} å­—ç¬¦", "stage3_retrieval"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # æœ€ç»ˆç»¼åˆ
            for chunk in self._emit_processing("æ­£åœ¨ç»¼åˆä¸‰ä¸ªé˜¶æ®µçš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...", "final_synthesis"):
                yield f"data: {json.dumps(chunk)}\n\n"

            final_answer = self._pipeline2_final_synthesis_stream(
                optimized_query, stage1_query, stage1_results, stage2_query, stage2_results, stage3_query, stage3_results
            )

            for chunk in self._emit_processing(f"âœ… æœ€ç»ˆç­”æ¡ˆç»¼åˆå®Œæˆ\n\n**æœ€ç»ˆç­”æ¡ˆ:**\n{final_answer}", "final_synthesis"):
                yield f"data: {json.dumps(chunk)}\n\n"

            # æ˜¾ç¤ºç®¡çº¿2æœ€ç»ˆç»“æœï¼ˆéæŠ˜å æ˜¾ç¤ºï¼‰
            pipeline2_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\n**ğŸ§  ç®¡çº¿2 - å¤šé˜¶æ®µæ£€ç´¢æœ€ç»ˆç­”æ¡ˆ**\n{final_answer}\n"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(pipeline2_msg)}\n\n"

            # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
            token_stats = self._get_token_stats()
            token_info_msg = {
                'choices': [{
                    'delta': {
                        'content': f"\n\n---\n**Tokenæ¶ˆè€—ç»Ÿè®¡**\n- è¾“å…¥Token: {token_stats['input_tokens']:,}\n- è¾“å‡ºToken: {token_stats['output_tokens']:,}\n- æ€»Token: {token_stats['total_tokens']:,}"
                    },
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(token_info_msg)}\n\n"

        except Exception as e:
            error_msg = f"âŒ CoT Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"Stream error: {e}")
            yield f'data: {json.dumps({"choices": [{"delta": {"content": error_msg}}]})}\n\n'

        yield "data: [DONE]\n\n"
