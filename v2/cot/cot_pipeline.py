"""
title: Chain of Thought Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: é«˜çº§æ€ç»´é“¾æ¨ç†Pipeline - æ”¯æŒå¤šç§CoTæŠ€æœ¯ã€æµå¼è¾“å‡ºå’Œæ™ºèƒ½æ¨ç†
requirements: requests, pydantic
"""

import os
import json
import requests
import time
from typing import List, Union, Generator, Iterator, Literal
from pydantic import BaseModel, Field
from enum import Enum


class CoTStrategy(Enum):
    """æ€ç»´é“¾ç­–ç•¥æšä¸¾"""
    BASIC_COT = "basic_cot"              # åŸºç¡€æ€ç»´é“¾
    TREE_OF_THOUGHTS = "tree_of_thoughts" # æ ‘çŠ¶æ€ç»´
    SELF_REFLECTION = "self_reflection"   # è‡ªæˆ‘åæ€
    REVERSE_CHAIN = "reverse_chain"       # åå‘æ€ç»´é“¾
    MULTI_PERSPECTIVE = "multi_perspective" # å¤šè§†è§’åˆ†æ
    STEP_BACK = "step_back"              # æ­¥é€€åˆ†æ
    ANALOGICAL = "analogical"            # ç±»æ¯”æ¨ç†
    PLAN = "plan"                        # è®¡åˆ’æ¨ç†æ€ç»´


# ================================
# æç¤ºè¯æ¨¡æ¿å‡½æ•° 
# ================================

def format_basic_cot_prompt(query: str, context_text: str) -> str:
    """åŸºç¡€æ€ç»´é“¾æ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œé€æ­¥æ·±å…¥çš„åˆ†ææ€è€ƒã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œé€æ­¥æ·±å…¥çš„åˆ†ææ€è€ƒã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„é€æ­¥æ¨ç†æ€ç»´è¿‡ç¨‹ï¼š

é¦–å…ˆï¼Œè®©æˆ‘ä»æ›´æ·±å±‚çš„è§’åº¦ç†è§£è¿™ä¸ªé—®é¢˜çš„èƒŒæ™¯å’Œç”¨æˆ·çš„çœŸå®éœ€æ±‚ã€‚åŸºäºå¯¹è¯å†å²å’Œé—®é¢˜æœ¬èº«ï¼Œæˆ‘éœ€è¦æ€è€ƒç”¨æˆ·ä¸ºä»€ä¹ˆä¼šæå‡ºè¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬å¯èƒ½é‡åˆ°äº†ä»€ä¹ˆå…·ä½“æƒ…å¢ƒæˆ–æŒ‘æˆ˜ï¼Œä»¥åŠä»–ä»¬çš„ä¸“ä¸šèƒŒæ™¯å’ŒçŸ¥è¯†å±‚æ¬¡å¦‚ä½•ã€‚

ç„¶åï¼Œè®©æ€ç»´æ²¿ç€é€»è¾‘çš„è„‰ç»œè‡ªç„¶æ¨è¿›ï¼Œä»é—®é¢˜çš„æ ¸å¿ƒå¼€å§‹ï¼Œæ¯ä¸ªæƒ³æ³•éƒ½è‡ªç„¶åœ°å¼•å‘ä¸‹ä¸€ä¸ªæƒ³æ³•ï¼Œå½¢æˆè¿è´¯çš„æ¨ç†é“¾æ¡ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ å¯¹ç”¨æˆ·æé—®èƒŒæ™¯å’ŒåŠ¨æœºçš„æ·±å…¥åˆ†æ
â€¢ åŸºäºç”¨æˆ·è§’è‰²å’Œä¸“ä¸šæ°´å¹³çš„ä¸ªæ€§åŒ–æ€è€ƒ
â€¢ å¯¹é—®é¢˜æœ¬è´¨çš„ç†è§£å’Œåˆ†æ
â€¢ ä»å·²çŸ¥ä¿¡æ¯å‡ºå‘çš„é€»è¾‘æ¨å¯¼
â€¢ æ¯ä¸€æ­¥æ¨ç†çš„ä¾æ®å’Œåˆç†æ€§
â€¢ ä»ç®€å•åˆ°å¤æ‚çš„é€’è¿›å¼æ€è€ƒ
â€¢ å¯¹ç»“è®ºçš„é€»è¾‘éªŒè¯

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ï¼Œè®©æ€ç»´è¿‡ç¨‹è¿è´¯è‡ªç„¶ï¼Œå¦‚åŒå†…å¿ƒçš„é€»è¾‘æ¨ç†æµæ·Œã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„é€æ­¥æ¨ç†ï¼š"""


def format_tree_thoughts_prompt(query: str, context_text: str) -> str:
    """æ ‘çŠ¶æ€ç»´æ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ ‘çŠ¶æ€ç»´åˆ†æï¼Œæ¢ç´¢å¤šä¸ªå¯èƒ½çš„æ€è€ƒæ–¹å‘ã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ ‘çŠ¶æ€ç»´åˆ†æï¼Œæ¢ç´¢å¤šä¸ªå¯èƒ½çš„æ€è€ƒæ–¹å‘ã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„æ ‘çŠ¶æ€ç»´æ¢ç´¢è¿‡ç¨‹ï¼š

é¦–å…ˆï¼Œæˆ‘éœ€è¦æ·±å…¥ç†è§£ç”¨æˆ·æå‡ºè¿™ä¸ªé—®é¢˜çš„æ·±å±‚åŸå› ã€‚é€šè¿‡åˆ†æå¯¹è¯å†å²å’Œé—®é¢˜è¡¨è¿°ï¼Œæˆ‘è¦æ€è€ƒç”¨æˆ·å¯èƒ½çš„èº«ä»½èƒŒæ™¯ã€ä¸“ä¸šé¢†åŸŸã€çŸ¥è¯†æ°´å¹³ï¼Œä»¥åŠä»–ä»¬åœ¨ä»€ä¹ˆæƒ…å¢ƒä¸‹äº§ç”Ÿäº†è¿™ä¸ªç–‘é—®ã€‚è¿™å°†å¸®åŠ©æˆ‘ç¡®å®šåº”è¯¥ä»å“ªäº›è§’åº¦å’Œæ·±åº¦æ¥æ¢ç´¢é—®é¢˜ã€‚

é¢å¯¹é—®é¢˜æ—¶è®©æ€ç»´è‡ªç„¶åœ°æ¢ç´¢ä¸åŒçš„å¯èƒ½æ–¹å‘ï¼Œåœ¨å„ç§æ€è·¯é—´æ¸¸èµ°æ¯”è¾ƒï¼Œæ·±å…¥æ¢ç´¢æœ€æœ‰ä»·å€¼çš„æ–¹å‘ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ å¯¹ç”¨æˆ·æé—®åŠ¨æœºå’ŒèƒŒæ™¯æƒ…å¢ƒçš„å¤šç»´åº¦åˆ†æ
â€¢ åŸºäºç”¨æˆ·è§’è‰²ç‰¹å¾çš„å·®å¼‚åŒ–æ€è€ƒè·¯å¾„
â€¢ è¯†åˆ«é—®é¢˜çš„å¤šä¸ªå¯èƒ½ç»´åº¦
â€¢ æ¢ç´¢ä¸åŒçš„è§£å†³è·¯å¾„å’Œè§’åº¦  
â€¢ æ¯”è¾ƒå„ç§æ€è·¯çš„ä¼˜ç¼ºç‚¹
â€¢ æ·±å…¥æŒ–æ˜æœ€æœ‰å¸Œæœ›çš„æ–¹å‘
â€¢ åœ¨ä¸åŒåˆ†æ”¯é—´å»ºç«‹è”ç³»

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´åœ¨ä¸åŒå¯èƒ½æ€§ä¹‹é—´è‡ªç„¶æµè½¬ï¼Œä½“ç°æ¢ç´¢å¼çš„æ€è€ƒè¿‡ç¨‹ã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„æ ‘çŠ¶æ€ç»´æ¢ç´¢ï¼š"""


def format_self_reflection_prompt(query: str, context_text: str) -> str:
    """è‡ªæˆ‘åæ€æ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±å…¥çš„è‡ªæˆ‘åæ€å¼åˆ†æã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±å…¥çš„è‡ªæˆ‘åæ€å¼åˆ†æã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„è‡ªæˆ‘åæ€æ€ç»´è¿‡ç¨‹ï¼š

è®©æˆ‘é¦–å…ˆåæ€ç”¨æˆ·ä¸ºä»€ä¹ˆä¼šåœ¨æ­¤æ—¶æå‡ºè¿™æ ·çš„é—®é¢˜ã€‚ç»“åˆå¯¹è¯å†å²ï¼Œæˆ‘éœ€è¦æ€è€ƒç”¨æˆ·å¯èƒ½çš„å›°æƒ‘ç‚¹åœ¨å“ªé‡Œï¼Œä»–ä»¬çš„ä¸“ä¸šèƒŒæ™¯å¦‚ä½•ï¼Œä»¥åŠä»–ä»¬æœŸæœ›è·å¾—ä»€ä¹ˆå±‚æ¬¡çš„ç­”æ¡ˆã€‚è¿™ç§å¯¹ç”¨æˆ·åŠ¨æœºå’Œèº«ä»½çš„æ·±å…¥ç†è§£ï¼Œå°†æŒ‡å¯¼æˆ‘å¦‚ä½•è¿›è¡Œæ›´æœ‰é’ˆå¯¹æ€§çš„åˆ†æã€‚

åœ¨æ€è€ƒè¿‡ç¨‹ä¸­è‡ªç„¶åœ°å¯¹è‡ªå·±çš„æƒ³æ³•è¿›è¡Œå®¡è§†ï¼Œå¯¹å¯èƒ½çš„ç–æ¼æˆ–é”™è¯¯ä¿æŒæ•æ„Ÿï¼Œè®©æ€ç»´åœ¨è‡ªæˆ‘è´¨ç–‘å’ŒéªŒè¯ä¸­ä¸æ–­å®Œå–„ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ å¯¹ç”¨æˆ·æé—®æ„å›¾å’Œä¸“ä¸šèƒŒæ™¯çš„åæ€æ€§åˆ†æ
â€¢ åŸºäºç”¨æˆ·ç‰¹å¾è°ƒæ•´æ€è€ƒæ·±åº¦å’Œè§’åº¦
â€¢ å¯¹åˆå§‹æƒ³æ³•çš„æ‰¹åˆ¤æ€§å®¡è§†
â€¢ è¯†åˆ«å¯èƒ½çš„è®¤çŸ¥åè¯¯å’Œç›²ç‚¹
â€¢ ä»ä¸åŒè§’åº¦éªŒè¯æ¨ç†çš„åˆç†æ€§
â€¢ è´¨ç–‘å‡è®¾å’Œå‰æçš„æ­£ç¡®æ€§
â€¢ åœ¨åæ€ä¸­æ·±åŒ–å’Œä¿®æ­£ç†è§£

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´åœ¨è´¨ç–‘ã€éªŒè¯ã€ä¿®æ­£ä¸­è‡ªç„¶å±•å¼€ï¼Œä½“ç°åæ€æ€§çš„æ·±åº¦æ€è€ƒã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„è‡ªæˆ‘åæ€ï¼š"""


def format_reverse_chain_prompt(query: str, context_text: str) -> str:
    """åå‘æ€ç»´é“¾æ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œåå‘æ¨ç†åˆ†æã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œåå‘æ¨ç†åˆ†æã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„åå‘æ¨ç†æ€ç»´è¿‡ç¨‹ï¼š

åœ¨å¼€å§‹åå‘æ¨ç†ä¹‹å‰ï¼Œè®©æˆ‘å…ˆæ·±å…¥åˆ†æç”¨æˆ·æå‡ºè¿™ä¸ªé—®é¢˜çš„çœŸå®éœ€æ±‚ã€‚é€šè¿‡è§‚å¯Ÿå¯¹è¯å†å²ä¸­çš„ç»†èŠ‚ï¼Œæˆ‘éœ€è¦åˆ¤æ–­ç”¨æˆ·çš„ä¸“ä¸šæ°´å¹³ã€é¢†åŸŸèƒŒæ™¯ï¼Œä»¥åŠä»–ä»¬å¸Œæœ›é€šè¿‡è¿™ä¸ªé—®é¢˜è§£å†³ä»€ä¹ˆå®é™…å›°éš¾ã€‚è¿™ç§å¯¹ç”¨æˆ·åŠ¨æœºçš„ç†è§£å°†å¸®åŠ©æˆ‘ç¡®å®šåˆé€‚çš„ç›®æ ‡è®¾å®šå’Œæ¨ç†æ·±åº¦ã€‚

ä»æƒ³è¦è¾¾åˆ°çš„ç›®æ ‡æˆ–ç†æƒ³ç»“æœå¼€å§‹æ€è€ƒï¼Œè‡ªç„¶åœ°è¿½æº¯å®ç°è¿™ä¸ªç›®æ ‡éœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Œå±‚å±‚é€’æ¨åœ°æ„å»ºèµ·é€šå‘ç›®æ ‡çš„æ€ç»´è·¯å¾„ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ åŸºäºç”¨æˆ·èƒŒæ™¯å’Œéœ€æ±‚çš„ç›®æ ‡é‡æ–°å®šä¹‰
â€¢ è€ƒè™‘ç”¨æˆ·ä¸“ä¸šæ°´å¹³çš„ä¸ªæ€§åŒ–è·¯å¾„è®¾è®¡
â€¢ æ˜ç¡®ç†æƒ³çš„ç»“æœæˆ–è§£å†³æ–¹æ¡ˆ
â€¢ å€’æ¨å®ç°ç›®æ ‡æ‰€éœ€çš„ç›´æ¥æ¡ä»¶
â€¢ é€å±‚åˆ†ææ¯ä¸ªæ¡ä»¶çš„å‰ç½®è¦æ±‚
â€¢ è¯†åˆ«å…³é”®çš„èŠ‚ç‚¹å’Œä¾èµ–å…³ç³»
â€¢ æ„å»ºä»ç°çŠ¶åˆ°ç›®æ ‡çš„å®Œæ•´è·¯å¾„

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´ä»ç›®æ ‡å‡ºå‘å‘èµ·ç‚¹è‡ªç„¶å›æº¯ï¼Œä½“ç°é€†å‘å·¥ç¨‹çš„æ€è€ƒè¿‡ç¨‹ã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„åå‘æ¨ç†ï¼š"""


def format_multi_perspective_prompt(query: str, context_text: str) -> str:
    """å¤šè§†è§’åˆ†ææ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œå¤šè§†è§’æ·±åº¦åˆ†æã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œå¤šè§†è§’æ·±åº¦åˆ†æã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„å¤šè§†è§’åˆ†ææ€ç»´è¿‡ç¨‹ï¼š

åœ¨è¿›è¡Œå¤šè§†è§’åˆ†æå‰ï¼Œæˆ‘é¦–å…ˆè¦ç†è§£ç”¨æˆ·æé—®çš„æ·±å±‚èƒŒæ™¯ã€‚é€šè¿‡åˆ†æå¯¹è¯å†å²ï¼Œæˆ‘éœ€è¦æ¨æ–­ç”¨æˆ·å¯èƒ½çš„èŒä¸šè§’è‰²ã€ä¸“ä¸šé¢†åŸŸã€ç»éªŒæ°´å¹³ï¼Œä»¥åŠä»–ä»¬åœ¨ä»€ä¹ˆå…·ä½“æƒ…å¢ƒä¸­é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ã€‚è¿™ç§å¯¹ç”¨æˆ·èº«ä»½çš„æ´å¯Ÿå°†æŒ‡å¯¼æˆ‘é€‰æ‹©æœ€ç›¸å…³çš„è§†è§’è¿›è¡Œåˆ†æã€‚

è®©æ€ç»´è‡ªç„¶åœ°ä»ä¸åŒçš„è§’åº¦å»å®¡è§†é—®é¢˜ï¼Œè®¾èº«å¤„åœ°è€ƒè™‘å„æ–¹çš„ç«‹åœºå’Œè§‚ç‚¹ï¼Œåœ¨å¤šé‡è§†è§’çš„äº¤ç»‡ä¸­å½¢æˆæ›´å…¨é¢çš„ç†è§£ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ åŸºäºç”¨æˆ·è§’è‰²ç‰¹å¾é€‰æ‹©ç›¸å…³åˆ†æè§†è§’
â€¢ ç»“åˆç”¨æˆ·ä¸“ä¸šèƒŒæ™¯è°ƒæ•´åˆ†ææ·±åº¦
â€¢ ä»ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„è§’åº¦æ€è€ƒ
â€¢ è€ƒè™‘çŸ­æœŸå’Œé•¿æœŸçš„ä¸åŒå½±å“
â€¢ åˆ†æç†è®ºä¸å®é™…åº”ç”¨çš„å·®å¼‚
â€¢ æƒè¡¡å„ç§å› ç´ å’Œåˆ¶çº¦æ¡ä»¶
â€¢ åœ¨ä¸åŒè§‚ç‚¹é—´å¯»æ‰¾å¹³è¡¡å’Œå…±è¯†

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´åœ¨ä¸åŒè§†è§’é—´è‡ªç„¶åˆ‡æ¢ï¼Œä½“ç°å…¨é¢æ€§å’ŒåŒ…å®¹æ€§çš„åˆ†æè¿‡ç¨‹ã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„å¤šè§†è§’åˆ†æï¼š"""


def format_step_back_prompt(query: str, context_text: str) -> str:
    """æ­¥é€€åˆ†ææ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ­¥é€€å¼æ·±å…¥åˆ†æã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ­¥é€€å¼æ·±å…¥åˆ†æã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„æ­¥é€€åˆ†ææ€ç»´è¿‡ç¨‹ï¼š

åœ¨è¿›è¡Œæ­¥é€€åˆ†æä¹‹å‰ï¼Œè®©æˆ‘ä»ç”¨æˆ·çš„è§’åº¦æ€è€ƒè¿™ä¸ªé—®é¢˜çš„æ¥æºã€‚é€šè¿‡ä»”ç»†è§‚å¯Ÿå¯¹è¯å†å²å’Œé—®é¢˜è¡¨è¿°ï¼Œæˆ‘éœ€è¦åˆ†æç”¨æˆ·å¯èƒ½çš„ä¸“ä¸šèƒŒæ™¯ã€çŸ¥è¯†ç»“æ„ï¼Œä»¥åŠä»–ä»¬ä¸ºä»€ä¹ˆä¼šåœ¨è¿™ä¸ªæ—¶å€™æå‡ºè¿™æ ·çš„é—®é¢˜ã€‚è¿™ç§å¯¹ç”¨æˆ·åŠ¨æœºå’Œèº«ä»½çš„ç†è§£å°†å¸®åŠ©æˆ‘ç¡®å®šåº”è¯¥é€€åˆ°ä»€ä¹ˆå±‚æ¬¡è¿›è¡Œå®è§‚åˆ†æã€‚

å…ˆè®©æ€ç»´è·³å‡ºå…·ä½“é—®é¢˜çš„ç»†èŠ‚ï¼Œä»æ›´å®è§‚çš„å±‚é¢å»ç†è§£é—®é¢˜çš„æœ¬è´¨å’ŒèƒŒæ™¯ï¼Œç„¶åå°†è¿™ç§å®è§‚ç†è§£è‡ªç„¶åœ°åº”ç”¨åˆ°å…·ä½“é—®é¢˜çš„åˆ†æä¸­ã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ åŸºäºç”¨æˆ·ç‰¹å¾ç¡®å®šåˆé€‚çš„æŠ½è±¡å±‚æ¬¡
â€¢ ç»“åˆç”¨æˆ·ä¸“ä¸šæ°´å¹³è°ƒæ•´åˆ†ææ·±åº¦
â€¢ å°†å…·ä½“é—®é¢˜ç½®äºæ›´å¤§çš„èƒŒæ™¯ä¸­ç†è§£
â€¢ è¯†åˆ«é—®é¢˜èƒŒåçš„æ ¹æœ¬åŸç†å’Œè§„å¾‹
â€¢ ä»æŠ½è±¡å±‚é¢æŠŠæ¡é—®é¢˜çš„æœ¬è´¨ç‰¹å¾
â€¢ å°†å®è§‚æ´å¯Ÿåº”ç”¨äºå…·ä½“æƒ…å†µ
â€¢ åœ¨å®è§‚å’Œå¾®è§‚é—´å»ºç«‹æœ‰æœºè”ç³»

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´ä»å®è§‚åˆ°å¾®è§‚è‡ªç„¶è¿‡æ¸¡ï¼Œä½“ç°ç”±æŠ½è±¡åˆ°å…·ä½“çš„åˆ†ææ·±åŒ–è¿‡ç¨‹ã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„æ­¥é€€åˆ†æï¼š"""


def format_analogical_prompt(query: str, context_text: str) -> str:
    """ç±»æ¯”æ¨ç†æ¨¡æ¿"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œç±»æ¯”æ¨ç†åˆ†æã€‚

å¾…åˆ†æé—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œç±»æ¯”æ¨ç†åˆ†æã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…åˆ†æé—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„ç±»æ¯”æ¨ç†æ€ç»´è¿‡ç¨‹ï¼š

åœ¨å¼€å§‹ç±»æ¯”æ¨ç†å‰ï¼Œæˆ‘éœ€è¦æ·±å…¥ç†è§£ç”¨æˆ·æå‡ºè¿™ä¸ªé—®é¢˜çš„èƒŒæ™¯åŠ¨æœºã€‚é€šè¿‡åˆ†æå¯¹è¯å†å²ï¼Œæˆ‘è¦åˆ¤æ–­ç”¨æˆ·çš„ä¸“ä¸šé¢†åŸŸã€çŸ¥è¯†åŸºç¡€ï¼Œä»¥åŠä»–ä»¬æœ€ç†Ÿæ‚‰çš„é¢†åŸŸæˆ–æ¦‚å¿µã€‚è¿™æ ·æˆ‘å°±èƒ½é€‰æ‹©ç”¨æˆ·å®¹æ˜“ç†è§£çš„ç±»æ¯”å¯¹è±¡ï¼Œè®©æ¨ç†è¿‡ç¨‹æ›´åŠ è´´è¿‘ç”¨æˆ·çš„è®¤çŸ¥èƒŒæ™¯ã€‚

è®©æ€ç»´è‡ªç„¶åœ°è”æƒ³åˆ°ç›¸ä¼¼çš„æƒ…å†µã€æ¡ˆä¾‹æˆ–ç°è±¡ï¼Œé€šè¿‡è¿™äº›ç›¸ä¼¼æ€§æ¥å¯å‘å¯¹å½“å‰é—®é¢˜çš„ç†è§£ï¼Œåœ¨ç±»æ¯”ä¸­å¯»æ‰¾è§£å†³é—®é¢˜çš„æ´å¯Ÿå’Œçµæ„Ÿã€‚åœ¨æ€è€ƒä¸­ä½“ç°ï¼š

â€¢ åŸºäºç”¨æˆ·èƒŒæ™¯é€‰æ‹©åˆé€‚çš„ç±»æ¯”é¢†åŸŸ
â€¢ è€ƒè™‘ç”¨æˆ·çŸ¥è¯†ç»“æ„çš„ä¸ªæ€§åŒ–ç±»æ¯”
â€¢ å¯»æ‰¾ä¸å½“å‰é—®é¢˜ç»“æ„ç›¸ä¼¼çš„å·²çŸ¥æ¡ˆä¾‹
â€¢ åˆ†æç±»æ¯”å¯¹è±¡çš„å…³é”®ç‰¹å¾å’Œæ¨¡å¼
â€¢ è¯†åˆ«ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§
â€¢ ä»æˆåŠŸæ¡ˆä¾‹ä¸­æå–å¯å€Ÿé‰´çš„æ™ºæ…§
â€¢ é€šè¿‡ç±»æ¯”è·å¾—æ–°çš„ç†è§£è§†è§’

è¯·ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€è¿›è¡Œæ€è€ƒï¼Œé¿å…ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰ç»“æ„åŒ–æ ¼å¼ã€‚è®©æ€ç»´åœ¨ä¸åŒé¢†åŸŸé—´è‡ªç„¶è·³è·ƒè”æƒ³ï¼Œä½“ç°åˆ›é€ æ€§çš„ç±»æ¯”æ€è€ƒè¿‡ç¨‹ã€‚ä¸è¦åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç»™å‡ºæ€»ç»“æˆ–ç»“è®ºã€‚

è¯·å¼€å§‹ä½ çš„ç±»æ¯”æ¨ç†ï¼š"""


def format_plan_prompt(query: str, context_text: str) -> str:
    """è®¡åˆ’æ¨ç†æ¨¡æ¿ - å…è®¸ç»“æ„åŒ–åˆ†ç‚¹"""
    base_content = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜åˆ¶å®šè¯¦ç»†çš„è§£å†³è®¡åˆ’ã€‚

å¾…è§£å†³é—®é¢˜: {query}"""

    if context_text.strip():
        base_content = f"""è¯·ç»“åˆå¯¹è¯å†å²ï¼Œå¯¹ä»¥ä¸‹é—®é¢˜åˆ¶å®šè¯¦ç»†çš„è§£å†³è®¡åˆ’ã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

å¾…è§£å†³é—®é¢˜: {query}"""

    return f"""{base_content}

è¯·å±•ç°ä½ çš„è®¡åˆ’æ¨ç†æ€ç»´è¿‡ç¨‹ï¼š

åœ¨åˆ¶å®šè®¡åˆ’ä¹‹å‰ï¼Œæˆ‘é¦–å…ˆè¦æ·±å…¥åˆ†æç”¨æˆ·æå‡ºè¿™ä¸ªé—®é¢˜çš„çœŸå®éœ€æ±‚å’ŒèƒŒæ™¯ã€‚é€šè¿‡å¯¹è¯å†å²ï¼Œæˆ‘éœ€è¦åˆ¤æ–­ç”¨æˆ·çš„ä¸“ä¸šæ°´å¹³ã€å¯ç”¨èµ„æºã€æ—¶é—´çº¦æŸï¼Œä»¥åŠä»–ä»¬åœ¨å®æ–½è®¡åˆ’æ—¶å¯èƒ½é¢ä¸´çš„å…·ä½“æŒ‘æˆ˜ã€‚åŸºäºè¿™ç§å¯¹ç”¨æˆ·æƒ…å†µçš„å…¨é¢äº†è§£ï¼Œæˆ‘å°†åˆ¶å®šçœŸæ­£é€‚åˆä»–ä»¬çš„ä¸ªæ€§åŒ–è§£å†³æ–¹æ¡ˆã€‚

è¿ç”¨ç³»ç»Ÿæ€§çš„è®¡åˆ’æ€ç»´ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å…·ä½“æ­¥éª¤ï¼Œåˆ¶å®šæ¸…æ™°çš„è¡ŒåŠ¨è·¯å¾„å’Œæ—¶é—´å®‰æ’ã€‚åœ¨åˆ¶å®šè®¡åˆ’æ—¶è¯·ä½¿ç”¨ç»“æ„åŒ–çš„æ€è€ƒæ–¹å¼ï¼š

**é˜¶æ®µä¸€ï¼šç”¨æˆ·éœ€æ±‚ä¸èƒŒæ™¯åˆ†æ**
- æ·±å…¥åˆ†æç”¨æˆ·çš„ä¸“ä¸šèƒŒæ™¯å’Œè§’è‰²ç‰¹å¾
- ç†è§£ç”¨æˆ·æé—®çš„çœŸå®åŠ¨æœºå’ŒæœŸæœ›
- è¯„ä¼°ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³å’Œæ‰§è¡Œèƒ½åŠ›
- è¯†åˆ«ç”¨æˆ·çš„å…·ä½“çº¦æŸå’Œèµ„æºé™åˆ¶

**é˜¶æ®µäºŒï¼šé—®é¢˜åˆ†æä¸ç›®æ ‡è®¾å®š**
- åŸºäºç”¨æˆ·ç‰¹å¾é‡æ–°å®šä¹‰é—®é¢˜æ ¸å¿ƒ
- è®¾å®šç¬¦åˆç”¨æˆ·å®é™…æƒ…å†µçš„å…·ä½“ç›®æ ‡
- ç¡®å®šé€‚åˆç”¨æˆ·æ°´å¹³çš„æˆåŠŸæ ‡å‡†

**é˜¶æ®µä¸‰ï¼šèµ„æºè¯„ä¼°ä¸çº¦æŸåˆ†æ** 
- è¯„ä¼°ç”¨æˆ·å¯ç”¨çš„èµ„æºå’Œå·¥å…·
- è¯†åˆ«ç”¨æˆ·å¯èƒ½é¢ä¸´çš„é™åˆ¶å’Œé£é™©å› ç´ 
- åˆ†æç”¨æˆ·çš„æ—¶é—´ã€æˆæœ¬ç­‰çº¦æŸæ¡ä»¶

**é˜¶æ®µå››ï¼šæ–¹æ¡ˆè®¾è®¡ä¸æ­¥éª¤è§„åˆ’**
- è®¾è®¡é€‚åˆç”¨æˆ·èƒŒæ™¯çš„è§£å†³æ–¹æ¡ˆ
- å°†æ–¹æ¡ˆåˆ†è§£ä¸ºç”¨æˆ·å¯æ‰§è¡Œçš„å…·ä½“æ­¥éª¤
- ç¡®å®šæ­¥éª¤é—´çš„ä¾èµ–å…³ç³»å’Œæ—¶é—´é¡ºåº

**é˜¶æ®µäº”ï¼šå®æ–½ç­–ç•¥ä¸ç›‘æ§æœºåˆ¶**
- åˆ¶å®šç¬¦åˆç”¨æˆ·ç‰¹ç‚¹çš„å…·ä½“å®æ–½ç­–ç•¥
- è®¾è®¡é€‚åˆç”¨æˆ·çš„è¿›åº¦ç›‘æ§å’Œè´¨é‡æ£€æŸ¥æœºåˆ¶  
- åŸºäºç”¨æˆ·ç»éªŒå‡†å¤‡åº”å¯¹å¼‚å¸¸æƒ…å†µçš„å¤‡é€‰æ–¹æ¡ˆ

è¯·æŒ‰ç…§ä»¥ä¸Šç»“æ„åŒ–æ€ç»´æ¡†æ¶è¿›è¡Œæ·±å…¥çš„è®¡åˆ’æ€è€ƒï¼Œå¯ä»¥ä½¿ç”¨åˆ†ç‚¹ã€ç¼–å·ç­‰æ¸…æ™°çš„æ ¼å¼æ¥ç»„ç»‡ä½ çš„æ€ç»´è¿‡ç¨‹ã€‚

è¯·å¼€å§‹ä½ çš„è®¡åˆ’æ¨ç†ï¼š"""


# ================================
# å†å²ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
# ================================

class HistoryContextManager:
    """å†å²ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¸“ä¸ºæ€ç»´é“¾ä¼˜åŒ–"""
    
    DEFAULT_HISTORY_TURNS = 3
    
    @classmethod
    def analyze_user_profile(cls, messages: List[dict]) -> dict:
        """åˆ†æç”¨æˆ·ç”»åƒå’Œè§’è‰²ç‰¹å¾"""
        if not messages or len(messages) < 2:
            return {
                "role_category": "general_user",
                "expertise_level": "beginner",
                "professional_domains": [],
                "interaction_patterns": [],
                "motivation_indicators": []
            }
        
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        all_user_text = " ".join([msg.get("content", "") for msg in user_messages])
        
        # åˆ†æä¸“ä¸šé¢†åŸŸå…³é”®è¯
        technical_domains = {
            "programming": ["ä»£ç ", "ç¼–ç¨‹", "å¼€å‘", "è½¯ä»¶", "ç®—æ³•", "å‡½æ•°", "bug", "debug", "API", "æ¡†æ¶"],
            "business": ["é¡¹ç›®", "ç®¡ç†", "æˆ˜ç•¥", "å¸‚åœº", "è¿è¥", "å›¢é˜Ÿ", "ä¸šåŠ¡", "å®¢æˆ·", "æ–¹æ¡ˆ"],
            "academic": ["ç ”ç©¶", "ç†è®º", "å­¦æœ¯", "è®ºæ–‡", "åˆ†æ", "æ–¹æ³•è®º", "å®éªŒ", "æ•°æ®"],
            "design": ["è®¾è®¡", "ç•Œé¢", "ç”¨æˆ·ä½“éªŒ", "UI", "UX", "è§†è§‰", "äº¤äº’", "åŸå‹"],
            "finance": ["è´¢åŠ¡", "æŠ•èµ„", "é‡‘è", "æˆæœ¬", "æ”¶ç›Š", "é¢„ç®—", "é£é™©", "èµ„é‡‘"],
            "healthcare": ["åŒ»ç–—", "å¥åº·", "æ²»ç–—", "è¯Šæ–­", "ç—…æ‚£", "ä¸´åºŠ", "è¯ç‰©"],
            "education": ["æ•™å­¦", "å­¦ä¹ ", "è¯¾ç¨‹", "å­¦ç”Ÿ", "æ•™è‚²", "åŸ¹è®­", "çŸ¥è¯†ä¼ æˆ"]
        }
        
        detected_domains = []
        for domain, keywords in technical_domains.items():
            if any(keyword in all_user_text for keyword in keywords):
                detected_domains.append(domain)
        
        # åˆ†æä¸“ä¸šæ°´å¹³
        expertise_indicators = {
            "expert": ["æ·±å…¥", "æ¶æ„", "åº•å±‚", "åŸç†", "æœ€ä½³å®è·µ", "ä¼˜åŒ–", "é«˜çº§"],
            "intermediate": ["å¦‚ä½•å®ç°", "å…·ä½“æ–¹æ³•", "æ­¥éª¤", "æµç¨‹", "ç»éªŒ", "é—®é¢˜è§£å†³"],
            "beginner": ["ä»€ä¹ˆæ˜¯", "æ€ä¹ˆ", "åŸºç¡€", "å…¥é—¨", "ç®€å•", "åˆå­¦", "ä¸å¤ªæ‡‚"]
        }
        
        expertise_level = "intermediate"  # é»˜è®¤ä¸­çº§
        for level, indicators in expertise_indicators.items():
            if any(indicator in all_user_text for indicator in indicators):
                expertise_level = level
                break
        
        # åˆ†æäº¤äº’æ¨¡å¼
        interaction_patterns = []
        if len(user_messages) > 3:
            interaction_patterns.append("engaged_conversation")
        if any("å…·ä½“" in msg.get("content", "") for msg in user_messages):
            interaction_patterns.append("detail_seeking")
        if any("ä¸ºä»€ä¹ˆ" in msg.get("content", "") for msg in user_messages):
            interaction_patterns.append("reason_seeking")
        if any("å¦‚ä½•" in msg.get("content", "") for msg in user_messages):
            interaction_patterns.append("solution_seeking")
        
        # åˆ†æåŠ¨æœºæŒ‡æ ‡
        motivation_indicators = []
        recent_user_text = " ".join([msg.get("content", "") for msg in user_messages[-3:]])
        
        if any(word in recent_user_text for word in ["æ€¥", "é©¬ä¸Š", "å¿«é€Ÿ", "ç«‹å³"]):
            motivation_indicators.append("urgent_need")
        if any(word in recent_user_text for word in ["å­¦ä¹ ", "äº†è§£", "çŸ¥è¯†"]):
            motivation_indicators.append("learning_oriented")
        if any(word in recent_user_text for word in ["è§£å†³", "é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜"]):
            motivation_indicators.append("problem_solving")
        if any(word in recent_user_text for word in ["æ¯”è¾ƒ", "é€‰æ‹©", "å†³ç­–"]):
            motivation_indicators.append("decision_making")
        if any(word in recent_user_text for word in ["æ”¹è¿›", "ä¼˜åŒ–", "æå‡"]):
            motivation_indicators.append("improvement_focused")
        
        return {
            "role_category": detected_domains[0] if detected_domains else "general_user",
            "expertise_level": expertise_level,
            "professional_domains": detected_domains,
            "interaction_patterns": interaction_patterns,
            "motivation_indicators": motivation_indicators
        }
    
    @classmethod
    def extract_recent_context(cls, messages: List[dict], max_turns: int = None) -> str:
        """æå–æœ€è¿‘çš„å†å²ä¼šè¯ä¸Šä¸‹æ–‡"""
        if not messages or len(messages) < 2:
            return ""
            
        max_turns = max_turns or cls.DEFAULT_HISTORY_TURNS
        max_messages = max_turns * 2
        
        recent_messages = messages[-max_messages:] if len(messages) > max_messages else messages
        context_text = ""
        
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                context_text += f"ç”¨æˆ·é—®é¢˜: {content}\n"
            elif role == "assistant":
                # æå–æ€ç»´è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
                if "ğŸ¤” **æ€ç»´è¿‡ç¨‹**:" in content:
                    thinking_part = content.split("ğŸ¤” **æ€ç»´è¿‡ç¨‹**:")[1].split("ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:")[0].strip()
                    final_part = content.split("ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:")[1].strip() if "ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**:" in content else ""
                    context_text += f"åŠ©æ‰‹æ€è€ƒ: {thinking_part[:200]}...\n"
                    context_text += f"åŠ©æ‰‹å›ç­”: {final_part[:200]}...\n"
                else:
                    context_text += f"åŠ©æ‰‹: {content[:300]}...\n"
                
        return context_text.strip()

    @classmethod
    def format_thinking_prompt(cls, 
                             query: str, 
                             messages: List[dict], 
                             strategy: CoTStrategy,
                             max_turns: int = None) -> str:
        """æ ¹æ®ç­–ç•¥æ ¼å¼åŒ–æ€ç»´é“¾ç”Ÿæˆæç¤º"""
        context_text = cls.extract_recent_context(messages, max_turns)
        
        # æ ¹æ®ä¸åŒç­–ç•¥è°ƒç”¨å¯¹åº”çš„æ¨¡æ¿å‡½æ•°
        strategy_templates = {
            CoTStrategy.BASIC_COT: format_basic_cot_prompt,
            CoTStrategy.TREE_OF_THOUGHTS: format_tree_thoughts_prompt,
            CoTStrategy.SELF_REFLECTION: format_self_reflection_prompt,
            CoTStrategy.REVERSE_CHAIN: format_reverse_chain_prompt,
            CoTStrategy.MULTI_PERSPECTIVE: format_multi_perspective_prompt,
            CoTStrategy.STEP_BACK: format_step_back_prompt,
            CoTStrategy.ANALOGICAL: format_analogical_prompt,
            CoTStrategy.PLAN: format_plan_prompt
        }
        
        template_func = strategy_templates.get(strategy, format_basic_cot_prompt)
        return template_func(query, context_text)

    @classmethod
    def format_answer_prompt(cls,
                           query: str,
                           thinking_process: str,
                           messages: List[dict],
                           max_turns: int = None) -> str:
        """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæç¤º"""
        context_text = cls.extract_recent_context(messages, max_turns)
        user_profile = cls.analyze_user_profile(messages)
        
        # æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´å›ç­”é£æ ¼
        role_descriptions = {
            "programming": "æŠ€æœ¯å¼€å‘äººå‘˜",
            "business": "å•†ä¸šç®¡ç†äººå‘˜", 
            "academic": "å­¦æœ¯ç ”ç©¶äººå‘˜",
            "design": "è®¾è®¡ä¸“ä¸šäººå‘˜",
            "finance": "é‡‘èä»ä¸šäººå‘˜",
            "healthcare": "åŒ»ç–—å¥åº·ä»ä¸šè€…",
            "education": "æ•™è‚²å·¥ä½œè€…",
            "general_user": "ä¸€èˆ¬ç”¨æˆ·"
        }
        
        expertise_descriptions = {
            "expert": "å…·æœ‰æ·±åšä¸“ä¸šåŸºç¡€çš„é«˜çº§ç”¨æˆ·",
            "intermediate": "å…·æœ‰ä¸€å®šç»éªŒçš„ä¸­çº§ç”¨æˆ·", 
            "beginner": "åˆå­¦è€…æˆ–æ–°æ‰‹ç”¨æˆ·"
        }
        
        user_role = role_descriptions.get(user_profile["role_category"], "ä¸€èˆ¬ç”¨æˆ·")
        user_level = expertise_descriptions.get(user_profile["expertise_level"], "å…·æœ‰ä¸€å®šç»éªŒçš„ä¸­çº§ç”¨æˆ·")
        
        prompt_content = f"""è¯·åŸºäºä»¥ä¸‹åˆ†æè¿‡ç¨‹ï¼Œä¸º{user_role}({user_level})ç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

ç”¨æˆ·ç”»åƒåˆ†æ:
- è§’è‰²ç±»å‹: {user_role}
- ä¸“ä¸šæ°´å¹³: {user_level}
- ä¸“ä¸šé¢†åŸŸ: {', '.join(user_profile['professional_domains']) if user_profile['professional_domains'] else 'é€šç”¨é¢†åŸŸ'}
- äº¤äº’ç‰¹å¾: {', '.join(user_profile['interaction_patterns']) if user_profile['interaction_patterns'] else 'æ ‡å‡†äº¤äº’'}
- éœ€æ±‚åŠ¨æœº: {', '.join(user_profile['motivation_indicators']) if user_profile['motivation_indicators'] else 'ä¸€èˆ¬å’¨è¯¢'}

åˆ†æè¿‡ç¨‹:
{thinking_process}

åŸå§‹é—®é¢˜: {query}"""

        if context_text.strip():
            prompt_content = f"""è¯·åŸºäºå¯¹è¯å†å²å’Œåˆ†æè¿‡ç¨‹ï¼Œä¸º{user_role}({user_level})ç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

ç›¸å…³å¯¹è¯å†å²:
{context_text}

ç”¨æˆ·ç”»åƒåˆ†æ:
- è§’è‰²ç±»å‹: {user_role}
- ä¸“ä¸šæ°´å¹³: {user_level}
- ä¸“ä¸šé¢†åŸŸ: {', '.join(user_profile['professional_domains']) if user_profile['professional_domains'] else 'é€šç”¨é¢†åŸŸ'}
- äº¤äº’ç‰¹å¾: {', '.join(user_profile['interaction_patterns']) if user_profile['interaction_patterns'] else 'æ ‡å‡†äº¤äº’'}
- éœ€æ±‚åŠ¨æœº: {', '.join(user_profile['motivation_indicators']) if user_profile['motivation_indicators'] else 'ä¸€èˆ¬å’¨è¯¢'}

åˆ†æè¿‡ç¨‹:
{thinking_process}

åŸå§‹é—®é¢˜: {query}"""

        # æ ¹æ®ç”¨æˆ·æ°´å¹³è°ƒæ•´å›ç­”è¦æ±‚
        if user_profile["expertise_level"] == "expert":
            answer_requirements = """
è¯·åŸºäºä¸Šè¿°åˆ†æè¿‡ç¨‹æä¾›é«˜çº§ä¸“ä¸šçš„æœ€ç»ˆç­”æ¡ˆï¼š
1. æä¾›æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚å’Œåº•å±‚åŸç†åˆ†æ
2. åŒ…å«é«˜çº§æ¦‚å¿µã€æœ€ä½³å®è·µå’Œä¼˜åŒ–å»ºè®®
3. è®¨è®ºç›¸å…³çš„å‰æ²¿å‘å±•å’Œæœªæ¥è¶‹åŠ¿
4. æä¾›å…·ä½“çš„å®ç°æ–¹æ¡ˆå’Œæ¶æ„å»ºè®®
5. å¦‚æœæ¶‰åŠå¤šç§è§£å†³æ–¹æ¡ˆï¼Œè¯·è¯¦ç»†æ¯”è¾ƒå„è‡ªçš„ä¼˜ç¼ºç‚¹
6. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§å’Œæƒå¨æ€§"""
        elif user_profile["expertise_level"] == "beginner":
            answer_requirements = """
è¯·åŸºäºä¸Šè¿°åˆ†æè¿‡ç¨‹æä¾›æ˜“äºç†è§£çš„æœ€ç»ˆç­”æ¡ˆï¼š
1. ä½¿ç”¨ç®€å•æ˜äº†çš„è¯­è¨€ï¼Œé¿å…è¿‡å¤šä¸“ä¸šæœ¯è¯­
2. æä¾›åŸºç¡€æ¦‚å¿µçš„è§£é‡Šå’ŒèƒŒæ™¯ä»‹ç»
3. ç»™å‡ºå…·ä½“çš„æ­¥éª¤æŒ‡å¯¼å’Œæ“ä½œå»ºè®®
4. åŒ…å«å®ç”¨çš„ç¤ºä¾‹å’Œç±»æ¯”è¯´æ˜
5. æä¾›è¿›ä¸€æ­¥å­¦ä¹ çš„æ–¹å‘å’Œèµ„æºå»ºè®®
6. ç¡®ä¿å†…å®¹å¾ªåºæ¸è¿›ï¼Œä¾¿äºåˆå­¦è€…æŒæ¡"""
        else:  # intermediate
            answer_requirements = """
è¯·åŸºäºä¸Šè¿°åˆ†æè¿‡ç¨‹æä¾›ä¸“ä¸šå®ç”¨çš„æœ€ç»ˆç­”æ¡ˆï¼š
1. ç»¼åˆåˆ†æè¿‡ç¨‹ä¸­çš„å…³é”®æ´å¯Ÿå’Œç»“è®º
2. æä¾›ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†çš„ä¸“ä¸šå›ç­”
3. åŒ…å«é€‚åº¦çš„æŠ€æœ¯ç»†èŠ‚å’Œå®è·µæŒ‡å¯¼
4. ç»™å‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆå’Œå®æ–½å»ºè®®
5. å¹³è¡¡ç†è®ºåˆ†æä¸å®é™…åº”ç”¨
6. å¦‚æœåˆ†æä¸­æ¶‰åŠå¤šä¸ªå¯èƒ½æ€§ï¼Œè¯·æ˜ç¡®è¯´æ˜å„è‡ªçš„é€‚ç”¨æ¡ä»¶"""

        prompt_content += answer_requirements

        prompt_content += """

è¯·ç›´æ¥æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œä½¿ç”¨ä¸“ä¸šè§„èŒƒä¸”é€‚åˆç›®æ ‡ç”¨æˆ·æ°´å¹³çš„è¡¨è¾¾æ–¹å¼ã€‚"""

        return prompt_content


# ================================
# Pipelineä¸»ä½“ç±»
# ================================

class Pipeline:
    class Valves(BaseModel):
        # OpenAI APIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # æ€ç»´é“¾ç‰¹å®šé…ç½®
        COT_STRATEGY: Literal[
            "basic_cot",
            "tree_of_thoughts", 
            "self_reflection",
            "reverse_chain",
            "multi_perspective",
            "step_back",
            "analogical",
            "plan"
        ] = Field(
            default="basic_cot",
            description="æ€ç»´é“¾æ¨ç†ç­–ç•¥ï¼šbasic_cot(åŸºç¡€æ€ç»´é“¾), tree_of_thoughts(æ ‘çŠ¶æ€ç»´), self_reflection(è‡ªæˆ‘åæ€), reverse_chain(åå‘æ€ç»´é“¾), multi_perspective(å¤šè§†è§’åˆ†æ), step_back(æ­¥é€€åˆ†æ), analogical(ç±»æ¯”æ¨ç†), plan(è®¡åˆ’æ¨ç†)"
        )
        THINKING_TEMPERATURE: float
        ANSWER_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int
        
        # é«˜çº§é…ç½®
        MAX_THINKING_TOKENS: int
        ENABLE_SELF_CORRECTION: bool
        USE_THINKING_CACHE: bool

    def __init__(self):
        self.name = "Chain of Thought Pipeline"
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "thinking_tokens": 0,
            "answer_tokens": 0, 
            "total_tokens": 0
        }
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # æ€ç»´é“¾ç‰¹å®šé…ç½®
                "COT_STRATEGY": os.getenv("COT_STRATEGY", "basic_cot"),
                "THINKING_TEMPERATURE": float(os.getenv("THINKING_TEMPERATURE", "0.8")),
                "ANSWER_TEMPERATURE": float(os.getenv("ANSWER_TEMPERATURE", "0.6")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
                
                # é«˜çº§é…ç½®
                "MAX_THINKING_TOKENS": int(os.getenv("MAX_THINKING_TOKENS", "2000")),
                "ENABLE_SELF_CORRECTION": os.getenv("ENABLE_SELF_CORRECTION", "false").lower() == "true",
                "USE_THINKING_CACHE": os.getenv("USE_THINKING_CACHE", "false").lower() == "true",
            }
        )

    async def on_startup(self):
        print(f"æ€ç»´é“¾æ¨ç†Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            
        # éªŒè¯ç­–ç•¥é…ç½®
        try:
            CoTStrategy(self.valves.COT_STRATEGY)
            print(f"âœ… æ€ç»´é“¾é…ç½®éªŒè¯æˆåŠŸ: ç­–ç•¥={self.valves.COT_STRATEGY}")
        except ValueError as e:
            print(f"âŒ æ€ç»´é“¾é…ç½®é”™è¯¯: {e}")

    async def on_shutdown(self):
        print(f"æ€ç»´é“¾æ¨ç†Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_thinking_tokens(self, text: str):
        """æ·»åŠ æ€ç»´è¿‡ç¨‹tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["thinking_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_answer_tokens(self, text: str):
        """æ·»åŠ ç­”æ¡ˆtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["answer_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "thinking_tokens": 0,
            "answer_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _get_strategy(self) -> CoTStrategy:
        """è·å–å½“å‰çš„ç­–ç•¥é…ç½®"""
        try:
            strategy = CoTStrategy(self.valves.COT_STRATEGY)
        except ValueError:
            strategy = CoTStrategy.BASIC_COT
        return strategy

    def _stage1_thinking(self, query: str, messages: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ1ï¼šç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"

        strategy = self._get_strategy()
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç°åœ¨éœ€è¦å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ·±åº¦æ€è€ƒã€‚è¯·å±•ç°ä½ çš„æ€ç»´è¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€ã€‚

æ€è€ƒè¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šçš„ä¹¦é¢è¯­è¨€ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾
- æ€è€ƒè¿‡ç¨‹è¦è‡ªç„¶è¿è´¯ï¼Œå¦‚åŒä¸“ä¸šäººå£«å†…å¿ƒçš„æ€ç»´æµæ·Œ
- é’ˆå¯¹å…·ä½“é—®é¢˜æ·±å…¥æ€è€ƒï¼ŒåŒ…å«å®è´¨æ€§çš„åˆ†æå†…å®¹
- æ€è€ƒè¿‡ç¨‹ä¸­ä¸è¦å‡ºç°æ€»ç»“æˆ–ç»“è®ºéƒ¨åˆ†
- è®©æ€ç»´è‡ªç„¶å±•å¼€ï¼Œä½“ç°çœŸå®çš„æ¨ç†è¿‡ç¨‹

è¯·å¼€å§‹ä½ çš„æ·±åº¦æ€è€ƒï¼š"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_thinking_prompt(
            query=query,
            messages=messages or [],
            strategy=strategy,
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.MAX_THINKING_TOKENS,
            "temperature": self.valves.THINKING_TEMPERATURE,
            "stream": stream
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_thinking_tokens(system_prompt)
        self._add_thinking_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print(f"ğŸ¤” ç”Ÿæˆæ€ç»´è¿‡ç¨‹ - ç­–ç•¥: {strategy.value}")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æ¸©åº¦: {self.valves.THINKING_TEMPERATURE}")
        
        try:
            if stream:
                for chunk in self._stream_openai_response(url, headers, payload, is_thinking=True):
                    yield chunk
            else:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT
                )
                response.raise_for_status()
                result = response.json()
                
                thinking = result["choices"][0]["message"]["content"]
                self._add_thinking_tokens(thinking)
                
                if self.valves.DEBUG_MODE:
                    print(f"âœ… æ€ç»´è¿‡ç¨‹ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(thinking)}")
                    
                return thinking

        except Exception as e:
            error_msg = f"æ€ç»´é“¾ç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            if stream:
                yield error_msg
            else:
                return error_msg

    def _stage2_answer(self, query: str, thinking_process: str, messages: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ2ï¼šåŸºäºæ€ç»´é“¾ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦åŸºäºå‰è¿°æ€ç»´åˆ†æè¿‡ç¨‹ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å®ç”¨çš„æœ€ç»ˆç­”æ¡ˆã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. ç»¼åˆæ€ç»´åˆ†æä¸­çš„å…³é”®æ´å¯Ÿå’Œç»“è®º
2. æä¾›ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†çš„ä¸“ä¸šå›ç­”
3. ç¡®ä¿ç­”æ¡ˆå‡†ç¡®æ€§å’Œå®ç”¨ä»·å€¼ï¼Œç›´æ¥å›åº”ç”¨æˆ·éœ€æ±‚
4. é€‚å½“æä¾›å…·ä½“çš„å»ºè®®ã€æ–¹æ¡ˆæˆ–æŒ‡å¯¼æ„è§
5. ä¿æŒå®¢è§‚ç«‹åœºï¼Œæ˜ç¡®è¡¨è¾¾ä»»ä½•ä¸ç¡®å®šæ€§æˆ–å±€é™æ€§

è¯·ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„ä¸­æ–‡è¡¨è¾¾ï¼Œç¡®ä¿è¯­è¨€è§„èŒƒä¸”æ˜“äºç†è§£ã€‚"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_answer_prompt(
            query=query,
            thinking_process=thinking_process,
            messages=messages or [],
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.ANSWER_TEMPERATURE,
            "stream": stream
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_answer_tokens(system_prompt)
        self._add_answer_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print("ğŸ“ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æ¸©åº¦: {self.valves.ANSWER_TEMPERATURE}")
        
        try:
            if stream:
                for chunk in self._stream_openai_response(url, headers, payload, is_thinking=False):
                    yield chunk
            else:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.valves.OPENAI_TIMEOUT
                )
                response.raise_for_status()
                result = response.json()

                answer = result["choices"][0]["message"]["content"]
                self._add_answer_tokens(answer)

                if self.valves.DEBUG_MODE:
                    print(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)}")

                return answer

        except Exception as e:
            error_msg = f"ç­”æ¡ˆç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            if stream:
                yield error_msg
            else:
                return error_msg

    def _generate_thinking_details(self, content: str, done: bool = False, duration: float = None) -> str:
        """ç”Ÿæˆæ€è€ƒå†…å®¹çš„ details æ ‡ç­¾
        
        å®ç°è¯´æ˜ï¼š
        - æµå¼è¾“å‡ºæ—¶ï¼Œæ¯æ¬¡æ¨é€å®Œæ•´çš„ <details> å—ï¼Œå†…å®¹é€æ­¥ä¸°å¯Œ
        - done=false è¡¨ç¤ºæ€è€ƒè¿›è¡Œä¸­ï¼Œdone=true è¡¨ç¤ºæ€è€ƒå®Œæˆ
        - æ€è€ƒå®Œæˆæ—¶ä¼šåŒ…å« duration å±æ€§ï¼Œæ˜¾ç¤ºæ€è€ƒè€—æ—¶
        - å‰ç«¯ä¼šè‡ªåŠ¨ç”¨æœ€æ–°çš„ details å—è¦†ç›–æ¸²æŸ“
        
        Args:
            content: æ€è€ƒå†…å®¹æ–‡æœ¬
            done: æ˜¯å¦æ€è€ƒå®Œæˆ
            duration: æ€è€ƒè€—æ—¶ï¼ˆç§’ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„ details HTML æ ‡ç­¾
        """
        done_attr = 'true' if done else 'false'
        duration_attr = f' duration="{int(duration)}"' if duration is not None else ''
        summary_text = f'Thought for {int(duration)} seconds' if done and duration else 'Thinkingâ€¦'
        
        return f'''<details type="reasoning" done="{done_attr}"{duration_attr}>
    <summary>{summary_text}</summary>
    <p>{content}</p>
</details>'''

    def _stream_thinking_with_details(self, user_message: str, messages: List[dict]) -> Generator:
        """æµå¼ç”Ÿæˆæ€è€ƒå†…å®¹ï¼Œæ¯æ¬¡è¾“å‡ºå®Œæ•´çš„detailså—"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return

        strategy = self._get_strategy()
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç°åœ¨éœ€è¦å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ·±åº¦æ€è€ƒã€‚è¯·å±•ç°ä½ çš„æ€ç»´è¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªç„¶æµç•…çš„ä¹¦é¢è¯­è¨€ã€‚

æ€è€ƒè¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šçš„ä¹¦é¢è¯­è¨€ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾
- æ€è€ƒè¿‡ç¨‹è¦è‡ªç„¶è¿è´¯ï¼Œå¦‚åŒä¸“ä¸šäººå£«å†…å¿ƒçš„æ€ç»´æµæ·Œ
- é’ˆå¯¹å…·ä½“é—®é¢˜æ·±å…¥æ€è€ƒï¼ŒåŒ…å«å®è´¨æ€§çš„åˆ†æå†…å®¹
- æ€è€ƒè¿‡ç¨‹ä¸­ä¸è¦å‡ºç°æ€»ç»“æˆ–ç»“è®ºéƒ¨åˆ†
- è®©æ€ç»´è‡ªç„¶å±•å¼€ï¼Œä½“ç°çœŸå®çš„æ¨ç†è¿‡ç¨‹

è¯·å¼€å§‹ä½ çš„æ·±åº¦æ€è€ƒï¼š"""

        # ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”Ÿæˆæç¤º
        user_prompt = HistoryContextManager.format_thinking_prompt(
            query=user_message,
            messages=messages or [],
            strategy=strategy,
            max_turns=self.valves.HISTORY_TURNS
        )

        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": self.valves.MAX_THINKING_TOKENS,
            "temperature": self.valves.THINKING_TEMPERATURE,
            "stream": True
        }
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        self._add_thinking_tokens(system_prompt)
        self._add_thinking_tokens(user_prompt)
        
        if self.valves.DEBUG_MODE:
            print(f"ğŸ¤” ç”Ÿæˆæ€ç»´è¿‡ç¨‹ - ç­–ç•¥: {strategy.value}")
            print(f"   æ¨¡å‹: {self.valves.OPENAI_MODEL}")
            print(f"   æ¸©åº¦: {self.valves.THINKING_TEMPERATURE}")
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_thinking_tokens(delta)
                                yield collected_content  # è¿”å›ç´¯ç§¯çš„å†…å®¹
                        except json.JSONDecodeError:
                            pass
            
            if self.valves.DEBUG_MODE:
                print(f"âœ… æ€ç»´è¿‡ç¨‹æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(collected_content)}")
                
        except Exception as e:
            error_msg = f"æ€ç»´é“¾ç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def _stream_openai_response(self, url, headers, payload, is_thinking=False):
        """æµå¼å¤„ç†OpenAIå“åº”"""
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                if is_thinking:
                                    self._add_thinking_tokens(delta)
                                else:
                                    self._add_answer_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            if self.valves.DEBUG_MODE:
                stage = "æ€ç»´è¿‡ç¨‹" if is_thinking else "æœ€ç»ˆç­”æ¡ˆ"
                print(f"âœ… {stage}æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(collected_content)}")
                
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°ï¼Œå¤„ç†ç”¨æˆ·è¯·æ±‚"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ’­ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        strategy = self._get_strategy()
        
        # é˜¶æ®µ1ï¼šç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING
        thinking_content = ""
        
        # å¼€å§‹è®¡æ—¶
        thinking_start_time = time.time()
        
        try:
            if stream_mode:
                # æµå¼æ¨¡å¼ - æ€ç»´è¿‡ç¨‹
                # å…ˆæ¨é€åˆå§‹çš„ details æ ‡ç­¾
                yield self._generate_thinking_details("æ­£åœ¨åˆ†æ...", done=False)
                
                for content in self._stream_thinking_with_details(user_message, messages):
                    thinking_content = content
                    # æ¯æ¬¡æ¨é€å®Œæ•´çš„ details å—
                    yield self._generate_thinking_details(thinking_content, done=False)
                    
                # è®¡ç®—æ€è€ƒæ—¶é•¿
                thinking_duration = time.time() - thinking_start_time
                # æ¨é€æœ€ç»ˆçš„å¸¦æ—¶é•¿çš„ details å—
                yield self._generate_thinking_details(thinking_content, done=True, duration=thinking_duration)
            else:
                # éæµå¼æ¨¡å¼ - æ€ç»´è¿‡ç¨‹
                # å…ˆæ¨é€åˆå§‹çš„ details æ ‡ç­¾
                yield self._generate_thinking_details("æ­£åœ¨åˆ†æ...", done=False)
                
                thinking_result = self._stage1_thinking(user_message, messages, stream=False)
                thinking_content = thinking_result
                
                # è®¡ç®—æ€è€ƒæ—¶é•¿
                thinking_duration = time.time() - thinking_start_time
                # æ¨é€æœ€ç»ˆçš„å¸¦æ—¶é•¿çš„ details å—
                yield self._generate_thinking_details(thinking_content, done=True, duration=thinking_duration)
                
        except Exception as e:
            error_msg = f"âŒ æ€ç»´è¿‡ç¨‹ç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg
            return

        yield "\n\n"
        
        # è‡ªæˆ‘çº é”™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.valves.ENABLE_SELF_CORRECTION and "é”™è¯¯" in thinking_content.lower():
            yield "ğŸ” **è‡ªæˆ‘çº é”™**: æ£€æµ‹åˆ°å¯èƒ½çš„é”™è¯¯ï¼Œæ­£åœ¨ä¿®æ­£...\n\n"
            # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªæˆ‘çº é”™é€»è¾‘
        
        # é˜¶æ®µ2ï¼šåŸºäºæ€ç»´é“¾ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        yield "ğŸ“ **æœ€ç»ˆç­”æ¡ˆ**: \n\n"
        
        try:
            if stream_mode:
                # æµå¼æ¨¡å¼ - æœ€ç»ˆç­”æ¡ˆ
                for chunk in self._stage2_answer(user_message, thinking_content, messages, stream=True):
                    yield chunk
                # æµå¼æ¨¡å¼ç»“æŸåæ·»åŠ tokenç»Ÿè®¡
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **åˆ†æç»Ÿè®¡**: æ€ç»´è¿‡ç¨‹ {token_info['thinking_tokens']} tokens, æœ€ç»ˆç­”æ¡ˆ {token_info['answer_tokens']} tokens, æ€»è®¡ {token_info['total_tokens']} tokens"
            else:
                # éæµå¼æ¨¡å¼ - æœ€ç»ˆç­”æ¡ˆ
                result = self._stage2_answer(user_message, thinking_content, messages, stream=False)
                yield result
                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **åˆ†æç»Ÿè®¡**: æ€ç»´è¿‡ç¨‹ {token_info['thinking_tokens']} tokens, æœ€ç»ˆç­”æ¡ˆ {token_info['answer_tokens']} tokens, æ€»è®¡ {token_info['total_tokens']} tokens"

        except Exception as e:
            error_msg = f"âŒ æœ€ç»ˆç­”æ¡ˆç”Ÿæˆé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

        # æä¾›æ”¹è¿›å»ºè®®
        if self.valves.DEBUG_MODE:
            strategy_suggestions = {
                CoTStrategy.BASIC_COT: "å¦‚éœ€æ›´æ·±å…¥åˆ†æï¼Œå¯å°è¯•tree_of_thoughtsæˆ–planç­–ç•¥",
                CoTStrategy.TREE_OF_THOUGHTS: "å¦‚éœ€å¿«é€Ÿåˆ†æï¼Œå¯å°è¯•basic_cotç­–ç•¥", 
                CoTStrategy.SELF_REFLECTION: "å¦‚éœ€å¤šè§’åº¦åˆ†æï¼Œå¯å°è¯•multi_perspectiveç­–ç•¥",
                CoTStrategy.REVERSE_CHAIN: "å¦‚éœ€æ­£å‘æ¨ç†ï¼Œå¯å°è¯•basic_cotç­–ç•¥",
                CoTStrategy.MULTI_PERSPECTIVE: "å¦‚éœ€æ·±åº¦åæ€ï¼Œå¯å°è¯•self_reflectionç­–ç•¥",
                CoTStrategy.STEP_BACK: "å¦‚éœ€å…·ä½“åˆ†æï¼Œå¯å°è¯•basic_cotç­–ç•¥",
                CoTStrategy.ANALOGICAL: "å¦‚éœ€é€»è¾‘æ¨ç†ï¼Œå¯å°è¯•basic_cotç­–ç•¥",
                CoTStrategy.PLAN: "å¦‚éœ€æ·±å…¥æ€è€ƒï¼Œå¯å°è¯•tree_of_thoughtsæˆ–self_reflectionç­–ç•¥"
            }
            
            if strategy in strategy_suggestions:
                yield f"\n\nğŸ’¡ **ä¼˜åŒ–å»ºè®®**: {strategy_suggestions[strategy]}"
