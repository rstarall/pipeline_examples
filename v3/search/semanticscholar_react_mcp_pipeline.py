"""
åŸºäºv2\search\semanticscholar_mcp\src\mcp_server.pyï¼Œå‚è€ƒv3\search\paperlist_react_mcp_pipeline.pyç¼–å†™æœ¬pipeline
ä½¿ç”¨ReActæ¨¡å¼ï¼Œå®ç°æ·±åº¦æ€è€ƒï¼Œå¹¶è¿›è¡Œå¤šè½®Semantic Scholar MCPå·¥å…·è°ƒç”¨ï¼Œæœ€ç»ˆç»™å‡ºå›ç­”ã€‚
1.Reasoningé˜¶æ®µï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€å†å²ä¼šè¯ã€å½“å‰è·å–åˆ°çš„è®ºæ–‡ä¿¡æ¯è¿›è¡Œè‡ªä¸»æ€è€ƒåˆ¤æ–­
  - åˆ¶å®šåˆæ¬¡å·¥å…·è°ƒç”¨çš„Action, è°ƒç”¨semantic scholarå·¥å…·ï¼Œè·å–è®ºæ–‡ä¿¡æ¯
2.Actioné˜¶æ®µï¼Œè°ƒç”¨MCPå·¥å…·ï¼Œè·å–ä¿¡æ¯
3.Observationé˜¶æ®µ(æ¯æ¬¡Actionåéƒ½éœ€è¦è¿›è¡Œè§‚å¯Ÿ)
  - æ ¹æ®Actionçš„æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜(é—®é¢˜çš„ç›¸å…³æ€§ï¼Œè¿›ä¸€æ­¥æ¢ç´¢çš„å¿…è¦æ€§)
  - å¦‚æœä¿¡æ¯ä¸å……åˆ†ï¼Œåˆ™åˆ¶å®šæ–°çš„Action(æ›´æ–°æŸ¥è¯¢å…³é”®è¯ã€ä½¿ç”¨å‰ä¸€æ­¥æ£€ç´¢åˆ°çš„ä¿¡æ¯æ›´æ–°æŸ¥è¯¢å…³é”®è¯)ï¼Œè°ƒç”¨semantic scholarå·¥å…·ï¼Œè·å–è®ºæ–‡ä¿¡æ¯
  - å¦‚æœä¿¡æ¯å……åˆ†ï¼Œåˆ™è·³è½¬ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
4.ç­”æ¡ˆç”Ÿæˆé˜¶æ®µï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€å†å²ä¼šè¯ã€å½“å‰è·å–åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶è¿”å›ç»™ç”¨æˆ·
5.é™¤äº†ç­”æ¡ˆç”Ÿæˆé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨_emit_processingæ–¹æ³•ï¼Œè¿”å›å¤„ç†è¿‡ç¨‹å†…å®¹å’Œæ€è€ƒï¼Œå‡å°‘debugæè¿°å†…å®¹çš„è¾“å‡º
6.å¯¹äºActionå’ŒObservationé˜¶æ®µï¼Œ_emit_processingé‡‡ç”¨åŠ¨æ€é€’è¿›çš„processing_stage
7.æ³¨æ„ä»£ç çš„æ•´æ´ç®€ç»ƒï¼Œå‡½æ•°çš„è§£è€¦ï¼Œé¿å…é‡å¤ä»£ç å’Œè¿‡å¤šdebugè¾“å‡º
8.æ³¨æ„ï¼šSemantic Scholar MCPå·¥å…·æ”¯æŒå¤æ‚çš„å­¦æœ¯æŸ¥è¯¢ï¼Œå¯ä»¥ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€ä½œè€…åç§°ã€æœŸåˆŠåç­‰
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
from typing import List, Union, Generator, Iterator, Dict, Any, Optional, AsyncGenerator
from pydantic import BaseModel
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ReActé˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "reasoning": "ğŸ¤” æ¨ç†åˆ†æ",
    "action": "ğŸ”§ æ‰§è¡ŒåŠ¨ä½œ", 
    "observation": "ğŸ‘ï¸ è§‚å¯Ÿç»“æœ",
    "answer_generation": "ğŸ“ ç”Ÿæˆç­”æ¡ˆ",
    "mcp_discovery": "ğŸ” MCPæœåŠ¡å‘ç°"
}

STAGE_GROUP = {
    "reasoning": "stage_group_1",
    "action": "stage_group_2",
    "observation": "stage_group_3", 
    "answer_generation": "stage_group_4",
    "mcp_discovery": "stage_group_0"
}

class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        MAX_REACT_ITERATIONS: int
        MIN_PAPERS_THRESHOLD: int
        
        # MCPé…ç½®
        MCP_SERVER_URL: str
        MCP_TIMEOUT: int
        MCP_TOOLS_EXPIRE_HOURS: int

    def __init__(self):
        self.name = "Semantic Scholar ReAct MCP Academic Paper Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0, 
            "output_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # MCPå·¥å…·ç¼“å­˜
        self.mcp_tools = {}
        self.tools_loaded = False
        self.tools_loaded_time = None
        self.session_id = None
        
        # ReActçŠ¶æ€
        self.react_state = {
            "papers_collected": [],  # å­˜å‚¨å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            "query_history": [],
            "query_terms_used": set(),  # å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯é›†åˆ
            "extracted_keywords_history": set(),  # å†å²æå–çš„å…³é”®è¯é›†åˆ
            "current_iteration": 0,
            "current_offset": 0,  # å½“å‰åç§»é‡
            "current_limit": 10,  # å½“å‰æ¯é¡µæ•°é‡
            "query_offsets": {}  # è®°å½•æ¯ä¸ªæŸ¥è¯¢è¯ä½¿ç”¨çš„åç§»é‡ {query: offset}
        }
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "MAX_REACT_ITERATIONS": int(os.getenv("MAX_REACT_ITERATIONS", "10")),
                "MIN_PAPERS_THRESHOLD": int(os.getenv("MIN_PAPERS_THRESHOLD", "20")),
                
                # MCPé…ç½® - é»˜è®¤æŒ‡å‘semantic scholaræœåŠ¡
                "MCP_SERVER_URL": os.getenv("MCP_SERVER_URL", "http://localhost:8992"),
                "MCP_TIMEOUT": int(os.getenv("MCP_TIMEOUT", "30")),
                "MCP_TOOLS_EXPIRE_HOURS": int(os.getenv("MCP_TOOLS_EXPIRE_HOURS", "12")),
            }
        )

    async def on_startup(self):
        print(f"Semantic Scholar ReAct MCP Pipelineå¯åŠ¨: {__name__}")
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥")
        print(f"ğŸ”— MCPæœåŠ¡å™¨: {self.valves.MCP_SERVER_URL}")

    async def on_shutdown(self):
        print(f"Semantic Scholar ReAct MCP Pipelineå…³é—­: {__name__}")

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    async def _initialize_mcp_session(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """åˆå§‹åŒ–MCPä¼šè¯å¹¶è·å–æœåŠ¡å™¨åˆ†é…çš„session ID"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        try:
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # Step 1: å‘é€initializeè¯·æ±‚ï¼ˆä¸å¸¦session IDï¼‰
            initialize_request = {
                "jsonrpc": "2.0",
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {
                        "sampling": {},
                        "roots": {"listChanged": True}
                    },
                    "clientInfo": {
                        "name": "Semantic Scholar ReAct MCP Pipeline",
                        "version": "1.0.0"
                    }
                },
                "id": "init-1"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=initialize_request,
                    headers={
                        "Content-Type": "application/json",
                        "Accept": "application/json, text/event-stream"
                    },
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # æ£€æŸ¥å“åº”å¤´ä¸­çš„session ID
                        server_session_id = response.headers.get("Mcp-Session-Id")
                        if server_session_id:
                            self.session_id = server_session_id
                        
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            init_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "init-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            init_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            init_response = await response.json()
                        
                        if not init_response:
                            raise Exception("No initialize response received")
                        
                        if "error" in init_response:
                            raise Exception(f"MCP initialize error: {init_response['error']}")
                        
                        # Step 2: å‘é€initializedé€šçŸ¥
                        initialized_notification = {
                            "jsonrpc": "2.0",
                            "method": "notifications/initialized"
                        }
                        
                        headers = {
                            "Content-Type": "application/json",
                            "Accept": "application/json, text/event-stream"
                        }
                        if hasattr(self, 'session_id') and self.session_id:
                            headers["Mcp-Session-Id"] = self.session_id
                        
                        async with session.post(
                            mcp_url,
                            json=initialized_notification,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                        ) as notify_response:
                            if notify_response.status not in [200, 202]:
                                pass  # å¿½ç•¥initializedé€šçŸ¥å¤±è´¥
                        
                        init_msg = "ğŸ”§ MCPä¼šè¯åˆå§‹åŒ–å®Œæˆ"
                        if stream_mode:
                            for chunk in self._emit_processing(init_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield init_msg + "\n"
                    else:
                        error_text = await response.text()
                        raise Exception(f"Initialize failed - HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ MCPä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    async def _discover_mcp_tools(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """é€šè¿‡MCP JSON-RPCåè®®å‘ç°æœåŠ¡å™¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        start_msg = f"ğŸ” æ­£åœ¨å‘ç°Semantic Scholar MCPå·¥å…·..."
        if stream_mode:
            for chunk in self._emit_processing(start_msg, "mcp_discovery"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield start_msg + "\n"
        
        # é¦–å…ˆåˆå§‹åŒ–MCPä¼šè¯
        if not hasattr(self, '_session_initialized'):
            async for init_output in self._initialize_mcp_session(stream_mode):
                yield init_output
            self._session_initialized = True
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_request = {
                "jsonrpc": "2.0",
                "method": "tools/list",
                "id": "tools-list-1"
            }
            
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=mcp_request,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            mcp_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "tools-list-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            mcp_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            mcp_response = await response.json()
                        
                        if not mcp_response:
                            raise Exception("No tools/list response received")
                        
                        if "error" in mcp_response:
                            raise Exception(f"MCP error: {mcp_response['error']}")
                        
                        tools = mcp_response.get("result", {}).get("tools", [])
                        
                        # åŠ è½½æ‰€æœ‰å·¥å…·
                        for tool in tools:
                            tool_name = tool.get("name")
                            if tool_name:
                                self.mcp_tools[tool_name] = {
                                    "name": tool_name,
                                    "description": tool.get("description", ""),
                                    "input_schema": tool.get("inputSchema", {})
                                }
                        
                        self.tools_loaded = True
                        self.tools_loaded_time = time.time()  # è®°å½•å·¥å…·åŠ è½½æ—¶é—´
                        
                        final_msg = f"âœ… å‘ç° {len(self.mcp_tools)} ä¸ªSemantic Scholar MCPå·¥å…·"
                        if len(self.mcp_tools) > 0:
                            final_msg += f": {', '.join(self.mcp_tools.keys())}"
                        
                        if stream_mode:
                            for chunk in self._emit_processing(final_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield final_msg + "\n"
                        
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ Semantic Scholar MCPå·¥å…·å‘ç°å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    def _are_tools_expired(self) -> bool:
        """æ£€æŸ¥MCPå·¥å…·æ˜¯å¦å·²è¿‡æœŸ"""
        if not self.tools_loaded or self.tools_loaded_time is None:
            return True
        
        current_time = time.time()
        expire_seconds = self.valves.MCP_TOOLS_EXPIRE_HOURS * 3600  # è½¬æ¢ä¸ºç§’
        return (current_time - self.tools_loaded_time) > expire_seconds

    async def _ensure_tools_loaded(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """ç¡®ä¿MCPå·¥å…·å·²åŠ è½½ä¸”æœªè¿‡æœŸ"""
        need_reload = False
        reason = ""
        
        if not self.tools_loaded:
            need_reload = True
            reason = "å·¥å…·æœªåŠ è½½"
        elif self._are_tools_expired():
            need_reload = True
            expired_hours = (time.time() - self.tools_loaded_time) / 3600
            reason = f"å·¥å…·å·²è¿‡æœŸ ({expired_hours:.1f} å°æ—¶å‰åŠ è½½)"
        
        if need_reload:
            reload_msg = f"ğŸ”„ {reason}ï¼Œæ­£åœ¨é‡æ–°å‘ç°Semantic Scholar MCPå·¥å…·..."
            if stream_mode:
                for chunk in self._emit_processing(reload_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield reload_msg + "\n"
            
            # æ¸…é™¤æ—§çš„å·¥å…·å’Œä¼šè¯çŠ¶æ€
            self.mcp_tools = {}
            self.tools_loaded = False
            self.tools_loaded_time = None
            if hasattr(self, '_session_initialized'):
                delattr(self, '_session_initialized')
            self.session_id = None
            
            async for discovery_output in self._discover_mcp_tools(stream_mode):
                yield discovery_output

    async def _call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨MCP JSON-RPCåè®®è°ƒç”¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            return {"error": "MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®"}
        
        if not self.tools_loaded or self._are_tools_expired():
            try:
                async for output in self._ensure_tools_loaded(stream_mode=False):
                    # Silently consume the debug output in this context
                    pass
            except Exception as e:
                return {"error": f"å·¥å…·åŠ è½½å¤±è´¥: {str(e)}"}
        
        if tool_name not in self.mcp_tools:
            return {"error": f"å·¥å…· '{tool_name}' ä¸å¯ç”¨"}
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # MCP JSON-RPCæ ¼å¼è¯·æ±‚ä½“
            jsonrpc_payload = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": arguments
                },
                "id": f"mcp_{tool_name}_{int(time.time())}"
            }
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    headers=headers,
                    json=jsonrpc_payload,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            result = None
                            request_id = jsonrpc_payload["id"]
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == request_id:  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            result = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            result = await response.json()
                        
                        if not result:
                            return {"error": "No response received"}
                        
                        # å¤„ç†MCP JSON-RPCå“åº”
                        if "result" in result:
                            return result["result"]
                        elif "error" in result:
                            return {"error": f"MCPé”™è¯¯: {result['error'].get('message', 'Unknown error')}"}
                        else:
                            return {"error": "æ— æ•ˆçš„MCPå“åº”æ ¼å¼"}
                    else:
                        return {"error": f"HTTP {response.status}: {await response.text()}"}
                        
        except asyncio.TimeoutError:
            logger.error(f"MCPå·¥å…·è°ƒç”¨è¶…æ—¶: {tool_name}")
            return {"error": "è¯·æ±‚è¶…æ—¶"}
        except aiohttp.ClientError as e:
            logger.error(f"MCP HTTPè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _execute_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """æ‰§è¡ŒMCPå·¥å…·å¹¶è¿”å›åŸå§‹ç»“æœ"""
        result = await self._call_mcp_tool(tool_name, arguments)
        
        # ç›´æ¥è¿”å›åŸå§‹JSONç»“æœï¼Œè®©LLMè‡ªä¸»å¤„ç†å†…å®¹
        return json.dumps(result, ensure_ascii=False, indent=2)

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI APIå¹¶ç»Ÿè®¡tokenä½¿ç”¨é‡"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        # ç»Ÿè®¡è¾“å…¥tokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
        input_text = ""
        for msg in messages:
            input_text += msg.get("content", "")
        input_tokens = len(input_text)
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            result = response.json()
            
            # è·å–å“åº”å†…å®¹
            response_content = result["choices"][0]["message"]["content"]
            
            # ç»Ÿè®¡è¾“å‡ºtokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
            output_tokens = len(response_content)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.token_stats["input_tokens"] += input_tokens
            self.token_stats["output_tokens"] += output_tokens
            self.token_stats["total_tokens"] += input_tokens + output_tokens
            self.token_stats["api_calls"] += 1
            
            return response_content
        except Exception as e:
            return f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”å¹¶ç»Ÿè®¡tokenä½¿ç”¨é‡"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        # ç»Ÿè®¡è¾“å…¥tokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
        input_text = ""
        for msg in messages:
            input_text += msg.get("content", "")
        input_tokens = len(input_text)
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        # ç”¨äºç´¯ç§¯è¾“å‡ºå†…å®¹
        output_content = ""
        
        try:
            response = requests.post(url, headers=headers, json=payload, stream=True, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                output_content += delta
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            # ç»Ÿè®¡è¾“å‡ºtokenæ•°é‡å¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            output_tokens = len(output_content)
            self.token_stats["input_tokens"] += input_tokens
            self.token_stats["output_tokens"] += output_tokens
            self.token_stats["total_tokens"] += input_tokens + output_tokens
            self.token_stats["api_calls"] += 1
            
        except Exception as e:
            yield f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"

    async def _reasoning_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActæ¨ç†é˜¶æ®µ - é’ˆå¯¹Semantic Scholaræœç´¢ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–"""
        context = self._build_conversation_context(user_message, messages)
        used_queries = list(self.react_state['query_terms_used'])
        
        reasoning_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æœç´¢åŠ©æ‰‹ã€‚è¯·åŸºäºç”¨æˆ·é—®é¢˜å’Œå·²æœ‰ä¿¡æ¯åˆ¶å®šæœç´¢ç­–ç•¥ã€‚

ç”¨æˆ·é—®é¢˜: {user_message}
å¯¹è¯å†å²: {context}
å·²ä½¿ç”¨æŸ¥è¯¢è¯: {used_queries}

**é‡è¦è¯´æ˜ï¼š**
- æœ¬ç³»ç»Ÿä½¿ç”¨çš„æ˜¯Semantic Scholar MCPå·¥å…·ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å­¦æœ¯æœç´¢å¼•æ“
- æ”¯æŒå¤æ‚çš„å­¦æœ¯æŸ¥è¯¢ï¼ŒåŒ…æ‹¬ä¸“ä¸šæœ¯è¯­ã€ä½œè€…åç§°ã€æœŸåˆŠåç§°ç­‰
- è¿”å›ç»“æœåŒ…å«å®Œæ•´çš„è®ºæ–‡å…ƒæ•°æ®ï¼šæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å¼•ç”¨æ•°ã€æœŸåˆŠç­‰

**åˆ†æä»»åŠ¡ï¼š**
1. åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢è®ºæ–‡ï¼Ÿ
2. å¦‚æœéœ€è¦æœç´¢ï¼Œä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ ¸å¿ƒå­¦æœ¯æŸ¥è¯¢å…³é”®è¯
3. é¿å…é‡å¤å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯: {used_queries}

**æŸ¥è¯¢è¯è¦æ±‚ï¼ˆé€‚é…Semantic Scholarç‰¹ç‚¹ï¼‰ï¼š**
- å¯ä»¥ä½¿ç”¨å¤æ‚çš„å­¦æœ¯æœ¯è¯­ç»„åˆ
- æ”¯æŒä½œè€…åç§°æŸ¥è¯¢ï¼ˆå¦‚ "author:Smith machine learning"ï¼‰
- æ”¯æŒæœŸåˆŠåç§°æŸ¥è¯¢ï¼ˆå¦‚ "venue:Nature artificial intelligence"ï¼‰
- æ”¯æŒå…·ä½“æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ "transformer attention mechanism"ï¼‰
- æ”¯æŒå¤šè¯ç»„åˆæŸ¥è¯¢ï¼ˆå¦‚ "deep learning medical image segmentation"ï¼‰

**ç¤ºä¾‹ï¼š**
ç”¨æˆ·é—®é¢˜"æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦å½±åƒä¸­çš„åº”ç”¨" â†’ æŸ¥è¯¢: "machine learning medical imaging" æˆ– "deep learning medical image"
ç”¨æˆ·é—®é¢˜"Transformeræ¶æ„çš„æœ€æ–°ç ”ç©¶" â†’ æŸ¥è¯¢: "transformer architecture attention mechanism"
ç”¨æˆ·é—®é¢˜"è‡ªç„¶è¯­è¨€å¤„ç†çš„BERTæ¨¡å‹" â†’ æŸ¥è¯¢: "BERT natural language processing"
ç”¨æˆ·é—®é¢˜"Geoffrey Hintonçš„æ·±åº¦å­¦ä¹ ç ”ç©¶" â†’ æŸ¥è¯¢: "author:Geoffrey Hinton deep learning"

å›å¤æ ¼å¼ï¼š
```json
{{
    "need_search": true/false,
    "query": "é€‚åˆSemantic Scholaræœç´¢çš„å­¦æœ¯æŸ¥è¯¢è¯",
    "reasoning": "åŸºäºç”¨æˆ·é—®é¢˜å’Œå·²æœ‰ä¿¡æ¯çš„åˆ†æ",
    "sufficient_info": true/false
}}
```"""

        if stream_mode:
            for chunk in self._emit_processing("åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¶å®šé€‚åˆSemantic Scholaræœç´¢çš„ç­–ç•¥...", "reasoning"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        decision = self._call_openai_api("", reasoning_prompt, json_mode=True)
        
        try:
            decision_data = json.loads(decision)
            if stream_mode:
                reasoning_content = f"æ¨ç†åˆ†æï¼š{decision_data.get('reasoning', 'æ— ')}"
                for chunk in self._emit_processing(reasoning_content, "reasoning"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("decision", decision_data)
        except json.JSONDecodeError:
            yield ("decision", {"need_search": False, "sufficient_info": True, "reasoning": "è§£æå¤±è´¥"})

    async def _action_phase(self, query: str, limit: int = 10, offset: int = 0, stream_mode: bool = False) -> AsyncGenerator[tuple, None]:
        """ReActåŠ¨ä½œé˜¶æ®µ - ä½¿ç”¨Semantic Scholarå·¥å…·"""
        # æ›´æ–°å½“å‰åç§»é‡çŠ¶æ€
        self.react_state["current_offset"] = offset
        self.react_state["current_limit"] = limit
        self.react_state["query_offsets"][query] = offset
        
        if stream_mode:
            action_msg = f"æ‰§è¡Œè®ºæ–‡æœç´¢ï¼š{query} (åç§»é‡{offset}ï¼Œé™åˆ¶{limit}ç¯‡)"
            for chunk in self._emit_processing(action_msg, "action"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # è°ƒç”¨semantic scholarå·¥å…·æœç´¢è®ºæ–‡
        tool_args = {
            "query": query,
            "limit": limit,
            "offset": offset
        }
        
        # è·å–åŸå§‹å·¥å…·è°ƒç”¨ç»“æœ
        tool_result = await self._execute_mcp_tool("search_papers", tool_args)

        # æ ¼å¼ç¾åŒ–
        try:
            json_result = json.loads(tool_result)
            tool_result = json.dumps(json_result, ensure_ascii=False, indent=2)
        except json.JSONDecodeError:
            pass
        
        # ä½¿ç”¨_emit_processingè¾“å‡ºå·¥å…·è¿”å›ç»“æœçš„markdownä»£ç æ¡†
        if stream_mode:
            tool_output_msg = f"**å·¥å…·è°ƒç”¨ç»“æœ:**\n\n```json\n{tool_result}\n```"
            for chunk in self._emit_processing(tool_output_msg, "action"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # è®°å½•æŸ¥è¯¢å†å²å’ŒæŸ¥è¯¢è¯
        self.react_state['query_history'].append(query)
        self.react_state['query_terms_used'].add(query.lower())
        
        yield ("result", tool_result)

    async def _observation_phase(self, action_result: str, query: str, user_message: str, stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActè§‚å¯Ÿé˜¶æ®µ - é’ˆå¯¹Semantic Scholarç»“æœæ ¼å¼è¿›è¡Œåˆ†æ"""
        if stream_mode:
            for chunk in self._emit_processing("è§‚å¯Ÿæœç´¢ç»“æœï¼Œåˆ†æè®ºæ–‡å†…å®¹ï¼Œæå–æ–°æŸ¥è¯¢å…³é”®è¯...", "observation"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # æ„å»ºè§‚å¯Ÿprompt
        used_queries = list(self.react_state['query_terms_used'])
        extracted_history = list(self.react_state['extracted_keywords_history'])
        unused_keywords = [kw for kw in extracted_history if kw.lower() not in self.react_state['query_terms_used']]
        current_offset = self.react_state.get('current_offset', 0)
        current_limit = self.react_state.get('current_limit', 10)
        
        observation_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·åŸºäºSemantic Scholaræœç´¢ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼š

ç”¨æˆ·é—®é¢˜: {user_message}  
ä½¿ç”¨çš„æŸ¥è¯¢è¯: {query}
å½“å‰åç§»é‡: {current_offset} (é™åˆ¶{current_limit}ç¯‡)
å½“å‰è¿­ä»£: {self.react_state['current_iteration']}/{self.valves.MAX_REACT_ITERATIONS}
å·²ä½¿ç”¨æŸ¥è¯¢è¯: {used_queries}
å†å²æå–çš„å…³é”®è¯: {extracted_history}
å†å²æœªä½¿ç”¨çš„å…³é”®è¯: {unused_keywords}

å½“å‰æœç´¢ç»“æœåŸå§‹æ•°æ®:
{action_result}

**å…³é”®ä»»åŠ¡ï¼š**
1. è‡ªä¸»åˆ†æå½“å‰æœç´¢ç»“æœçš„åŸå§‹JSONæ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦æˆåŠŸæ‰¾åˆ°ç›¸å…³è®ºæ–‡
2. æŸ¥çœ‹JSONä¸­çš„successå­—æ®µï¼Œäº†è§£æœç´¢æ˜¯å¦æˆåŠŸ
3. ä»”ç»†åˆ†æJSONä¸­"results"æ•°ç»„å†…è®ºæ–‡çš„æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å¼•ç”¨æ•°ç­‰å†…å®¹
4. ä»è®ºæ–‡å†…å®¹ä¸­è¯†åˆ«ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„**å…³é”®è¯**å’Œ**ä¸“ä¸šæœ¯è¯­**
5. è®°å½•é«˜ç›¸å…³åº¦çš„å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€ç›¸å…³æ€§æƒé‡ï¼‰
6. åŸºäºtotal_countåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šç»“æœéœ€è¦è·å–
7. é€‰æ‹©èƒ½å¤Ÿè¿›ä¸€æ­¥æ·±å…¥æ¢ç´¢ç›¸å…³ä¸»é¢˜çš„æ–°æŸ¥è¯¢è¯

**é’ˆå¯¹Semantic Scholarçš„æŸ¥è¯¢è¯é€‰æ‹©ç­–ç•¥ï¼š**
- å¯ä»¥ä½¿ç”¨å¤æ‚çš„å­¦æœ¯æœ¯è¯­ç»„åˆ
- æ”¯æŒä½œè€…æŸ¥è¯¢ï¼šauthor:"ä½œè€…å" + ä¸»é¢˜
- æ”¯æŒæœŸåˆŠæŸ¥è¯¢ï¼švenue:"æœŸåˆŠå" + ä¸»é¢˜  
- æ”¯æŒå…·ä½“æŠ€æœ¯æœ¯è¯­å’Œæ–¹æ³•åç§°
- ä¼˜å…ˆçº§1: ä»å½“å‰è®ºæ–‡å†…å®¹ä¸­æå–çš„æ–°ä¸“ä¸šåè¯
- ä¼˜å…ˆçº§2: ç»“åˆä½œè€…æˆ–æœŸåˆŠçš„æ·±åº¦æŸ¥è¯¢
- ä¼˜å…ˆçº§3: å†å²æœªä½¿ç”¨çš„å…³é”®è¯ï¼ˆå¦‚æœä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ï¼‰

**æŸ¥è¯¢è¯ç¤ºä¾‹ï¼š**
- å¤åˆæŸ¥è¯¢: "transformer attention mechanism NLP"
- ä½œè€…æŸ¥è¯¢: "author:Yoshua Bengio deep learning"
- æœŸåˆŠæŸ¥è¯¢: "venue:Nature machine learning medical"
- æŠ€æœ¯æŸ¥è¯¢: "BERT fine-tuning language model"
- é¢†åŸŸæŸ¥è¯¢: "computer vision object detection CNN"

**åˆ†é¡µç­–ç•¥ï¼š**
- åŸºäºtotal_countåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šç»“æœ
- å¦‚æœå½“å‰ç»“æœç›¸å…³æ€§é«˜ä¸”total_count > current_offset + current_limitï¼Œå¯è€ƒè™‘è·å–æ›´å¤š
- ä½¿ç”¨current_offset + current_limitä½œä¸ºnew_offsetå€¼
- åˆ†é¡µé€‚ç”¨äºå½“å‰æŸ¥è¯¢è¯ï¼Œé¿å…é¢‘ç¹åˆ†é¡µå½±å“æ•ˆç‡

**å…³é”®è®ºæ–‡ç­›é€‰æ ‡å‡†ï¼š**
- åŸºäºè®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦ã€å¼•ç”¨æ•°è¯„ä¼°ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
- ä¼˜å…ˆè€ƒè™‘å¼•ç”¨æ•°è¾ƒé«˜çš„é‡è¦è®ºæ–‡
- è®°å½•æ‰€æœ‰æ‰¾åˆ°çš„è®ºæ–‡ï¼Œä½†æŒ‰ç›¸å…³æ€§æƒé‡æ’åº
- ç›¸å…³æ€§æƒé‡åº”åæ˜ è®ºæ–‡å¯¹ç”¨æˆ·é—®é¢˜çš„ç›´æ¥ç›¸å…³ç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰

å›å¤æ ¼å¼ï¼š
```json
{{
    "relevance_score": 0-10,
    "sufficient_info": true/false,
    "need_more_search": true/false,
    "suggested_query": "æ–°çš„å­¦æœ¯æŸ¥è¯¢è¯(if needed)",
    "query_source": "current_papers/author_focus/venue_focus/historical_keywords",
    "new_offset": 0,
    "limit": 10,
    "need_pagination": true/false,
    "pagination_reason": "åˆ†é¡µåŸå› è¯´æ˜(if needed)",
    "extracted_keywords": ["ä»å½“å‰è®ºæ–‡ä¸­è¯†åˆ«çš„å…³é”®æœ¯è¯­åˆ—è¡¨"],
    "key_papers": [
        {{
            "title": "è®ºæ–‡æ ‡é¢˜",
            "authors": "ä½œè€…åˆ—è¡¨", 
            "year": "å‘è¡¨å¹´ä»½",
            "venue": "æœŸåˆŠ/ä¼šè®®",
            "abstract": "æ‘˜è¦å†…å®¹",
            "citation_count": "å¼•ç”¨æ•°",
            "relevance_weight": 0.0-1.0,
            "key_findings": "å…³é”®å‘ç°æˆ–ç»“è®º",
            "urls": ["DOIé“¾æ¥", "å¼€æ”¾è®¿é—®PDFé“¾æ¥", "è®ºæ–‡URLç­‰"]
        }}
    ],
    "observation": "åŸºäºè®ºæ–‡å†…å®¹çš„è¯¦ç»†åˆ†æ"
}}
```"""

        observation = self._call_openai_api("", observation_prompt, json_mode=True)
        
        try:
            observation_data = json.loads(observation)
            
            # å¤„ç†æå–çš„å…³é”®è¯å†å²è®°å½•
            extracted_keywords = observation_data.get('extracted_keywords', [])
            if extracted_keywords:
                self.react_state['extracted_keywords_history'].update(extracted_keywords)
            
            # å¤„ç†å…³é”®è®ºæ–‡ä¿¡æ¯ï¼Œæ›´æ–°papers_collected
            key_papers = observation_data.get('key_papers', [])
            selected_papers = []
            added_count = 0
            
            if key_papers:
                # æŒ‰ç›¸å…³æ€§æƒé‡æ’åºï¼Œé€‰æ‹©å‰80%çš„è®ºæ–‡
                papers_with_weights = []
                for paper in key_papers:
                    relevance_weight = paper.get('relevance_weight', 0.8)  # é»˜è®¤æƒé‡0.8
                    papers_with_weights.append((paper, relevance_weight))
                
                # æŒ‰æƒé‡é™åºæ’åº
                papers_with_weights.sort(key=lambda x: x[1], reverse=True)
                
                # é€‰æ‹©å‰80%çš„è®ºæ–‡ï¼ˆè‡³å°‘5ç¯‡ï¼‰
                num_to_select = max(5, int(len(papers_with_weights) * 0.8))
                selected_papers = papers_with_weights[:num_to_select]
                
                # æ·»åŠ åˆ°æ”¶é›†åˆ—è¡¨ï¼Œé¿å…é‡å¤
                for paper, weight in selected_papers:
                    # ç®€å•çš„é‡å¤æ£€æŸ¥ï¼šåŸºäºæ ‡é¢˜
                    paper_title = paper.get('title', '').strip().lower()
                    if paper_title and not any(
                        existing_paper.get('title', '').strip().lower() == paper_title 
                        for existing_paper in self.react_state['papers_collected']
                    ):
                        self.react_state['papers_collected'].append(paper)
                        added_count += 1
            
            if stream_mode:
                obs_content = f"è§‚å¯Ÿåˆ†æï¼š{observation_data.get('observation', 'æ— ')}"
                
                if extracted_keywords:
                    obs_content += f"\næå–çš„å…³é”®è¯ï¼š{', '.join(extracted_keywords)}"
                
                if key_papers:
                    obs_content += f"\nå‘ç° {len(key_papers)} ç¯‡å…³é”®è®ºæ–‡"
                    obs_content += f"ï¼ŒæŒ‰ç›¸å…³æ€§é€‰æ‹©({len(selected_papers)}ç¯‡)ï¼Œå®é™…æ–°å¢({added_count}ç¯‡)å·²æ”¶å½•"
                
                query_source = observation_data.get('query_source', 'current_papers')
                source_desc = {
                    'author_focus': 'ä½œè€…èšç„¦æŸ¥è¯¢',
                    'venue_focus': 'æœŸåˆŠèšç„¦æŸ¥è¯¢',
                    'historical_keywords': 'å†å²å…³é”®è¯é‡ç”¨',
                    'current_papers': 'å½“å‰è®ºæ–‡æå–'
                }.get(query_source, 'å½“å‰è®ºæ–‡æå–')
                obs_content += f"\nå»ºè®®æŸ¥è¯¢è¯æ¥æºï¼š{source_desc}"
                
                # æ˜¾ç¤ºåˆ†é¡µä¿¡æ¯
                if observation_data.get('need_pagination', False):
                    new_offset = observation_data.get('new_offset', current_offset + current_limit)
                    limit = observation_data.get('limit', current_limit)
                    pagination_reason = observation_data.get('pagination_reason', 'éœ€è¦æ›´å¤šè®ºæ–‡')
                    obs_content += f"\nåˆ†é¡µå»ºè®®ï¼šåç§»é‡{new_offset} (é™åˆ¶{limit}ç¯‡) - {pagination_reason}"
                
                for chunk in self._emit_processing(obs_content, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("observation", observation_data)
        except json.JSONDecodeError:
            yield ("observation", {"sufficient_info": True, "need_more_search": False})

    async def _answer_generation_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ"""
        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        context = self._build_conversation_context(user_message, messages)
        papers_summary = self._summarize_collected_papers()
        
        # è·å–è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯
        total_papers_count = len(self.react_state['papers_collected'])
        
        final_prompt = f"""åŸºäºæ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜: {user_message}
å¯¹è¯å†å²: {context}

ğŸ“Š **æ£€ç´¢ç»Ÿè®¡**: é€šè¿‡Semantic Scholaræ£€ç´¢ï¼Œå…±æ”¶é›†åˆ° {total_papers_count} ç¯‡ç›¸å…³å­¦æœ¯è®ºæ–‡

æ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯:
{papers_summary}

**é‡è¦è¦æ±‚ï¼š**
1. **å……åˆ†åˆ©ç”¨æ‰€æœ‰æ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯** - ä¸è¦é—æ¼ä»»ä½•ç›¸å…³ç ”ç©¶
2. **è¯¦ç»†å¼•ç”¨è®ºæ–‡** - æ¯ä¸ªè§‚ç‚¹éƒ½è¦æ ‡æ³¨æ¥æºè®ºæ–‡çš„æ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½
3. **æ•´åˆå¤šç¯‡ç ”ç©¶** - ç»¼åˆåˆ†æä¸åŒç ”ç©¶çš„å‘ç°ï¼ŒæŒ‡å‡ºå…±è¯†å’Œåˆ†æ­§
4. **æä¾›å…·ä½“æ•°æ®** - å¼•ç”¨è®ºæ–‡ä¸­çš„å…·ä½“ç ”ç©¶æ•°æ®ã€ç»“æœã€ç»“è®ºã€å¼•ç”¨æ•°
5. **ç»“æ„åŒ–å›ç­”** - æŒ‰é€»è¾‘é¡ºåºç»„ç»‡å†…å®¹ï¼Œä¾¿äºç†è§£
6. **å®Œæ•´æ€§** - ç¡®ä¿å›ç­”æ¶µç›–ç”¨æˆ·é—®é¢˜çš„å„ä¸ªæ–¹é¢
7. **å­¦æœ¯æƒå¨æ€§** - ä¼˜å…ˆå¼•ç”¨é«˜å¼•ç”¨æ•°çš„é‡è¦è®ºæ–‡

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰è®ºæ–‡ä¿¡æ¯æä¾›å…¨é¢ã€è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚åŒ…å«ç›¸å…³è®ºæ–‡çš„å®Œæ•´å¼•ç”¨ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½ã€æœŸåˆŠã€å¼•ç”¨æ•°ç­‰ï¼‰ã€‚
å¦‚æœæœ‰DOIã€URLã€å¼€æ”¾è®¿é—®PDFé“¾æ¥ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨markdownæ ¼å¼è¾“å‡ºå¯ç‚¹å‡»é“¾æ¥(ä¸è¦é—æ¼æœ‰æ•ˆé“¾æ¥)ã€‚"""

        system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»”ç»†åˆ†ææ‰€æœ‰æä¾›çš„è®ºæ–‡ä¿¡æ¯
2. å……åˆ†åˆ©ç”¨æ¯ä¸€ç¯‡ç›¸å…³è®ºæ–‡çš„å†…å®¹
3. æä¾›å…¨é¢ã€è¯¦ç»†ã€æœ‰æ·±åº¦çš„å­¦æœ¯å›ç­”
4. ç¡®ä¿æ¯ä¸ªè§‚ç‚¹éƒ½æœ‰è®ºæ–‡æ”¯æ’‘å’Œå¼•ç”¨
5. æ•´åˆå¤šä¸ªç ”ç©¶æ¥æºï¼Œæä¾›ç»¼åˆæ€§è§è§£
6. ä¼˜å…ˆå¼•ç”¨é«˜å½±å“åŠ›ï¼ˆé«˜å¼•ç”¨æ•°ï¼‰çš„è®ºæ–‡
7. åœ¨å›ç­”å¼€å¤´ç®€è¦æåŠæ£€ç´¢åˆ°çš„è®ºæ–‡æ•°é‡ç»Ÿè®¡ï¼Œä½“ç°ç ”ç©¶çš„å…¨é¢æ€§"""

        if stream_mode:
            for chunk in self._stream_openai_response(final_prompt, system_prompt):
                chunk_data = {
                    'choices': [{
                        'delta': {'content': chunk},
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
            
            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯ï¼ˆæµå¼æ¨¡å¼ï¼‰
            stats_text = self._get_token_stats_text()
            stats_chunk_data = {
                'choices': [{
                    'delta': {'content': stats_text},
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(stats_chunk_data)}\n\n"
        else:
            answer = self._call_openai_api(system_prompt, final_prompt)
            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯ï¼ˆéæµå¼æ¨¡å¼ï¼‰
            stats_text = self._get_token_stats_text()
            yield answer + stats_text

    def _get_token_stats_text(self) -> str:
        """æ ¼å¼åŒ–tokenç»Ÿè®¡ä¿¡æ¯"""
        stats = self.token_stats
        stats_text = f"""

---

**ğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡**
- è¾“å…¥Tokenæ•°: {stats['input_tokens']:,} å­—ç¬¦
- è¾“å‡ºTokenæ•°: {stats['output_tokens']:,} å­—ç¬¦  
- æ€»Tokenæ•°: {stats['total_tokens']:,} å­—ç¬¦
- APIè°ƒç”¨æ¬¡æ•°: {stats['api_calls']} æ¬¡
- å¹³å‡æ¯æ¬¡è°ƒç”¨: {stats['total_tokens']//max(stats['api_calls'], 1):,} å­—ç¬¦

*æ³¨: Tokenæ•°é‡åŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼Œå®é™…ä½¿ç”¨é‡å¯èƒ½ç•¥æœ‰å·®å¼‚*
"""
        return stats_text

    def _build_conversation_context(self, user_message: str, messages: List[dict]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not messages or len(messages) <= 1:
            return "æ— å†å²å¯¹è¯"
        
        context_text = ""
        recent_messages = messages[-4:] if len(messages) > 4 else messages
        for msg in recent_messages:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "")
            if len(content) > 200:
                content = content[:200] + "..."
            context_text += f"{role}: {content}\n"
        
        return context_text

    def _summarize_collected_papers(self) -> str:
        """æ€»ç»“æ”¶é›†åˆ°çš„å…³é”®è®ºæ–‡ä¿¡æ¯"""
        if not self.react_state['papers_collected']:
            return "æœªæ”¶é›†åˆ°å…³é”®è®ºæ–‡ä¿¡æ¯"
        
        summary = f"æ”¶é›†åˆ° {len(self.react_state['papers_collected'])} ç¯‡å…³é”®è®ºæ–‡:\n\n"
        for i, paper in enumerate(self.react_state['papers_collected'], 1):
            summary += f"=== å…³é”®è®ºæ–‡ {i} ===\n"
            summary += f"æ ‡é¢˜: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
            summary += f"ä½œè€…: {paper.get('authors', 'æœªçŸ¥ä½œè€…')}\n"
            if paper.get('year'):
                summary += f"å¹´ä»½: {paper.get('year')}\n"
            if paper.get('venue'):
                summary += f"æœŸåˆŠ/ä¼šè®®: {paper.get('venue')}\n"
            if paper.get('citation_count'):
                summary += f"å¼•ç”¨æ•°: {paper.get('citation_count')}\n"
            summary += f"ç›¸å…³æ€§æƒé‡: {paper.get('relevance_weight', 1.0)}\n"
            if paper.get('key_findings'):
                summary += f"å…³é”®å‘ç°: {paper.get('key_findings')}\n"
            if paper.get('urls'):
                urls = paper.get('urls')
                if isinstance(urls, list) and urls:
                    summary += f"ç›¸å…³é“¾æ¥: {', '.join(urls)}\n"
            if paper.get('abstract'):
                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                abstract = paper.get('abstract')
                if len(abstract) > 500:
                    abstract = abstract[:500] + "..."
                summary += f"æ‘˜è¦: {abstract}\n"
            summary += "\n"
        
        return summary

    async def _react_loop(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ReActä¸»å¾ªç¯"""
        # é‡ç½®çŠ¶æ€å’Œtokenç»Ÿè®¡
        self.react_state = {
            "papers_collected": [],  # å­˜å‚¨å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            "query_history": [],
            "query_terms_used": set(),  # å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯é›†åˆ
            "extracted_keywords_history": set(),  # å†å²æå–çš„å…³é”®è¯é›†åˆ
            "current_iteration": 0,
            "current_offset": 0,  # å½“å‰åç§»é‡
            "current_limit": 10,  # å½“å‰æ¯é¡µæ•°é‡
            "query_offsets": {}  # è®°å½•æ¯ä¸ªæŸ¥è¯¢è¯ä½¿ç”¨çš„åç§»é‡ {query: offset}
        }
        
        # é‡ç½®tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0, 
            "output_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # ç¡®ä¿å·¥å…·å·²åŠ è½½
        try:
            async for tools_output in self._ensure_tools_loaded(stream_mode):
                yield tools_output
        except Exception as e:
            error_msg = f"âŒ Semantic Scholar MCPå·¥å…·åŠ è½½å¤±è´¥: {str(e)}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            return
        
        # 1. Reasoningé˜¶æ®µï¼ˆåªåœ¨å¼€å§‹æ‰§è¡Œä¸€æ¬¡ï¼‰
        initial_decision = None
        async for phase_result in self._reasoning_phase(user_message, messages, stream_mode):
            result_type, content = phase_result
            if result_type == "processing":
                yield content
            elif result_type == "decision":
                initial_decision = content
                break
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢
        if not initial_decision.get("need_search", False) or initial_decision.get("sufficient_info", False):
            # ç›´æ¥è¿›å…¥ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
            async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
                yield answer_chunk
            return
        
        # 2. Action-Observationå¾ªç¯
        max_iterations = self.valves.MAX_REACT_ITERATIONS
        current_query = initial_decision.get("query", "")
        
        while self.react_state['current_iteration'] < max_iterations and current_query:
            self.react_state['current_iteration'] += 1
            
            # Actioné˜¶æ®µ - è·å–å½“å‰æŸ¥è¯¢çš„åç§»é‡ä¿¡æ¯
            current_offset = self.react_state.get("current_offset", 0)
            current_limit = self.react_state.get("current_limit", 10)
            
            action_result = None
            async for phase_result in self._action_phase(current_query, current_limit, current_offset, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "result":
                    action_result = content
                    break
            
            # ç­‰å¾…actionå®Œå…¨æ‰§è¡Œå®Œæˆåå†è¿›è¡Œobservation
            if action_result is None:
                break
                
            # Observationé˜¶æ®µ
            observation = None
            async for phase_result in self._observation_phase(action_result, current_query, user_message, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "observation":
                    observation = content
                    break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†é¡µï¼ˆä¼˜å…ˆçº§é«˜äºæ–°æŸ¥è¯¢ï¼‰
            if observation and observation.get("need_pagination", False):
                # åˆ†é¡µï¼šä½¿ç”¨ç›¸åŒæŸ¥è¯¢è¯ï¼Œæ›´æ–°åç§»é‡
                new_offset = observation.get("new_offset", current_offset + current_limit)
                new_limit = observation.get("limit", current_limit)
                
                # æ›´æ–°åç§»é‡çŠ¶æ€
                self.react_state["current_offset"] = new_offset
                self.react_state["current_limit"] = new_limit
                
                # ç»§ç»­ä½¿ç”¨ç›¸åŒæŸ¥è¯¢è¯è¿›è¡Œä¸‹ä¸€è½®æœç´¢
                # current_query ä¿æŒä¸å˜
                continue
            
            # æ£€æŸ¥å·²æ”¶é›†è®ºæ–‡æ•°é‡ï¼Œå¦‚æœè¾¾åˆ°é˜ˆå€¼åˆ™å¼ºåˆ¶åœæ­¢
            collected_papers_count = len(self.react_state['papers_collected'])
            if collected_papers_count >= self.valves.MIN_PAPERS_THRESHOLD:
                if stream_mode:
                    stop_content = f"\nâœ… å·²æ”¶é›†è¶³å¤Ÿè®ºæ–‡({collected_papers_count}ç¯‡ >= {self.valves.MIN_PAPERS_THRESHOLD}ç¯‡é˜ˆå€¼)ï¼Œåœæ­¢æœç´¢"
                    for chunk in self._emit_processing(stop_content, "observation"):
                        yield f'data: {json.dumps(chunk)}\n\n'
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢
            if not observation or not observation.get("need_more_search", False) or observation.get("sufficient_info", False):
                break
            
            # è·å–ä¸‹ä¸€è½®æŸ¥è¯¢è¯ï¼ˆé‡ç½®åç§»é‡ä¸º0ï¼‰
            current_query = observation.get("suggested_query", "")
            self.react_state["current_offset"] = 0  # æ–°æŸ¥è¯¢è¯ä»åç§»é‡0å¼€å§‹
            
            # å¦‚æœå»ºè®®çš„æŸ¥è¯¢è¯å·²ç»ä½¿ç”¨è¿‡ï¼Œåˆ™åœæ­¢
            if current_query and current_query.lower() in self.react_state['query_terms_used']:
                break
        
        # 3. ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
        async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
            yield answer_chunk

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å­¦æœ¯è®ºæ–‡æœç´¢é—®é¢˜"
            return

        stream_mode = self.valves.ENABLE_STREAMING
        
        try:
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ReActå¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async_gen = self._react_loop(user_message, messages, stream_mode)
                
                while True:
                    try:
                        result = loop.run_until_complete(async_gen.__anext__())
                        yield result
                    except StopAsyncIteration:
                        break
                
                # æµå¼æ¨¡å¼ç»“æŸæ ‡è®°
                if stream_mode:
                    done_msg = {
                        'choices': [{
                            'delta': {},
                            'finish_reason': 'stop'
                        }]
                    }
                    yield f"data: {json.dumps(done_msg)}\n\n"
                    yield "data: [DONE]\n\n"
                    
            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if stream_mode:
                error_chunk = {
                    'choices': [{
                        'delta': {'content': error_msg},
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield error_msg
