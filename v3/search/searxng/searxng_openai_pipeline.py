"""
title: SearxNG OpenAI Pipeline
author: open-webui
date: 2024-12-20
version: 2.0
license: MIT
description: åŸºäºSearxNG APIçš„è”ç½‘æœç´¢pipelineï¼Œç®€åŒ–ç‰ˆæœ¬
requirements: requests, pydantic, aiohttp, beautifulsoup4, asyncio

é˜¶æ®µè¯´æ˜ï¼š
1. é˜¶æ®µ1: é—®é¢˜ä¼˜åŒ– - æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå½“å‰é—®é¢˜è¾“å‡ºä¼˜åŒ–åçš„é—®é¢˜
2. é˜¶æ®µ2: ç½‘ç»œæœç´¢ - ä½¿ç”¨SearxNGè¿›è¡Œæœç´¢å¹¶ç­›é€‰æœ€ç›¸å…³çš„ç»“æœ
3. é˜¶æ®µ3: å†…å®¹è·å– - å¹¶å‘è·å–é€‰ä¸­ç½‘é¡µçš„å†…å®¹
4. é˜¶æ®µ4: ç”Ÿæˆæœ€ç»ˆå›ç­” - åŸºäºè·å–çš„å†…å®¹ç”Ÿæˆå‡†ç¡®çš„å›ç­”
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
import re
from typing import List, Union, Generator, Iterator, Dict, Any
from pydantic import BaseModel
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

# åç«¯é˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "query_optimization": "é—®é¢˜ä¼˜åŒ–",
    "web_search": "ç½‘ç»œæœç´¢",
    "content_fetch": "å†…å®¹è·å–",
    "final_answer": "ç”Ÿæˆæœ€ç»ˆå›ç­”",
}

STAGE_GROUP = {
    "query_optimization": "stage_group_1",
    "web_search": "stage_group_2",
    "content_fetch": "stage_group_3",
    "final_answer": "stage_group_4",
}

class Pipeline:
    class Valves(BaseModel):
        # SearxNG APIé…ç½®
        SEARXNG_URL: str
        SEARXNG_SEARCH_COUNT: int
        SEARXNG_LANGUAGE: str
        SEARXNG_TIMEOUT: int
        SEARXNG_CATEGORIES: str
        SEARXNG_TIME_RANGE: str
        SEARXNG_SAFESEARCH: int
        
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        SELECTED_URLS_COUNT: int
        CONTENT_FETCH_TIMEOUT: int
        MAX_CONTENT_LENGTH: int
        
        # å†å²ä¼šè¯é…ç½®
        HISTORY_TURNS: int

    def __init__(self):
        self.name = "SearxNG Search OpenAI Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        self.valves = self.Valves(
            **{
                # SearxNGæœç´¢é…ç½®
                "SEARXNG_URL": os.getenv("SEARXNG_URL", "http://117.50.252.245:8081"),
                "SEARXNG_SEARCH_COUNT": int(os.getenv("SEARXNG_SEARCH_COUNT", "20")),
                "SEARXNG_LANGUAGE": os.getenv("SEARXNG_LANGUAGE", "zh-CN"),
                "SEARXNG_TIMEOUT": int(os.getenv("SEARXNG_TIMEOUT", "15")),
                "SEARXNG_CATEGORIES": os.getenv("SEARXNG_CATEGORIES", "general"),
                "SEARXNG_TIME_RANGE": os.getenv("SEARXNG_TIME_RANGE", ""),
                "SEARXNG_SAFESEARCH": int(os.getenv("SEARXNG_SAFESEARCH", "0")),
                
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "google/gemini-2.5-flash"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "SELECTED_URLS_COUNT": int(os.getenv("SELECTED_URLS_COUNT", "10")),
                "CONTENT_FETCH_TIMEOUT": int(os.getenv("CONTENT_FETCH_TIMEOUT", "10")),
                "MAX_CONTENT_LENGTH": int(os.getenv("MAX_CONTENT_LENGTH", "5000")),
                
                # å†å²ä¼šè¯é…ç½®
                "HISTORY_TURNS": int(os.getenv("HISTORY_TURNS", "3")),
            }
        )

    async def on_startup(self):
        print(f"SearxNG Search OpenAI Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            
        # æµ‹è¯•SearxNG APIè¿æ¥
        try:
            print("ğŸ”§ å¼€å§‹æµ‹è¯•SearxNG APIè¿æ¥...")
            test_response = self._search_searxng("test query")
            if test_response and "results" in test_response:
                print("âœ… SearxNGæœç´¢APIè¿æ¥æˆåŠŸ")
            else:
                print("âš ï¸ SearxNGæœç´¢APIæµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ SearxNGæœç´¢APIè¿æ¥å¤±è´¥: {e}")

    async def on_shutdown(self):
        print(f"SearxNG Search OpenAI Pipelineå…³é—­: {__name__}")

    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_input_tokens(self, text: str):
        """æ·»åŠ è¾“å…¥tokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["input_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _add_output_tokens(self, text: str):
        """æ·»åŠ è¾“å‡ºtokenç»Ÿè®¡"""
        tokens = self._estimate_tokens(text)
        self.token_stats["output_tokens"] += tokens
        self.token_stats["total_tokens"] += tokens

    def _reset_token_stats(self):
        """é‡ç½®tokenç»Ÿè®¡"""
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    def _get_token_stats(self) -> dict:
        """è·å–tokenç»Ÿè®¡ä¿¡æ¯"""
        return self.token_stats.copy()

    def _identify_website_type(self, url: str) -> str:
        """è¯†åˆ«ç½‘ç«™ç±»å‹"""
        url_lower = url.lower()
        
        if 'wikipedia' in url_lower:
            return 'wiki'
        elif 'baike.baidu' in url_lower:
            return 'ç™¾åº¦ç™¾ç§‘'
        elif 'wiki.mbalib' in url_lower:
            return 'MBAæ™ºåº“ç™¾ç§‘'
        elif any(keyword in url_lower for keyword in ['arxiv', 'doi', '.pdf']):
            return 'è®ºæ–‡'
        elif any(keyword in url_lower for keyword in ['blog', 'csdn', 'cnblogs', 'jianshu', 'zhihu', 'segmentfault']):
            return 'åšå®¢'
        elif any(keyword in url_lower for keyword in ['github', 'stackoverflow', 'medium']):
            return 'æŠ€æœ¯æ–‡æ¡£'
        else:
            return 'å…¶ä»–'

    def _search_searxng(self, query: str) -> dict:
        """è°ƒç”¨SearxNG APIè¿›è¡Œæœç´¢"""
        url = f"{self.valves.SEARXNG_URL}/search"
        
        params = {
            'q': query.strip(),
            'format': 'json',
            'language': self.valves.SEARXNG_LANGUAGE,
            'safesearch': self.valves.SEARXNG_SAFESEARCH,
            'categories': self.valves.SEARXNG_CATEGORIES
        }
        
        if self.valves.SEARXNG_TIME_RANGE:
            params['time_range'] = self.valves.SEARXNG_TIME_RANGE
        
        try:
            if self.valves.DEBUG_MODE:
                print(f"ğŸ” SearxNGæœç´¢: {query}")
            
            response = requests.get(
                url,
                params=params,
                timeout=self.valves.SEARXNG_TIMEOUT
            )
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            if self.valves.DEBUG_MODE:
                print(f"âŒ SearxNGæœç´¢é”™è¯¯: {str(e)}")
            return {"error": str(e)}

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        # æ·»åŠ è¾“å…¥tokenç»Ÿè®¡
        if system_prompt and system_prompt.strip():
            self._add_input_tokens(system_prompt)
        self._add_input_tokens(user_prompt)
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            answer = result["choices"][0]["message"]["content"]
            self._add_output_tokens(answer)
            
            return answer
            
        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            return error_msg

    def _stage1_optimize_query(self, user_message: str, messages: List[dict]) -> str:
        """é˜¶æ®µ1: é—®é¢˜ä¼˜åŒ–"""
        # æå–å†å²ä¸Šä¸‹æ–‡
        context_text = ""
        if messages and len(messages) > 1:
            recent_messages = messages[-self.valves.HISTORY_TURNS*2:] if len(messages) > self.valves.HISTORY_TURNS*2 else messages
            for msg in recent_messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    context_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    context_text += f"åŠ©æ‰‹: {content}\n"
        
        system_prompt = ""
        
        user_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·çš„å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼Œä¼˜åŒ–æœç´¢æŸ¥è¯¢ä»¥è·å¾—æ›´å¥½çš„æœç´¢ç»“æœã€‚

ä¼˜åŒ–åŸåˆ™ï¼š
1. æå–æ ¸å¿ƒå…³é”®è¯
2. å»é™¤å†—ä½™è¯æ±‡  
3. ä¿ç•™é‡è¦é™å®šè¯
4. ç»“åˆå†å²ä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çœŸå®æ„å›¾
5. ç¡®ä¿æŸ¥è¯¢ç®€æ´ä¸”ç²¾å‡†

å†å²å¯¹è¯ä¸Šä¸‹æ–‡:
{context_text if context_text else "æ— å†å²å¯¹è¯"}

å½“å‰ç”¨æˆ·é—®é¢˜: {user_message}

è¯·ä¼˜åŒ–è¿™ä¸ªé—®é¢˜ä»¥è·å¾—æ›´å¥½çš„æœç´¢ç»“æœï¼Œç›´æ¥è¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢è¯ï¼Œä¸éœ€è¦è§£é‡Šã€‚"""

        response = self._call_openai_api(system_prompt, user_prompt)
        
        # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹é—®é¢˜
        if "é”™è¯¯" in response or not response.strip():
            return user_message
        
        return response.strip()

    def _stage2_search_and_select(self, optimized_query: str) -> List[dict]:
        """é˜¶æ®µ2: æœç´¢å¹¶é€‰æ‹©æœ€ä½³ç»“æœ"""
        search_results = self._search_searxng(optimized_query)
        
        if "error" in search_results or not search_results.get("results"):
            return []
        
        # å¤„ç†æœç´¢ç»“æœ
        all_results = []
        for result in search_results.get("results", []):
            result_info = {
                "title": result.get("title", "").strip(),
                "link": result.get("url", "").strip(),
                "snippet": result.get("content", "").strip(),
                "website_type": self._identify_website_type(result.get("url", "")),
            }
            if result_info["title"] and result_info["link"]:
                all_results.append(result_info)
        
        if not all_results:
            return []
        
        # å¦‚æœç»“æœè¾ƒå°‘ï¼Œç›´æ¥è¿”å›
        if len(all_results) <= self.valves.SELECTED_URLS_COUNT:
            return all_results
        
        # ä½¿ç”¨LLMé€‰æ‹©æœ€ä½³ç»“æœ
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯ç­›é€‰ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æœç´¢ç»“æœä¸­é€‰æ‹©ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ç½‘ç«™ã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
1. ç›¸å…³æ€§æ˜¯æœ€é‡è¦çš„æ ‡å‡†
2. ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
3. è¯„ä¼°æ¯ä¸ªæœç´¢ç»“æœçš„æ ‡é¢˜å’Œæ‘˜è¦æ˜¯å¦ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
4. ä¼˜å…ˆé€‰æ‹©å†…å®¹ç›¸å…³åº¦é«˜çš„ç»“æœ"""
        
        results_text = ""
        for i, result in enumerate(all_results):
            results_text += f"[{i}] æ ‡é¢˜: {result['title']}\n"
            results_text += f"    é“¾æ¥: {result['link']}\n"
            results_text += f"    æ‘˜è¦: {result['snippet']}\n"
            results_text += f"    ç½‘ç«™ç±»å‹: {result['website_type']}\n\n"

        user_prompt = f"""ç”¨æˆ·é—®é¢˜: {optimized_query}

è¯·ä»ä»¥ä¸‹æœç´¢ç»“æœä¸­é€‰æ‹©{self.valves.SELECTED_URLS_COUNT}ä¸ªä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ç»“æœã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. æ ‡é¢˜å’Œæ‘˜è¦æ˜¯å¦ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆæœ€é‡è¦ï¼‰
2. å†…å®¹æ˜¯å¦åŒ…å«é—®é¢˜çš„å…³é”®è¯å’Œæ¦‚å¿µ

æœç´¢ç»“æœ:
{results_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›æœ€ç›¸å…³çš„{self.valves.SELECTED_URLS_COUNT}ä¸ªç»“æœçš„ç´¢å¼•ï¼š
{{
    "selected_indices": [0, 1, 2, ...]
}}

æ³¨æ„ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ç›¸å…³æ€§é€‰æ‹©ï¼Œä¸è¦è¢«ç½‘ç«™ç±»å‹å½±å“ã€‚"""

        response = self._call_openai_api(system_prompt, user_prompt, json_mode=True)
        
        try:
            selection = json.loads(response)
            selected_indices = selection.get("selected_indices", [])
            selected_results = [all_results[i] for i in selected_indices if 0 <= i < len(all_results)]
            return selected_results[:self.valves.SELECTED_URLS_COUNT]
        except:
            # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œè¿”å›å‰Nä¸ªç»“æœ
            return all_results[:self.valves.SELECTED_URLS_COUNT]

    async def _fetch_url_content(self, session: aiohttp.ClientSession, url: str) -> dict:
        """å¼‚æ­¥è·å–ç½‘é¡µå†…å®¹"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=self.valves.CONTENT_FETCH_TIMEOUT)) as response:
                if response.status == 200:
                    html_content = await response.text()
                    # ä½¿ç”¨BeautifulSoupæå–æ–‡æœ¬å†…å®¹
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–ä¸»è¦æ–‡æœ¬å†…å®¹
                    text_content = soup.get_text()
                    # æ¸…ç†æ–‡æœ¬
                    lines = (line.strip() for line in text_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    if len(text_content) > self.valves.MAX_CONTENT_LENGTH:
                        text_content = text_content[:self.valves.MAX_CONTENT_LENGTH] + "..."
                    
                    return {
                        "url": url,
                        "status": "success",
                        "content": text_content,
                        "title": soup.title.string if soup.title else ""
                    }
                else:
                    return {
                        "url": url,
                        "status": "error",
                        "content": f"HTTP {response.status}",
                        "title": ""
                    }
        except Exception as e:
            return {
                "url": url,
                "status": "error", 
                "content": str(e),
                "title": ""
            }

    async def _stage3_fetch_content(self, selected_results: List[dict]) -> List[dict]:
        """é˜¶æ®µ3: å¹¶å‘è·å–ç½‘é¡µå†…å®¹"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for result in selected_results:
                task = self._fetch_url_content(session, result["link"])
                tasks.append(task)
            
            contents = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å†…å®¹å¹¶ä¸åŸç»“æœåˆå¹¶
            enriched_results = []
            for i, content in enumerate(contents):
                if isinstance(content, dict):
                    enriched_result = selected_results[i].copy()
                    enriched_result.update(content)
                    enriched_results.append(enriched_result)
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    enriched_result = selected_results[i].copy()
                    enriched_result.update({
                        "status": "error",
                        "content": str(content),
                        "title": ""
                    })
                    enriched_results.append(enriched_result)
            
            return enriched_results

    def _stage4_generate_final_answer(self, user_message: str, enriched_results: List[dict], stream: bool = False) -> Union[str, Generator]:
        """é˜¶æ®µ4: ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        # æ„å»ºä¿¡æ¯æºæ–‡æœ¬å’Œé“¾æ¥åˆ—è¡¨
        source_content = ""
        all_sources = []
        successful_sources = []
        
        for i, result in enumerate(enriched_results, 1):
            # è®°å½•æ‰€æœ‰æ¥æºä¿¡æ¯
            source_info = {
                "index": i,
                "title": result.get('title', result.get('title', 'æœªçŸ¥æ ‡é¢˜')),
                "link": result['link'],
                "website_type": result['website_type'],
                "status": result.get("status", "unknown")
            }
            all_sources.append(source_info)
            
            if result.get("status") == "success" and result.get("content"):
                source_content += f"[æ¥æº{i}] {result.get('title', result['title'])}\n"
                source_content += f"é“¾æ¥: {result['link']}\n"
                source_content += f"ç½‘ç«™ç±»å‹: {result['website_type']}\n"
                source_content += f"å†…å®¹æ‘˜è¦: {result['snippet']}\n"
                source_content += f"ä¸»è¦å†…å®¹: {result['content']}\n\n"
                successful_sources.append(i)
            else:
                # å³ä½¿è·å–å¤±è´¥ï¼Œä¹Ÿæ·»åŠ åŸºæœ¬ä¿¡æ¯
                source_content += f"[æ¥æº{i}] {result.get('title', result['title'])}\n"
                source_content += f"é“¾æ¥: {result['link']}\n"
                source_content += f"ç½‘ç«™ç±»å‹: {result['website_type']}\n"
                source_content += f"å†…å®¹æ‘˜è¦: {result.get('snippet', 'æ— æ‘˜è¦')}\n"
                source_content += f"çŠ¶æ€: å†…å®¹è·å–å¤±è´¥ - {result.get('content', 'æœªçŸ¥é”™è¯¯')}\n\n"
        
        if not successful_sources:
            return "æŠ±æ­‰ï¼Œæ‰€æœ‰ç½‘é¡µå†…å®¹è·å–éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æä¾›åŸºäºç½‘é¡µå†…å®¹çš„å›ç­”ã€‚"
        
        # æ„å»ºæ‰€æœ‰é“¾æ¥çš„markdownæ ¼å¼
        all_links_md = ""
        for source in all_sources:
            status_indicator = "âœ…" if source["status"] == "success" else "âŒ"
            all_links_md += f"{status_indicator} [{source['title']}]({source['link']}) ({source['website_type']})\n"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼åŸºäºç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„ä¿¡æ¯æºæ¥ç”Ÿæˆå›ç­”ã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
1. ç”¨æˆ·é—®é¢˜æ˜¯å”¯ä¸€çš„å¯¼å‘ - ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·é—®é¢˜çš„è¦æ±‚å›ç­”
2. ä¸è¦è¢«ä¿¡æ¯æºçš„å†…å®¹å¸¦å - åªæå–ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯
3. å¦‚æœä¿¡æ¯æºåŒ…å«å¤§é‡æ— å…³å†…å®¹ï¼Œè¦æœ‰åˆ¤æ–­èƒ½åŠ›ç­›é€‰å‡ºç›¸å…³éƒ¨åˆ†
4. å¦‚æœä¿¡æ¯æºä¸è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¯šå®è¯´æ˜
5. ä¸è¦ä¸ºäº†ä½¿ç”¨æ‰€æœ‰ä¿¡æ¯æºè€Œå¼ºè¡Œå †ç Œæ— å…³å†…å®¹

å›ç­”è´¨é‡æ ‡å‡†ï¼š
- ç²¾å‡†æ€§ï¼šå›ç­”å¿…é¡»ç›´æ¥é’ˆå¯¹ç”¨æˆ·é—®é¢˜
- ç›¸å…³æ€§ï¼šåªåŒ…å«ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯
- å®Œæ•´æ€§ï¼šåœ¨ç›¸å…³èŒƒå›´å†…å°½å¯èƒ½å®Œæ•´å›ç­”
- è¯šå®æ€§ï¼šä¸ç¼–é€ ä¿¡æ¯ï¼Œä¸ç¡®å®šæ—¶è¯´æ˜"""

        user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_message}

è¯·ä¸¥æ ¼åŸºäºç”¨æˆ·é—®é¢˜å’Œä»¥ä¸‹ä¿¡æ¯æºç”Ÿæˆå›ç­”ã€‚

å…³é”®è¦æ±‚ï¼š
1. ç´§æ‰£ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦åç¦»ä¸»é¢˜
2. ä»ä¿¡æ¯æºä¸­ç­›é€‰å‡ºä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„å†…å®¹
3. å¿½ç•¥ä¿¡æ¯æºä¸­ä¸é—®é¢˜æ— å…³çš„å†…å®¹ï¼ˆå¦‚å¹¿å‘Šã€å¯¼èˆªã€æ— å…³æ®µè½ç­‰ï¼‰
4. åªä½¿ç”¨èƒ½å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜çš„ä¿¡æ¯æºï¼Œä¸è¦å¼ºè¡Œä½¿ç”¨æ‰€æœ‰æº
5. å¦‚æœæŸä¸ªä¿¡æ¯æºä¸é—®é¢˜æ— å…³ï¼Œå¯ä»¥å¿½ç•¥å®ƒ
6. åœ¨å›ç­”ä¸­ä½¿ç”¨markdowné“¾æ¥æ ¼å¼å¼•ç”¨ç›¸å…³æ¥æºï¼š[æ ‡é¢˜](é“¾æ¥)

æ‰€æœ‰å¯ç”¨æ¥æºé“¾æ¥ï¼š
{all_links_md}

åˆ¤æ–­æ ‡å‡†ï¼š
- è¿™ä¸ªä¿¡æ¯æ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
- è¿™ä¸ªä¿¡æ¯æ˜¯å¦å¯¹ç†è§£ç­”æ¡ˆæœ‰å¸®åŠ©ï¼Ÿ
- å¦‚æœç­”æ¡ˆæ˜¯"å¦"ï¼Œå°±ä¸è¦åŒ…å«è¿™ä¸ªä¿¡æ¯

ä¿¡æ¯æºè¯¦æƒ…:
{source_content}

è¯·ä¸¥æ ¼åŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆå›ç­”ï¼Œè¦æ±‚ï¼š
1. åªå›ç­”ä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„å†…å®¹
2. å¯¹ç›¸å…³ä¿¡æ¯æºä½¿ç”¨markdowné“¾æ¥æ ¼å¼å¼•ç”¨,[æ ‡é¢˜](é“¾æ¥)
3. æœ«å°¾åˆ—å‡ºå®é™…ä½¿ç”¨çš„å‚è€ƒæ¥æº
4. ä¸è¦ä¸ºäº†å‡‘å­—æ•°è€ŒåŒ…å«æ— å…³ä¿¡æ¯"""

        if stream:
            return self._stream_openai_response(user_prompt, system_prompt)
        else:
            return self._call_openai_api(system_prompt, user_prompt)

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=self.valves.OPENAI_TIMEOUT
            )
            response.raise_for_status()
            
            collected_content = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                collected_content += delta
                                self._add_output_tokens(delta)
                                yield delta
                        except json.JSONDecodeError:
                            pass
                            
        except Exception as e:
            error_msg = f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        # é‡ç½®tokenç»Ÿè®¡
        self._reset_token_stats()

        if self.valves.DEBUG_MODE:
            print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print(f"ğŸ”§ æ¨¡å‹ID: {model_id}")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

        # éªŒè¯è¾“å…¥
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŸ¥è¯¢å†…å®¹"
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼æ¨¡å¼  
        stream_mode = body.get("stream", False) and self.valves.ENABLE_STREAMING
        
        try:
            # é˜¶æ®µ1: é—®é¢˜ä¼˜åŒ–
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨ä¼˜åŒ–æœç´¢é—®é¢˜...", "query_optimization"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ”„ **é˜¶æ®µ1**: æ­£åœ¨ä¼˜åŒ–æœç´¢é—®é¢˜..."
            
            optimized_query = self._stage1_optimize_query(user_message, messages)
            
            if stream_mode:
                opt_info = f"âœ… é—®é¢˜ä¼˜åŒ–å®Œæˆ\nä¼˜åŒ–åçš„é—®é¢˜: {optimized_query}"
                for chunk in self._emit_processing(opt_info, "query_optimization"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield f"âœ… ä¼˜åŒ–åçš„é—®é¢˜: {optimized_query}\n"

            # é˜¶æ®µ2: æœç´¢å’Œé€‰æ‹©
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢å’Œç»“æœç­›é€‰...", "web_search"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ” **é˜¶æ®µ2**: æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢å’Œç»“æœç­›é€‰..."
            
            selected_results = self._stage2_search_and_select(optimized_query)
            
            if not selected_results:
                yield "âŒ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯"
                return
            
            # å±•ç¤ºé€‰ä¸­çš„ä¿¡æ¯æº
            source_info = f"âœ… å·²é€‰æ‹©{len(selected_results)}ä¸ªä¿¡æ¯æº:\n"
            for i, result in enumerate(selected_results, 1):
                source_info += f"[{i}] {result['title']} ({result['website_type']})\n    {result['link']}\n"
            
            if stream_mode:
                for chunk in self._emit_processing(source_info, "web_search"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield source_info

            # é˜¶æ®µ3: è·å–ç½‘é¡µå†…å®¹
            if stream_mode:
                for chunk in self._emit_processing("æ­£åœ¨è·å–ç½‘é¡µå†…å®¹...", "content_fetch"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield "ğŸ“„ **é˜¶æ®µ3**: æ­£åœ¨è·å–ç½‘é¡µå†…å®¹..."
            
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                enriched_results = loop.run_until_complete(self._stage3_fetch_content(selected_results))
            finally:
                loop.close()
            
            # ç»Ÿè®¡æˆåŠŸè·å–çš„å†…å®¹
            successful_count = sum(1 for r in enriched_results if r.get("status") == "success")
            
            content_info = f"âœ… å†…å®¹è·å–å®Œæˆï¼ŒæˆåŠŸè·å–{successful_count}/{len(enriched_results)}ä¸ªç½‘é¡µå†…å®¹"
            
            if stream_mode:
                for chunk in self._emit_processing(content_info, "content_fetch"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield content_info

            # é˜¶æ®µ4: ç”Ÿæˆæœ€ç»ˆå›ç­”
            if stream_mode:
                # æµå¼æ¨¡å¼å¼€å§‹ç”Ÿæˆå›ç­”çš„æ ‡è¯†
                answer_start_msg = {
                    'choices': [{
                        'delta': {
                            'content': "\n**ğŸ’­ ç”Ÿæˆæœ€ç»ˆå›ç­”**\n"
                        },
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(answer_start_msg)}\n\n"
            else:
                yield "ğŸ¤– **é˜¶æ®µ4**: æ­£åœ¨åŸºäºè·å–çš„å†…å®¹ç”Ÿæˆå›ç­”..."

            # ç”Ÿæˆæœ€ç»ˆå›ç­”
            if stream_mode:
                for chunk in self._stage4_generate_final_answer(user_message, enriched_results, stream=True):
                    yield chunk
                # æµå¼æ¨¡å¼ç»“æŸåæ·»åŠ tokenç»Ÿè®¡
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"
            else:
                result = self._stage4_generate_final_answer(user_message, enriched_results, stream=False)
                yield result
                # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
                token_info = self._get_token_stats()
                yield f"\n\n---\nğŸ“Š **Tokenç»Ÿè®¡**: è¾“å…¥ {token_info['input_tokens']}, è¾“å‡º {token_info['output_tokens']}, æ€»è®¡ {token_info['total_tokens']}"

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if self.valves.DEBUG_MODE:
                print(f"âŒ {error_msg}")
            yield error_msg