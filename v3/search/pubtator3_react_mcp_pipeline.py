"""
å‚è€ƒv2\search\pubtator3_mcp_pipeline.pyç¼–å†™æœ¬pipeline
ä½¿ç”¨ReActæ¨¡å¼ï¼Œå®ç°æ·±åº¦æ€è€ƒï¼Œå¹¶è¿›è¡Œå¤šè½®pubtator3 MCPå·¥å…·è°ƒç”¨ï¼Œæœ€ç»ˆç»™å‡ºå›ç­”ã€‚
1.Reasoningé˜¶æ®µï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€å†å²ä¼šè¯ã€å½“å‰è·å–åˆ°çš„è®ºæ–‡ä¿¡æ¯è¿›è¡Œè‡ªä¸»æ€è€ƒåˆ¤æ–­
  - åˆ¶å®šåˆæ¬¡å·¥å…·è°ƒç”¨çš„Action, è°ƒç”¨pubtator3å·¥å…·ï¼Œè·å–è®ºæ–‡ä¿¡æ¯
2.Actioné˜¶æ®µï¼Œè°ƒç”¨MCPå·¥å…·ï¼Œè·å–ä¿¡æ¯
3.Observationé˜¶æ®µ(æ¯æ¬¡Actionåéƒ½éœ€è¦è¿›è¡Œè§‚å¯Ÿ)
  - æ ¹æ®Actionçš„æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜(é—®é¢˜çš„ç›¸å…³æ€§ï¼Œè¿›ä¸€æ­¥æ¢ç´¢çš„å¿…è¦æ€§)
  - å¦‚æœä¿¡æ¯ä¸å……åˆ†ï¼Œåˆ™åˆ¶å®šæ–°çš„Action(æ›´æ–°æŸ¥è¯¢å…³é”®è¯ã€ä½¿ç”¨å‰ä¸€æ­¥æ£€ç´¢åˆ°çš„ä¿¡æ¯æ›´æ–°æŸ¥è¯¢å…³é”®è¯)ï¼Œè°ƒç”¨pubtator3å·¥å…·ï¼Œè·å–è®ºæ–‡ä¿¡æ¯
  - å¦‚æœä¿¡æ¯å……åˆ†ï¼Œåˆ™è·³è½¬ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
4.ç­”æ¡ˆç”Ÿæˆé˜¶æ®µï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€å†å²ä¼šè¯ã€å½“å‰è·å–åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶è¿”å›ç»™ç”¨æˆ·
5.é™¤äº†ç­”æ¡ˆç”Ÿæˆé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨_emit_processingæ–¹æ³•ï¼Œè¿”å›å¤„ç†è¿‡ç¨‹å†…å®¹å’Œæ€è€ƒï¼Œå‡å°‘debugæè¿°å†…å®¹çš„è¾“å‡º
6.å¯¹äºActionå’ŒObservationé˜¶æ®µï¼Œ_emit_processingé‡‡ç”¨åŠ¨æ€é€’è¿›çš„processing_stage
7.æ³¨æ„ä»£ç çš„æ•´æ´ç®€ç»ƒï¼Œå‡½æ•°çš„è§£è€¦ï¼Œé¿å…é‡å¤ä»£ç å’Œè¿‡å¤šdebugè¾“å‡º
"""

import os
import json
import requests
import asyncio
import aiohttp
import time
from typing import List, Union, Generator, Iterator, Dict, Any, Optional, AsyncGenerator
from pydantic import BaseModel
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ReActé˜¶æ®µæ ‡é¢˜æ˜ å°„
STAGE_TITLES = {
    "reasoning": "ğŸ¤” æ¨ç†åˆ†æ",
    "action": "ğŸ”§ æ‰§è¡ŒåŠ¨ä½œ", 
    "observation": "ğŸ‘ï¸ è§‚å¯Ÿç»“æœ",
    "answer_generation": "ğŸ“ ç”Ÿæˆç­”æ¡ˆ",
    "mcp_discovery": "ğŸ” MCPæœåŠ¡å‘ç°"
}

STAGE_GROUP = {
    "reasoning": "stage_group_1",
    "action": "stage_group_2",
    "observation": "stage_group_3", 
    "answer_generation": "stage_group_4",
    "mcp_discovery": "stage_group_0"
}

class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float
        
        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        MAX_REACT_ITERATIONS: int
        MIN_PAPERS_THRESHOLD: int
        
        # MCPé…ç½®
        MCP_SERVER_URL: str
        MCP_TIMEOUT: int
        MCP_TOOLS_EXPIRE_HOURS: int

    def __init__(self):
        self.name = "PubTator3 ReAct MCP Academic Paper Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0, 
            "output_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # MCPå·¥å…·ç¼“å­˜
        self.mcp_tools = {}
        self.tools_loaded = False
        self.tools_loaded_time = None
        self.session_id = None
        
        # ReActçŠ¶æ€
        self.react_state = {
            "papers_collected": [],  # å­˜å‚¨å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            "query_history": [],
            "query_terms_used": set(),  # å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯é›†åˆ
            "extracted_keywords_history": set(),  # å†å²æå–çš„å…³é”®è¯é›†åˆ
            "current_iteration": 0,
            "current_page": 1,  # å½“å‰é¡µç 
            "current_page_size": 10,  # å½“å‰é¡µå¤§å°
            "query_pages": {}  # è®°å½•æ¯ä¸ªæŸ¥è¯¢è¯ä½¿ç”¨çš„é¡µç  {query: page}
        }
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "MAX_REACT_ITERATIONS": int(os.getenv("MAX_REACT_ITERATIONS", "5")),
                "MIN_PAPERS_THRESHOLD": int(os.getenv("MIN_PAPERS_THRESHOLD", "10")),
                
                # MCPé…ç½®
                "MCP_SERVER_URL": os.getenv("MCP_SERVER_URL", "http://localhost:8991"),
                "MCP_TIMEOUT": int(os.getenv("MCP_TIMEOUT", "30")),
                "MCP_TOOLS_EXPIRE_HOURS": int(os.getenv("MCP_TOOLS_EXPIRE_HOURS", "12")),
            }
        )

    async def on_startup(self):
        print(f"PubTator3 ReAct MCP Pipelineå¯åŠ¨: {__name__}")
        if not self.valves.OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥")
        print(f"ğŸ”— MCPæœåŠ¡å™¨: {self.valves.MCP_SERVER_URL}")

    async def on_shutdown(self):
        print(f"PubTator3 ReAct MCP Pipelineå…³é—­: {__name__}")

    def _emit_processing(self, content: str, stage: str = "processing") -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + '\n',
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    async def _initialize_mcp_session(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """åˆå§‹åŒ–MCPä¼šè¯å¹¶è·å–æœåŠ¡å™¨åˆ†é…çš„session ID"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        try:
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # Step 1: å‘é€initializeè¯·æ±‚ï¼ˆä¸å¸¦session IDï¼‰
            initialize_request = {
                "jsonrpc": "2.0",
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {
                        "sampling": {},
                        "roots": {"listChanged": True}
                    },
                    "clientInfo": {
                        "name": "PubTator3 ReAct MCP Pipeline",
                        "version": "1.0.0"
                    }
                },
                "id": "init-1"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=initialize_request,
                    headers={
                        "Content-Type": "application/json",
                        "Accept": "application/json, text/event-stream"
                    },
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # æ£€æŸ¥å“åº”å¤´ä¸­çš„session ID
                        server_session_id = response.headers.get("Mcp-Session-Id")
                        if server_session_id:
                            self.session_id = server_session_id
                        
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            init_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "init-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            init_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            init_response = await response.json()
                        
                        if not init_response:
                            raise Exception("No initialize response received")
                        
                        if "error" in init_response:
                            raise Exception(f"MCP initialize error: {init_response['error']}")
                        
                        # Step 2: å‘é€initializedé€šçŸ¥
                        initialized_notification = {
                            "jsonrpc": "2.0",
                            "method": "notifications/initialized"
                        }
                        
                        headers = {
                            "Content-Type": "application/json",
                            "Accept": "application/json, text/event-stream"
                        }
                        if hasattr(self, 'session_id') and self.session_id:
                            headers["Mcp-Session-Id"] = self.session_id
                        
                        async with session.post(
                            mcp_url,
                            json=initialized_notification,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                        ) as notify_response:
                            if notify_response.status not in [200, 202]:
                                pass  # å¿½ç•¥initializedé€šçŸ¥å¤±è´¥
                        
                        init_msg = "ğŸ”§ MCPä¼šè¯åˆå§‹åŒ–å®Œæˆ"
                        if stream_mode:
                            for chunk in self._emit_processing(init_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield init_msg + "\n"
                    else:
                        error_text = await response.text()
                        raise Exception(f"Initialize failed - HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ MCPä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    async def _discover_mcp_tools(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """é€šè¿‡MCP JSON-RPCåè®®å‘ç°æœåŠ¡å™¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            raise Exception("MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®")
        
        start_msg = f"ğŸ” æ­£åœ¨å‘ç°PubTator3 MCPå·¥å…·..."
        if stream_mode:
            for chunk in self._emit_processing(start_msg, "mcp_discovery"):
                yield f'data: {json.dumps(chunk)}\n\n'
        else:
            yield start_msg + "\n"
        
        # é¦–å…ˆåˆå§‹åŒ–MCPä¼šè¯
        if not hasattr(self, '_session_initialized'):
            async for init_output in self._initialize_mcp_session(stream_mode):
                yield init_output
            self._session_initialized = True
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_request = {
                "jsonrpc": "2.0",
                "method": "tools/list",
                "id": "tools-list-1"
            }
            
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    json=mcp_request,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            mcp_response = None
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == "tools-list-1":  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            mcp_response = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            mcp_response = await response.json()
                        
                        if not mcp_response:
                            raise Exception("No tools/list response received")
                        
                        if "error" in mcp_response:
                            raise Exception(f"MCP error: {mcp_response['error']}")
                        
                        tools = mcp_response.get("result", {}).get("tools", [])
                        
                        # åŠ è½½æ‰€æœ‰å·¥å…·
                        for tool in tools:
                            tool_name = tool.get("name")
                            if tool_name:
                                self.mcp_tools[tool_name] = {
                                    "name": tool_name,
                                    "description": tool.get("description", ""),
                                    "input_schema": tool.get("inputSchema", {})
                                }
                        
                        self.tools_loaded = True
                        self.tools_loaded_time = time.time()  # è®°å½•å·¥å…·åŠ è½½æ—¶é—´
                        
                        final_msg = f"âœ… å‘ç° {len(self.mcp_tools)} ä¸ªPubTator3 MCPå·¥å…·"
                        if len(self.mcp_tools) > 0:
                            final_msg += f": {', '.join(self.mcp_tools.keys())}"
                        
                        if stream_mode:
                            for chunk in self._emit_processing(final_msg, "mcp_discovery"):
                                yield f'data: {json.dumps(chunk)}\n\n'
                        else:
                            yield final_msg + "\n"
                        
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            error_msg = f"âŒ PubTator3 MCPå·¥å…·å‘ç°å¤±è´¥: {e}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            raise

    def _are_tools_expired(self) -> bool:
        """æ£€æŸ¥MCPå·¥å…·æ˜¯å¦å·²è¿‡æœŸ"""
        if not self.tools_loaded or self.tools_loaded_time is None:
            return True
        
        current_time = time.time()
        expire_seconds = self.valves.MCP_TOOLS_EXPIRE_HOURS * 3600  # è½¬æ¢ä¸ºç§’
        return (current_time - self.tools_loaded_time) > expire_seconds

    async def _ensure_tools_loaded(self, stream_mode: bool = False) -> AsyncGenerator[str, None]:
        """ç¡®ä¿MCPå·¥å…·å·²åŠ è½½ä¸”æœªè¿‡æœŸ"""
        need_reload = False
        reason = ""
        
        if not self.tools_loaded:
            need_reload = True
            reason = "å·¥å…·æœªåŠ è½½"
        elif self._are_tools_expired():
            need_reload = True
            expired_hours = (time.time() - self.tools_loaded_time) / 3600
            reason = f"å·¥å…·å·²è¿‡æœŸ ({expired_hours:.1f} å°æ—¶å‰åŠ è½½)"
        
        if need_reload:
            reload_msg = f"ğŸ”„ {reason}ï¼Œæ­£åœ¨é‡æ–°å‘ç°PubTator3 MCPå·¥å…·..."
            if stream_mode:
                for chunk in self._emit_processing(reload_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield reload_msg + "\n"
            
            # æ¸…é™¤æ—§çš„å·¥å…·å’Œä¼šè¯çŠ¶æ€
            self.mcp_tools = {}
            self.tools_loaded = False
            self.tools_loaded_time = None
            if hasattr(self, '_session_initialized'):
                delattr(self, '_session_initialized')
            self.session_id = None
            
            async for discovery_output in self._discover_mcp_tools(stream_mode):
                yield discovery_output

    async def _call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨MCP JSON-RPCåè®®è°ƒç”¨å·¥å…·"""
        if not self.valves.MCP_SERVER_URL:
            return {"error": "MCPæœåŠ¡å™¨åœ°å€æœªé…ç½®"}
        
        if not self.tools_loaded or self._are_tools_expired():
            try:
                async for output in self._ensure_tools_loaded(stream_mode=False):
                    # Silently consume the debug output in this context
                    pass
            except Exception as e:
                return {"error": f"å·¥å…·åŠ è½½å¤±è´¥: {str(e)}"}
        
        if tool_name not in self.mcp_tools:
            return {"error": f"å·¥å…· '{tool_name}' ä¸å¯ç”¨"}
        
        try:
            # æ„å»ºMCP JSON-RPCè¯·æ±‚
            mcp_url = f"{self.valves.MCP_SERVER_URL.strip().rstrip('/')}/mcp"
            
            # MCP JSON-RPCæ ¼å¼è¯·æ±‚ä½“
            jsonrpc_payload = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": arguments
                },
                "id": f"mcp_{tool_name}_{int(time.time())}"
            }
            
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            }
            if hasattr(self, 'session_id') and self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mcp_url,
                    headers=headers,
                    json=jsonrpc_payload,
                    timeout=aiohttp.ClientTimeout(total=self.valves.MCP_TIMEOUT)
                ) as response:
                    if response.status == 200:
                        # å¤„ç†å“åº”ï¼Œå¯èƒ½æ˜¯JSONæˆ–SSEæµ
                        content_type = response.headers.get("Content-Type", "")
                        if "text/event-stream" in content_type:
                            # å¤„ç†SSEæµ
                            result = None
                            request_id = jsonrpc_payload["id"]
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                        if data.get("id") == request_id:  # åŒ¹é…æˆ‘ä»¬çš„è¯·æ±‚ID
                                            result = data
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            # ç›´æ¥JSONå“åº”
                            result = await response.json()
                        
                        if not result:
                            return {"error": "No response received"}
                        
                        # å¤„ç†MCP JSON-RPCå“åº”
                        if "result" in result:
                            return result["result"]
                        elif "error" in result:
                            return {"error": f"MCPé”™è¯¯: {result['error'].get('message', 'Unknown error')}"}
                        else:
                            return {"error": "æ— æ•ˆçš„MCPå“åº”æ ¼å¼"}
                    else:
                        return {"error": f"HTTP {response.status}: {await response.text()}"}
                        
        except asyncio.TimeoutError:
            logger.error(f"MCPå·¥å…·è°ƒç”¨è¶…æ—¶: {tool_name}")
            return {"error": "è¯·æ±‚è¶…æ—¶"}
        except aiohttp.ClientError as e:
            logger.error(f"MCP HTTPè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _execute_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """æ‰§è¡ŒMCPå·¥å…·å¹¶è¿”å›åŸå§‹ç»“æœ"""
        result = await self._call_mcp_tool(tool_name, arguments)
        
        # ç›´æ¥è¿”å›åŸå§‹JSONç»“æœï¼Œè®©LLMè‡ªä¸»å¤„ç†å†…å®¹
        return json.dumps(result, ensure_ascii=False, indent=2)

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI APIå¹¶ç»Ÿè®¡tokenä½¿ç”¨é‡"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        # ç»Ÿè®¡è¾“å…¥tokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
        input_text = ""
        for msg in messages:
            input_text += msg.get("content", "")
        input_tokens = len(input_text)
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            result = response.json()
            
            # è·å–å“åº”å†…å®¹
            response_content = result["choices"][0]["message"]["content"]
            
            # ç»Ÿè®¡è¾“å‡ºtokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
            output_tokens = len(response_content)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.token_stats["input_tokens"] += input_tokens
            self.token_stats["output_tokens"] += output_tokens
            self.token_stats["total_tokens"] += input_tokens + output_tokens
            self.token_stats["api_calls"] += 1
            
            return response_content
        except Exception as e:
            return f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"

    def _stream_openai_response(self, user_prompt: str, system_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”å¹¶ç»Ÿè®¡tokenä½¿ç”¨é‡"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        # ç»Ÿè®¡è¾“å…¥tokenæ•°é‡ï¼ˆç”¨å­—ç¬¦æ•°ä¼°è®¡ï¼‰
        input_text = ""
        for msg in messages:
            input_text += msg.get("content", "")
        input_tokens = len(input_text)
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        # ç”¨äºç´¯ç§¯è¾“å‡ºå†…å®¹
        output_content = ""
        
        try:
            response = requests.post(url, headers=headers, json=payload, stream=True, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                output_content += delta
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            # ç»Ÿè®¡è¾“å‡ºtokenæ•°é‡å¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            output_tokens = len(output_content)
            self.token_stats["input_tokens"] += input_tokens
            self.token_stats["output_tokens"] += output_tokens
            self.token_stats["total_tokens"] += input_tokens + output_tokens
            self.token_stats["api_calls"] += 1
            
        except Exception as e:
            yield f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"

    async def _reasoning_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActæ¨ç†é˜¶æ®µ"""
        context = self._build_conversation_context(user_message, messages)
        used_queries = list(self.react_state['query_terms_used'])
        

        
        reasoning_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æœç´¢åŠ©æ‰‹ã€‚è¯·åŸºäºç”¨æˆ·é—®é¢˜å’Œå·²æœ‰ä¿¡æ¯åˆ¶å®šæœç´¢ç­–ç•¥ã€‚

ç”¨æˆ·é—®é¢˜: {user_message}
å¯¹è¯å†å²: {context}
å·²ä½¿ç”¨æŸ¥è¯¢è¯: {used_queries}

**åˆ†æä»»åŠ¡ï¼š**
1. åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢è®ºæ–‡ï¼Ÿ
2. å¦‚æœéœ€è¦æœç´¢ï¼Œä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ ¸å¿ƒä¸“ä¸šåè¯ä½œä¸ºæŸ¥è¯¢å…³é”®è¯
3. é¿å…é‡å¤å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯: {used_queries}

**æŸ¥è¯¢è¯è¦æ±‚ï¼š**
- ä»ç”¨æˆ·é—®é¢˜ä¸­æå–çš„æ ¸å¿ƒä¸“ä¸šåè¯
- ä½¿ç”¨æ ‡å‡†è‹±æ–‡åŒ»å­¦/ç”Ÿç‰©/åŒ–å­¦æœ¯è¯­
- å¯ä»¥æ˜¯å•ä¸ªå…³é”®è¯æˆ–å¤šä¸ªå…³é”®è¯çš„ç©ºæ ¼ç»„åˆ
- å¤šä¸ªå…³é”®è¯ç”¨ç©ºæ ¼åˆ†éš”ï¼Œç³»ç»Ÿä¼šæœç´¢åŒ…å«è¿™äº›è¯çš„æ‘˜è¦
- ä¸ä½¿ç”¨ANDã€ORã€NOTç­‰å¸ƒå°”æ“ä½œç¬¦

**ç¤ºä¾‹ï¼š**
ç”¨æˆ·é—®é¢˜"facial cleanserå¯¹skin healthçš„å½±å“" â†’ æŸ¥è¯¢: "facial cleanser skin health"
ç”¨æˆ·é—®é¢˜"ç»´ç”Ÿç´ Cå¯¹çš®è‚¤æŠ—è¡°è€çš„ä½œç”¨" â†’ æŸ¥è¯¢: "vitamin C anti-aging"
ç”¨æˆ·é—®é¢˜"probioticsåœ¨dermatologyä¸­çš„åº”ç”¨" â†’ æŸ¥è¯¢: "probiotics dermatology"
ç”¨æˆ·é—®é¢˜"ceramidesçš„ä¿æ¹¿æœºåˆ¶" â†’ æŸ¥è¯¢: "ceramides moisturizing mechanism"

å›å¤æ ¼å¼ï¼š
```json
{{
    "need_search": true/false,
    "query": "ä»ç”¨æˆ·é—®é¢˜æå–çš„ä¸“ä¸šåè¯",
    "reasoning": "åŸºäºç”¨æˆ·é—®é¢˜å’Œå·²æœ‰ä¿¡æ¯çš„åˆ†æ",
    "sufficient_info": true/false
}}
```"""

        if stream_mode:
            for chunk in self._emit_processing("åˆ†æç”¨æˆ·é—®é¢˜ï¼ŒåŸºäºå·²æœ‰ä¿¡æ¯åˆ¶å®šæœç´¢ç­–ç•¥...", "reasoning"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        decision = self._call_openai_api("", reasoning_prompt, json_mode=True)
        
        try:
            decision_data = json.loads(decision)
            if stream_mode:
                reasoning_content = f"æ¨ç†åˆ†æï¼š{decision_data.get('reasoning', 'æ— ')}"
                for chunk in self._emit_processing(reasoning_content, "reasoning"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("decision", decision_data)
        except json.JSONDecodeError:
            yield ("decision", {"need_search": False, "sufficient_info": True, "reasoning": "è§£æå¤±è´¥"})

    async def _action_phase(self, query: str, page: int = 1, page_size: int = 10, stream_mode: bool = False) -> AsyncGenerator[tuple, None]:
        """ReActåŠ¨ä½œé˜¶æ®µ"""
        # æ›´æ–°å½“å‰é¡µç çŠ¶æ€
        self.react_state["current_page"] = page
        self.react_state["current_page_size"] = page_size
        self.react_state["query_pages"][query] = page
        
        if stream_mode:
            action_msg = f"æ‰§è¡Œè®ºæ–‡æœç´¢ï¼š{query} (ç¬¬{page}é¡µï¼Œæ¯é¡µ{page_size}ç¯‡)"
            for chunk in self._emit_processing(action_msg, "action"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # è°ƒç”¨pubtator3å·¥å…·æœç´¢è®ºæ–‡
        tool_args = {
            "query": query,
            "page": page,
            "page_size": page_size,
            "include_full_abstracts": True
        }
        
        # è·å–åŸå§‹å·¥å…·è°ƒç”¨ç»“æœ
        tool_result = await self._execute_mcp_tool("search_papers_by_abstract", tool_args)

        #æ ¼å¼ç¾åŒ–
        try:
            json_result = json.loads(tool_result)
            tool_result = json.dumps(json_result, ensure_ascii=False, indent=2)
        except json.JSONDecodeError:
            pass
        
        # ä½¿ç”¨_emit_processingè¾“å‡ºå·¥å…·è¿”å›ç»“æœçš„markdownä»£ç æ¡†
        if stream_mode:
            tool_output_msg = f"**å·¥å…·è°ƒç”¨ç»“æœ:**\n\n```json\n{tool_result}\n```"
            for chunk in self._emit_processing(tool_output_msg, "action"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # è®°å½•æŸ¥è¯¢å†å²å’ŒæŸ¥è¯¢è¯
        self.react_state['query_history'].append(query)
        self.react_state['query_terms_used'].add(query.lower())
        
        yield ("result", tool_result)

    async def _observation_phase(self, action_result: str, query: str, user_message: str, stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActè§‚å¯Ÿé˜¶æ®µ"""
        if stream_mode:
            for chunk in self._emit_processing("è§‚å¯Ÿæœç´¢ç»“æœï¼Œåˆ†ææ‘˜è¦å†…å®¹ï¼Œæå–æ–°æŸ¥è¯¢å…³é”®è¯...", "observation"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # æ„å»ºè§‚å¯Ÿprompt
        used_queries = list(self.react_state['query_terms_used'])
        extracted_history = list(self.react_state['extracted_keywords_history'])
        unused_keywords = [kw for kw in extracted_history if kw.lower() not in self.react_state['query_terms_used']]
        current_page = self.react_state.get('current_page', 1)
        current_page_size = self.react_state.get('current_page_size', 10)
        
        observation_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæœç´¢ç»“æœä¸­çš„è®ºæ–‡æ‘˜è¦å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼š

ç”¨æˆ·é—®é¢˜: {user_message}  
ä½¿ç”¨çš„æŸ¥è¯¢è¯: {query}
å½“å‰é¡µç : {current_page} (æ¯é¡µ{current_page_size}ç¯‡)
å½“å‰è¿­ä»£: {self.react_state['current_iteration']}/{self.valves.MAX_REACT_ITERATIONS}
å·²ä½¿ç”¨æŸ¥è¯¢è¯: {used_queries}
å†å²æå–çš„å…³é”®è¯: {extracted_history}
å†å²æœªä½¿ç”¨çš„å…³é”®è¯: {unused_keywords}

å½“å‰æœç´¢ç»“æœåŸå§‹æ•°æ®:
{action_result}

**å…³é”®ä»»åŠ¡ï¼š**
1. è‡ªä¸»åˆ†æå½“å‰æœç´¢ç»“æœçš„åŸå§‹JSONæ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦æˆåŠŸæ‰¾åˆ°ç›¸å…³è®ºæ–‡
2. ä»”ç»†åˆ†æJSONä¸­è®ºæ–‡çš„æ‘˜è¦å†…å®¹ï¼Œè¯†åˆ«ä¸“ä¸šæœ¯è¯­ã€åŒ–å­¦æˆåˆ†ã€ç”Ÿç‰©å­¦æ¦‚å¿µã€æŠ€æœ¯æ–¹æ³•ç­‰
3. ä»æ‘˜è¦å†…å®¹ä¸­è¯†åˆ«ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„**åè¯**å…³é”®è¯
4. è®°å½•é«˜ç›¸å…³åº¦çš„å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€DOIã€æ‘˜è¦ã€ç›¸å…³æ€§æƒé‡ï¼‰
5. è¯„ä¼°å½“å‰é¡µç»“æœçš„è´¨é‡å’Œæ•°é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç¿»é¡µè·å–æ›´å¤šè®ºæ–‡
6. é€‰æ‹©èƒ½å¤Ÿè¿›ä¸€æ­¥æ·±å…¥æ¢ç´¢ç›¸å…³ä¸»é¢˜çš„æ–°æŸ¥è¯¢è¯
7. å¦‚æœä»å½“å‰æ‘˜è¦ä¸­æ— æ³•æ‰¾åˆ°æ–°çš„æœ‰ç”¨å…³é”®è¯ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨å†å²æœªä½¿ç”¨çš„å…³é”®è¯: {unused_keywords}
8. é¿å…ä½¿ç”¨å·²ç»æŸ¥è¯¢è¿‡çš„è¯: {used_queries}

**æŸ¥è¯¢è¯é€‰æ‹©ç­–ç•¥ï¼š**
- ä¼˜å…ˆçº§1: ä»å½“å‰æ‘˜è¦ä¸­æå–çš„æ–°ä¸“ä¸šåè¯
- ä¼˜å…ˆçº§2: å†å²æœªä½¿ç”¨çš„å…³é”®è¯ï¼ˆå¦‚æœä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ï¼‰
- å¯ä»¥æ˜¯å•ä¸ªä¸“ä¸šåè¯æˆ–å¤šä¸ªå…³é”®è¯çš„ç©ºæ ¼ç»„åˆ
- å¤šä¸ªå…³é”®è¯ç”¨ç©ºæ ¼åˆ†éš”ï¼Œç³»ç»Ÿä¼šæœç´¢åŒ…å«è¿™äº›è¯çš„æ‘˜è¦
- ä¸ä½¿ç”¨ANDã€ORã€NOTç­‰å¸ƒå°”æ“ä½œç¬¦

**æŸ¥è¯¢è¯æ„é€ ç¤ºä¾‹ï¼š**
- å•ä¸ªæœ¯è¯­: "ceramides", "retinol", "hyaluronic acid"
- ç»„åˆæŸ¥è¯¢: "vitamin C collagen", "probiotics dermatology"
- ç›¸å…³æœ¯è¯­ç»„åˆ: "vitamin C ascorbic acid", "retinol tretinoin"
- å¤šè¯ç»„åˆ: "anti-aging peptides", "skin barrier function"
- æœºåˆ¶ç›¸å…³: "ceramides skin moisture", "collagen synthesis aging"

**ç¿»é¡µç­–ç•¥ï¼š**
- å¦‚æœå½“å‰é¡µè®ºæ–‡æ•°é‡å°‘äºæœŸæœ›æ•°é‡ï¼Œä¸”å†…å®¹è´¨é‡é«˜ï¼Œè€ƒè™‘ç¿»é¡µè·å–æ›´å¤šè®ºæ–‡
- å¦‚æœå½“å‰é¡µè®ºæ–‡ç›¸å…³æ€§è¾ƒé«˜ï¼Œå¯ä»¥ç¿»é¡µå¯»æ‰¾æ›´å¤šç›¸å…³ç ”ç©¶
- ç¿»é¡µä»…é€‚ç”¨äºå½“å‰æŸ¥è¯¢è¯ï¼Œä¸è¦é¢‘ç¹ç¿»é¡µé¿å…æ•ˆç‡ä½ä¸‹
- é¡µå¤§å°å¯ä»¥è°ƒæ•´ï¼ˆå»ºè®®5-20ç¯‡ï¼‰ï¼Œé»˜è®¤10ç¯‡

**å…³é”®è®ºæ–‡ç­›é€‰æ ‡å‡†ï¼š**
- ä»”ç»†åˆ†ææ¯ç¯‡è®ºæ–‡æ‘˜è¦ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
- åŸºäºæ‘˜è¦å†…å®¹è¯„ä¼°è®ºæ–‡å¯¹å›ç­”ç”¨æˆ·é—®é¢˜çš„ä»·å€¼
- è®°å½•æ‰€æœ‰æ‰¾åˆ°çš„è®ºæ–‡ï¼Œä½†æŒ‰ç›¸å…³æ€§æƒé‡æ’åº
- ç›¸å…³æ€§æƒé‡åº”åæ˜ è®ºæ–‡å¯¹ç”¨æˆ·é—®é¢˜çš„ç›´æ¥ç›¸å…³ç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰

å›å¤æ ¼å¼ï¼š
```json
{{
    "relevance_score": 0-10,
    "sufficient_info": true/false,
    "need_more_search": true/false,
    "suggested_query": "æ–°æŸ¥è¯¢è¯æˆ–å†å²æœªä½¿ç”¨å…³é”®è¯(if needed)",
    "query_source": "current_abstract/historical_keywords",
    "next_page": 0,
    "page_size": 10,
    "need_pagination": true/false,
    "pagination_reason": "ç¿»é¡µåŸå› è¯´æ˜(if needed)",
    "extracted_keywords": ["ä»å½“å‰æ‘˜è¦ä¸­è¯†åˆ«çš„å…³é”®æœ¯è¯­åˆ—è¡¨"],
    "key_papers": [
        {{
            "title": "è®ºæ–‡æ ‡é¢˜",
            "authors": "ä½œè€…åˆ—è¡¨", 
            "doi": "DOIæˆ–é“¾æ¥",
            "abstract": "æ‘˜è¦å†…å®¹",
            "relevance_weight": 0.0-1.0,
            "key_findings": "å…³é”®å‘ç°æˆ–ç»“è®º",
            "urls": ["DOIé“¾æ¥", "å¼€æ”¾è®¿é—®PDFé“¾æ¥", "è®ºæ–‡URLç­‰"]
        }}
    ],
    "observation": "åŸºäºæ‘˜è¦å†…å®¹çš„è¯¦ç»†åˆ†æ"
}}
```"""

        observation = self._call_openai_api("", observation_prompt, json_mode=True)
        
        try:
            observation_data = json.loads(observation)
            
            # å¤„ç†æå–çš„å…³é”®è¯å†å²è®°å½•
            extracted_keywords = observation_data.get('extracted_keywords', [])
            if extracted_keywords:
                self.react_state['extracted_keywords_history'].update(extracted_keywords)
            
            # å¤„ç†å…³é”®è®ºæ–‡ä¿¡æ¯ï¼Œæ›´æ–°papers_collected
            key_papers = observation_data.get('key_papers', [])
            if key_papers:
                # æŒ‰ç›¸å…³æ€§æƒé‡æ’åºï¼Œé€‰æ‹©å‰60%çš„è®ºæ–‡
                papers_with_weights = []
                for paper in key_papers:
                    relevance_weight = paper.get('relevance_weight', 0.8)  # é»˜è®¤æƒé‡0.8
                    papers_with_weights.append((paper, relevance_weight))
                
                # æŒ‰æƒé‡é™åºæ’åº
                papers_with_weights.sort(key=lambda x: x[1], reverse=True)
                
                # é€‰æ‹©å‰80%çš„è®ºæ–‡ï¼ˆè‡³å°‘5ç¯‡ï¼‰
                num_to_select = max(5, int(len(papers_with_weights) * 0.8))
                selected_papers = papers_with_weights[:num_to_select]
                
                # æ·»åŠ åˆ°æ”¶é›†åˆ—è¡¨
                for paper, weight in selected_papers:
                    self.react_state['papers_collected'].append(paper)
            
            if stream_mode:
                obs_content = f"è§‚å¯Ÿåˆ†æï¼š{observation_data.get('observation', 'æ— ')}"
                
                if extracted_keywords:
                    obs_content += f"\næå–çš„å…³é”®è¯ï¼š{', '.join(extracted_keywords)}"
                
                if key_papers:
                    obs_content += f"\nå‘ç° {len(key_papers)} ç¯‡å…³é”®è®ºæ–‡"
                    num_selected =min(max(5, int(len(key_papers) * 0.8)),len(key_papers)) #
                    obs_content += f"ï¼ŒæŒ‰ç›¸å…³æ€§é€‰æ‹©({num_selected}ç¯‡)å·²æ”¶å½•"
                
                query_source = observation_data.get('query_source', 'current_abstract')
                if query_source == 'historical_keywords':
                    obs_content += f"\nå»ºè®®æŸ¥è¯¢è¯æ¥æºï¼šå†å²å…³é”®è¯é‡ç”¨"
                else:
                    obs_content += f"\nå»ºè®®æŸ¥è¯¢è¯æ¥æºï¼šå½“å‰æ‘˜è¦æå–"
                
                # æ˜¾ç¤ºç¿»é¡µä¿¡æ¯
                if observation_data.get('need_pagination', False):
                    next_page = observation_data.get('next_page', current_page + 1)
                    page_size = observation_data.get('page_size', current_page_size)
                    pagination_reason = observation_data.get('pagination_reason', 'éœ€è¦æ›´å¤šè®ºæ–‡')
                    obs_content += f"\nç¿»é¡µå»ºè®®ï¼šç¬¬{next_page}é¡µ (æ¯é¡µ{page_size}ç¯‡) - {pagination_reason}"
                
                for chunk in self._emit_processing(obs_content, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("observation", observation_data)
        except json.JSONDecodeError:
            yield ("observation", {"sufficient_info": True, "need_more_search": False})

    async def _answer_generation_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ"""
        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        context = self._build_conversation_context(user_message, messages)
        papers_summary = self._summarize_collected_papers()
        
        # è·å–è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯
        total_papers_count = len(self.react_state['papers_collected'])
        
        final_prompt = f"""åŸºäºæ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜: {user_message}
å¯¹è¯å†å²: {context}

ğŸ“Š **æ£€ç´¢ç»Ÿè®¡**: é€šè¿‡PubTator3æ£€ç´¢ï¼Œå…±æ”¶é›†åˆ° {total_papers_count} ç¯‡ç›¸å…³å­¦æœ¯è®ºæ–‡

æ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯:
{papers_summary}

**é‡è¦è¦æ±‚ï¼š**
1. **å……åˆ†åˆ©ç”¨æ‰€æœ‰æ”¶é›†åˆ°çš„è®ºæ–‡ä¿¡æ¯** - ä¸è¦é—æ¼ä»»ä½•ç›¸å…³ç ”ç©¶
2. **è¯¦ç»†å¼•ç”¨è®ºæ–‡** - æ¯ä¸ªè§‚ç‚¹éƒ½è¦æ ‡æ³¨æ¥æºè®ºæ–‡çš„æ ‡é¢˜ã€ä½œè€…
3. **æ•´åˆå¤šç¯‡ç ”ç©¶** - ç»¼åˆåˆ†æä¸åŒç ”ç©¶çš„å‘ç°ï¼ŒæŒ‡å‡ºå…±è¯†å’Œåˆ†æ­§
4. **æä¾›å…·ä½“æ•°æ®** - å¼•ç”¨è®ºæ–‡ä¸­çš„å…·ä½“ç ”ç©¶æ•°æ®ã€ç»“æœã€ç»“è®º
5. **ç»“æ„åŒ–å›ç­”** - æŒ‰é€»è¾‘é¡ºåºç»„ç»‡å†…å®¹ï¼Œä¾¿äºç†è§£
6. **å®Œæ•´æ€§** - ç¡®ä¿å›ç­”æ¶µç›–ç”¨æˆ·é—®é¢˜çš„å„ä¸ªæ–¹é¢

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰è®ºæ–‡ä¿¡æ¯æä¾›å…¨é¢ã€è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚åŒ…å«ç›¸å…³è®ºæ–‡çš„å®Œæ•´å¼•ç”¨ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€DOIç­‰ï¼‰ã€‚
å¦‚æœæœ‰DOIæˆ–é“¾æ¥ï¼Œè¯·ä½¿ç”¨markdownæ ¼å¼è¾“å‡ºå¯ç‚¹å‡»é“¾æ¥ã€‚"""

        system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»”ç»†åˆ†ææ‰€æœ‰æä¾›çš„è®ºæ–‡ä¿¡æ¯
2. å……åˆ†åˆ©ç”¨æ¯ä¸€ç¯‡ç›¸å…³è®ºæ–‡çš„å†…å®¹
3. æä¾›å…¨é¢ã€è¯¦ç»†ã€æœ‰æ·±åº¦çš„å­¦æœ¯å›ç­”
4. ç¡®ä¿æ¯ä¸ªè§‚ç‚¹éƒ½æœ‰è®ºæ–‡æ”¯æ’‘å’Œå¼•ç”¨
5. æ•´åˆå¤šä¸ªç ”ç©¶æ¥æºï¼Œæä¾›ç»¼åˆæ€§è§è§£"""

        if stream_mode:
            for chunk in self._stream_openai_response(final_prompt, system_prompt):
                chunk_data = {
                    'choices': [{
                        'delta': {'content': chunk},
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
            
            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯ï¼ˆæµå¼æ¨¡å¼ï¼‰
            stats_text = self._get_token_stats_text()
            stats_chunk_data = {
                'choices': [{
                    'delta': {'content': stats_text},
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(stats_chunk_data)}\n\n"
        else:
            answer = self._call_openai_api(system_prompt, final_prompt)
            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯ï¼ˆéæµå¼æ¨¡å¼ï¼‰
            stats_text = self._get_token_stats_text()
            yield answer + stats_text

    def _get_token_stats_text(self) -> str:
        """æ ¼å¼åŒ–tokenç»Ÿè®¡ä¿¡æ¯"""
        stats = self.token_stats
        stats_text = f"""

---

**ğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡**
- è¾“å…¥Tokenæ•°: {stats['input_tokens']:,} å­—ç¬¦
- è¾“å‡ºTokenæ•°: {stats['output_tokens']:,} å­—ç¬¦  
- æ€»Tokenæ•°: {stats['total_tokens']:,} å­—ç¬¦
- APIè°ƒç”¨æ¬¡æ•°: {stats['api_calls']} æ¬¡
- å¹³å‡æ¯æ¬¡è°ƒç”¨: {stats['total_tokens']//max(stats['api_calls'], 1):,} å­—ç¬¦

*æ³¨: Tokenæ•°é‡åŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼Œå®é™…ä½¿ç”¨é‡å¯èƒ½ç•¥æœ‰å·®å¼‚*
"""
        return stats_text

    def _build_conversation_context(self, user_message: str, messages: List[dict]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not messages or len(messages) <= 1:
            return "æ— å†å²å¯¹è¯"
        
        context_text = ""
        recent_messages = messages[-4:] if len(messages) > 4 else messages
        for msg in recent_messages:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "")
            if len(content) > 200:
                content = content[:200] + "..."
            context_text += f"{role}: {content}\n"
        
        return context_text

    def _summarize_collected_papers(self) -> str:
        """æ€»ç»“æ”¶é›†åˆ°çš„å…³é”®è®ºæ–‡ä¿¡æ¯"""
        if not self.react_state['papers_collected']:
            return "æœªæ”¶é›†åˆ°å…³é”®è®ºæ–‡ä¿¡æ¯"
        
        summary = f"æ”¶é›†åˆ° {len(self.react_state['papers_collected'])} ç¯‡å…³é”®è®ºæ–‡:\n\n"
        for i, paper in enumerate(self.react_state['papers_collected'], 1):
            summary += f"=== å…³é”®è®ºæ–‡ {i} ===\n"
            summary += f"æ ‡é¢˜: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
            summary += f"ä½œè€…: {paper.get('authors', 'æœªçŸ¥ä½œè€…')}\n"
            if paper.get('doi'):
                summary += f"DOI: {paper.get('doi')}\n"
            summary += f"ç›¸å…³æ€§æƒé‡: {paper.get('relevance_weight', 1.0)}\n"
            if paper.get('key_findings'):
                summary += f"å…³é”®å‘ç°: {paper.get('key_findings')}\n"
            if paper.get('abstract'):
                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                abstract = paper.get('abstract')
                if len(abstract) > 500:
                    abstract = abstract[:500] + "..."
                summary += f"æ‘˜è¦: {abstract}\n"
            summary += "\n"
        
        return summary

    async def _react_loop(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ReActä¸»å¾ªç¯"""
        # é‡ç½®çŠ¶æ€å’Œtokenç»Ÿè®¡
        self.react_state = {
            "papers_collected": [],  # å­˜å‚¨å…³é”®è®ºæ–‡ä¿¡æ¯ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            "query_history": [],
            "query_terms_used": set(),  # å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯é›†åˆ
            "extracted_keywords_history": set(),  # å†å²æå–çš„å…³é”®è¯é›†åˆ
            "current_iteration": 0,
            "current_page": 1,  # å½“å‰é¡µç 
            "current_page_size": 10,  # å½“å‰é¡µå¤§å°
            "query_pages": {}  # è®°å½•æ¯ä¸ªæŸ¥è¯¢è¯ä½¿ç”¨çš„é¡µç  {query: page}
        }
        
        # é‡ç½®tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0, 
            "output_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # ç¡®ä¿å·¥å…·å·²åŠ è½½
        try:
            async for tools_output in self._ensure_tools_loaded(stream_mode):
                yield tools_output
        except Exception as e:
            error_msg = f"âŒ PubTator3 MCPå·¥å…·åŠ è½½å¤±è´¥: {str(e)}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "mcp_discovery"):
                    yield f'data: {json.dumps(chunk)}\n\n'
            else:
                yield error_msg + "\n"
            return
        
        # 1. Reasoningé˜¶æ®µï¼ˆåªåœ¨å¼€å§‹æ‰§è¡Œä¸€æ¬¡ï¼‰
        initial_decision = None
        async for phase_result in self._reasoning_phase(user_message, messages, stream_mode):
            result_type, content = phase_result
            if result_type == "processing":
                yield content
            elif result_type == "decision":
                initial_decision = content
                break
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢
        if not initial_decision.get("need_search", False) or initial_decision.get("sufficient_info", False):
            # ç›´æ¥è¿›å…¥ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
            async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
                yield answer_chunk
            return
        
        # 2. Action-Observationå¾ªç¯
        max_iterations = self.valves.MAX_REACT_ITERATIONS
        current_query = initial_decision.get("query", "")
        
        while self.react_state['current_iteration'] < max_iterations and current_query:
            self.react_state['current_iteration'] += 1
            
            # Actioné˜¶æ®µ - è·å–å½“å‰æŸ¥è¯¢çš„é¡µç ä¿¡æ¯
            current_page = self.react_state.get("current_page", 1)
            current_page_size = self.react_state.get("current_page_size", 10)
            
            action_result = None
            async for phase_result in self._action_phase(current_query, current_page, current_page_size, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "result":
                    action_result = content
                    break
            
            # ç­‰å¾…actionå®Œå…¨æ‰§è¡Œå®Œæˆåå†è¿›è¡Œobservation
            if action_result is None:
                break
                
            # Observationé˜¶æ®µ
            observation = None
            async for phase_result in self._observation_phase(action_result, current_query, user_message, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "observation":
                    observation = content
                    break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»é¡µï¼ˆä¼˜å…ˆçº§é«˜äºæ–°æŸ¥è¯¢ï¼‰
            if observation and observation.get("need_pagination", False):
                # ç¿»é¡µï¼šä½¿ç”¨ç›¸åŒæŸ¥è¯¢è¯ï¼Œæ›´æ–°é¡µç 
                next_page = observation.get("next_page", current_page + 1)
                new_page_size = observation.get("page_size", current_page_size)
                
                # æ›´æ–°é¡µç çŠ¶æ€
                self.react_state["current_page"] = next_page
                self.react_state["current_page_size"] = new_page_size
                
                # ç»§ç»­ä½¿ç”¨ç›¸åŒæŸ¥è¯¢è¯è¿›è¡Œä¸‹ä¸€è½®æœç´¢
                # current_query ä¿æŒä¸å˜
                continue
            
            # æ£€æŸ¥å·²æ”¶é›†è®ºæ–‡æ•°é‡ï¼Œå¦‚æœè¾¾åˆ°é˜ˆå€¼åˆ™å¼ºåˆ¶åœæ­¢
            collected_papers_count = len(self.react_state['papers_collected'])
            if collected_papers_count >= self.valves.MIN_PAPERS_THRESHOLD:
                if stream_mode:
                    stop_content = f"\nâœ… å·²æ”¶é›†è¶³å¤Ÿè®ºæ–‡({collected_papers_count}ç¯‡ >= {self.valves.MIN_PAPERS_THRESHOLD}ç¯‡é˜ˆå€¼)ï¼Œåœæ­¢æœç´¢"
                    for chunk in self._emit_processing(stop_content, "observation"):
                        yield f'data: {json.dumps(chunk)}\n\n'
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢
            if not observation or not observation.get("need_more_search", False) or observation.get("sufficient_info", False):
                break
            
            # è·å–ä¸‹ä¸€è½®æŸ¥è¯¢è¯ï¼ˆé‡ç½®é¡µç ä¸º1ï¼‰
            current_query = observation.get("suggested_query", "")
            self.react_state["current_page"] = 1  # æ–°æŸ¥è¯¢è¯ä»ç¬¬1é¡µå¼€å§‹
            
            # å¦‚æœå»ºè®®çš„æŸ¥è¯¢è¯å·²ç»ä½¿ç”¨è¿‡ï¼Œåˆ™åœæ­¢
            if current_query and current_query.lower() in self.react_state['query_terms_used']:
                break
        
        # 3. ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
        async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
            yield answer_chunk

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å­¦æœ¯è®ºæ–‡æœç´¢é—®é¢˜"
            return

        stream_mode = self.valves.ENABLE_STREAMING
        
        try:
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ReActå¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async_gen = self._react_loop(user_message, messages, stream_mode)
                
                while True:
                    try:
                        result = loop.run_until_complete(async_gen.__anext__())
                        yield result
                    except StopAsyncIteration:
                        break
                
                # æµå¼æ¨¡å¼ç»“æŸæ ‡è®°
                if stream_mode:
                    done_msg = {
                        'choices': [{
                            'delta': {},
                            'finish_reason': 'stop'
                        }]
                    }
                    yield f"data: {json.dumps(done_msg)}\n\n"
                    yield "data: [DONE]\n\n"
                    
            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if stream_mode:
                error_chunk = {
                    'choices': [{
                        'delta': {'content': error_msg},
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield error_msg