"""
title: LightRAG ReAct Pipeline
author: open-webui
date: 2024-12-20
version: 1.0
license: MIT
description: A ReAct-based pipeline for querying LightRAG knowledge base with support for multiple query modes (local, global, hybrid, naive, mix) and conversation history
requirements: requests
"""
import os
import json
import logging
import requests
import asyncio
import time
from typing import List, Union, Generator, Iterator, AsyncGenerator, Dict, Any
from pydantic import BaseModel

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ===========================================
# ReActé˜¶æ®µæ ‡é¢˜æ˜ å°„
# ===========================================

STAGE_TITLES = {
    "reasoning": "ğŸ¤” æ¨ç†åˆ†æ",
    "action": "ğŸ”§ æ‰§è¡Œæ£€ç´¢",
    "observation": "ğŸ‘ï¸ è§‚å¯Ÿç»“æœ", 
    "answer_generation": "ğŸ“ ç”Ÿæˆç­”æ¡ˆ",
    "lightrag_discovery": "ğŸ” LightRAGæœåŠ¡å‘ç°"
}

STAGE_GROUP = {
    "reasoning": "stage_group_1",
    "action": "stage_group_2", 
    "observation": "stage_group_3",
    "answer_generation": "stage_group_4",
    "lightrag_discovery": "stage_group_0"
}

# ===========================================
# æç¤ºè¯æ¨¡æ¿å®šä¹‰
# ===========================================

REASONING_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†æ£€ç´¢åŠ©æ‰‹ã€‚è¯·åŸºäºç”¨æˆ·é—®é¢˜å’Œå†å²å¯¹è¯åˆ†ææ˜¯å¦éœ€è¦ä½¿ç”¨LightRAGè¿›è¡ŒçŸ¥è¯†æ£€ç´¢ã€‚

ç”¨æˆ·å½“å‰é—®é¢˜: {user_message}
å†å²å¯¹è¯ä¸Šä¸‹æ–‡: {conversation_context}
å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯: {used_queries}
å·²æ”¶é›†çš„ä¿¡æ¯æ‘˜è¦: {collected_info_summary}

è¯·åˆ†æï¼š
1. ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾å’Œéœ€æ±‚
2. å†å²å¯¹è¯ä¸­æ˜¯å¦å·²æœ‰ç›¸å…³ä¿¡æ¯
3. æ˜¯å¦éœ€è¦é€šè¿‡LightRAGè·å–æ›´å¤šçŸ¥è¯†
4. å¦‚æœéœ€è¦æ£€ç´¢ï¼Œç”Ÿæˆé€‚åˆçš„æŸ¥è¯¢é—®é¢˜

 **LightRAGæ£€ç´¢ç‰¹ç‚¹åŠæ¨¡å¼ï¼š**
 - naive: æœ´ç´ å‘é‡æœç´¢ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆç®€å•æ–‡æ¡£æ£€ç´¢
 - local: æœ¬åœ°æ¨¡å¼ï¼Œä¸“æ³¨ç‰¹å®šå®ä½“çš„è¯¦ç»†ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
 - global: å…¨å±€æ¨¡å¼ï¼Œä¾§é‡å®ä½“é—´å…³ç³»å’Œå…¨å±€çŸ¥è¯†ç»“æ„
 - hybrid: æ··åˆæ¨¡å¼ï¼Œç»“åˆlocalå’Œglobalä¼˜åŠ¿
 - mix: æ··åˆæ£€ç´¢æ¨¡å¼ï¼Œæ•´åˆçŸ¥è¯†å›¾è°±å’Œå‘é‡æ£€ç´¢ï¼Œæœ€å…¨é¢æ•ˆæœæœ€å¥½ï¼ˆé»˜è®¤æ¨èï¼‰
 - æ”¯æŒä¸­è‹±æ–‡æ··åˆæŸ¥è¯¢ï¼Œé€‚ç”¨äºå¤æ‚çŸ¥è¯†å›¾è°±æ¨ç†
 
 å›å¤æ ¼å¼ï¼š
 ```json
 {{
     "need_search": true/false,
     "search_query": "é€‚åˆLightRAGæ£€ç´¢çš„æŸ¥è¯¢é—®é¢˜",
     "reasoning": "åˆ†ææ€è·¯å’Œåˆ¤æ–­ä¾æ®",
     "search_mode": "naive/local/global/hybrid/mix",
     "sufficient_info": true/false
 }}
```"""

OBSERVATION_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„ä¿¡æ¯åˆ†æä¸“å®¶ã€‚è¯·åˆ†æLightRAGæ£€ç´¢ç»“æœï¼Œåˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……åˆ†ï¼Œæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ£€ç´¢ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {user_message}
å½“å‰æ£€ç´¢æŸ¥è¯¢: {current_query}
æ£€ç´¢æ¨¡å¼: {search_mode}
æ£€ç´¢ç»“æœ: {search_result}

å·²æ”¶é›†çš„å†å²ä¿¡æ¯:
{collected_info}

è¯·åˆ†æï¼š
1. å½“å‰æ£€ç´¢ç»“æœçš„è´¨é‡å’Œç›¸å…³æ€§
2. æ˜¯å¦å·²è·å¾—è¶³å¤Ÿä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ä»€ä¹ˆæ ·çš„è¡¥å……æŸ¥è¯¢
4. æå–å½“å‰ç»“æœä¸­çš„å…³é”®ä¿¡æ¯ç‚¹

 **æŸ¥è¯¢ä¼˜åŒ–å»ºè®®ï¼ˆé€‰æ‹©åˆé€‚çš„æ£€ç´¢æ¨¡å¼ï¼‰ï¼š**
 - naive: ç®€å•å¿«é€Ÿæ£€ç´¢ï¼Œé€‚åˆåŸºç¡€æ–‡æ¡£æŸ¥æ‰¾
 - local: æ·±å…¥å®ä½“ç»†èŠ‚ï¼Œé€‚åˆæŸ¥è¯¢ç‰¹å®šæ¦‚å¿µ/äººç‰©ä¿¡æ¯  
 - global: å…³ç³»æ¨ç†ï¼Œé€‚åˆéœ€è¦ç†è§£å®ä½“é—´å…³ç³»çš„é—®é¢˜
 - hybrid: å¹³è¡¡æ¨¡å¼ï¼Œé€‚åˆä¸€èˆ¬å¤æ‚é—®é¢˜
 - mix: æœ€å…¨é¢æ£€ç´¢ï¼Œæ•ˆæœæœ€å¥½ï¼Œä¼˜å…ˆæ¨èï¼ˆé»˜è®¤é€‰æ‹©ï¼‰
 - å¯ä¼˜åŒ–æŸ¥è¯¢å…³é”®è¯ã€ä»ä¸åŒè§’åº¦æ„é€ é—®é¢˜ã€åŸºäºå·²æœ‰ç»“æœæ‰©å±•æŸ¥è¯¢
 
 å›å¤æ ¼å¼ï¼š
 ```json
 {{
     "relevance_score": 1-10,
     "sufficient_info": true/false,
     "need_more_search": true/false,
     "next_query": "ä¸‹ä¸€è½®æ£€ç´¢æŸ¥è¯¢(if needed)",
     "next_mode": "naive/local/global/hybrid/mix",
     "optimization_reason": "æŸ¥è¯¢ä¼˜åŒ–ç†ç”±",
     "key_information": ["ä»å½“å‰ç»“æœæå–çš„å…³é”®ä¿¡æ¯ç‚¹"],
     "observation": "è¯¦ç»†çš„ç»“æœåˆ†æå’Œæ€è€ƒ"
 }}
```"""

ANSWER_GENERATION_PROMPT = """åŸºäºæ”¶é›†åˆ°çš„LightRAGæ£€ç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›å…¨é¢å‡†ç¡®çš„ç­”æ¡ˆã€‚

ç”¨æˆ·é—®é¢˜: {user_message}
å†å²å¯¹è¯: {conversation_context}

æ”¶é›†åˆ°çš„ä¿¡æ¯:
{collected_information}

**å›ç­”è¦æ±‚ï¼š**
1. å……åˆ†åˆ©ç”¨æ‰€æœ‰æ£€ç´¢åˆ°çš„ä¿¡æ¯
2. ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€è¯¦ç»†ã€æœ‰é€»è¾‘
3. å¦‚æœä¿¡æ¯ä¸­æœ‰çŸ›ç›¾ï¼Œéœ€è¦æŒ‡å‡ºå¹¶åˆ†æ
4. å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯šå®è¯´æ˜å±€é™æ€§
5. ç»“æ„åŒ–ç»„ç»‡å›ç­”å†…å®¹
6. çªå‡ºå…³é”®ä¿¡æ¯å’Œæ´å¯Ÿ

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›å…¨é¢çš„å›ç­”ã€‚"""

class Pipeline:
    class Valves(BaseModel):
        # OpenAIé…ç½®
        OPENAI_API_KEY: str
        OPENAI_BASE_URL: str
        OPENAI_MODEL: str
        OPENAI_TIMEOUT: int
        OPENAI_MAX_TOKENS: int
        OPENAI_TEMPERATURE: float

        # LightRAGé…ç½®
        LIGHTRAG_BASE_URL: str
        LIGHTRAG_DEFAULT_MODE: str
        LIGHTRAG_TIMEOUT: int

        # Pipelineé…ç½®
        ENABLE_STREAMING: bool
        DEBUG_MODE: bool
        MAX_REACT_ITERATIONS: int
        MIN_INFO_THRESHOLD: int

    def __init__(self):
        self.name = "LightRAG ReAct Pipeline"
        
        # åˆå§‹åŒ–tokenç»Ÿè®¡
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0, 
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # ReActçŠ¶æ€ç®¡ç†
        self.react_state = {
            "collected_information": [],  # å­˜å‚¨æ£€ç´¢åˆ°çš„ä¿¡æ¯
            "query_history": [],          # æŸ¥è¯¢å†å²
            "used_queries": set(),        # å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯
            "current_iteration": 0,       # å½“å‰è¿­ä»£æ¬¡æ•°
            "search_modes_used": set(),   # å·²ä½¿ç”¨çš„æœç´¢æ¨¡å¼
        }
        
        self.valves = self.Valves(
            **{
                # OpenAIé…ç½®
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
                "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o"),
                "OPENAI_TIMEOUT": int(os.getenv("OPENAI_TIMEOUT", "60")),
                "OPENAI_MAX_TOKENS": int(os.getenv("OPENAI_MAX_TOKENS", "4000")),
                "OPENAI_TEMPERATURE": float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
                
                # LightRAGé…ç½®
                "LIGHTRAG_BASE_URL": os.getenv("LIGHTRAG_BASE_URL", "http://117.50.252.245:9621"),
                "LIGHTRAG_DEFAULT_MODE": os.getenv("LIGHTRAG_DEFAULT_MODE", "mix"),
                "LIGHTRAG_TIMEOUT": int(os.getenv("LIGHTRAG_TIMEOUT", "30")),
                
                # Pipelineé…ç½®
                "ENABLE_STREAMING": os.getenv("ENABLE_STREAMING", "true").lower() == "true",
                "DEBUG_MODE": os.getenv("DEBUG_MODE", "false").lower() == "true",
                "MAX_REACT_ITERATIONS": int(os.getenv("MAX_REACT_ITERATIONS", "5")),
                "MIN_INFO_THRESHOLD": int(os.getenv("MIN_INFO_THRESHOLD", "3")),
            }
        )

    async def on_startup(self):
        logger.info(f"COT LightRAG ReAct Pipelineå¯åŠ¨: {__name__}")
        
        # éªŒè¯å¿…éœ€çš„APIå¯†é’¥
        if not self.valves.OPENAI_API_KEY:
            logger.error("âŒ ç¼ºå°‘OpenAI APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # æµ‹è¯•LightRAGè¿æ¥
        try:
            response = requests.get(f"{self.valves.LIGHTRAG_BASE_URL}/health", timeout=5)
            if response.status_code == 200:
                logger.info("âœ… LightRAGæœåŠ¡è¿æ¥æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸ LightRAGæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°LightRAGæœåŠ¡: {e}")

    async def on_shutdown(self):
        logger.info(f"COT LightRAG ReAct Pipelineå…³é—­: {__name__}")

    def _emit_processing(self, content: str, stage: str = "processing" , next_line = True) -> Generator[dict, None, None]:
        """å‘é€å¤„ç†è¿‡ç¨‹å†…å®¹"""
        yield {
            'choices': [{
                'delta': {
                    'processing_content': content + ('\n' if next_line else ''),
                    'processing_title': STAGE_TITLES.get(stage, "å¤„ç†ä¸­"),
                    'processing_stage': STAGE_GROUP.get(stage, "stage_group_1")
                },
                'finish_reason': None
            }]
        }

    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        # ä¸­æ–‡å­—ç¬¦æŒ‰1ä¸ªtokenè®¡ç®—ï¼Œè‹±æ–‡å•è¯æŒ‰å¹³å‡1.3ä¸ªtokenè®¡ç®—
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_text = ''.join(char if not ('\u4e00' <= char <= '\u9fff') else ' ' for char in text)
        english_words = len([word for word in english_text.split() if word.strip()])
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        return max(estimated_tokens, 1)

    def _add_token_stats(self, input_text: str, output_text: str):
        """æ·»åŠ tokenç»Ÿè®¡"""
        input_tokens = self._estimate_tokens(input_text)
        output_tokens = self._estimate_tokens(output_text)
        self.token_stats["input_tokens"] += input_tokens
        self.token_stats["output_tokens"] += output_tokens
        self.token_stats["total_tokens"] += input_tokens + output_tokens
        self.token_stats["api_calls"] += 1

    def _build_conversation_context(self, user_message: str, messages: List[dict]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not messages or len(messages) <= 1:
            return "æ— å†å²å¯¹è¯"
        
        context_parts = []
        recent_messages = messages[-4:] if len(messages) > 4 else messages
        
        for msg in recent_messages:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "").strip()
            if content and len(content) > 200:
                content = content[:200] + "..."
            context_parts.append(f"{role}: {content}")
        
        return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"

    def _call_openai_api(self, system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.valves.OPENAI_API_KEY:
            return "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            result = response.json()
            
            response_content = result["choices"][0]["message"]["content"]
            
            # ç»Ÿè®¡tokenä½¿ç”¨
            input_text = system_prompt + user_prompt
            self._add_token_stats(input_text, response_content)
            
            return response_content
        except Exception as e:
            return f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}"

    def _stream_openai_response(self, system_prompt: str, user_prompt: str) -> Generator:
        """æµå¼å¤„ç†OpenAIå“åº”"""
        if not self.valves.OPENAI_API_KEY:
            yield "é”™è¯¯: æœªè®¾ç½®OpenAI APIå¯†é’¥"
            return
        
        url = f"{self.valves.OPENAI_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.valves.OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})
        
        payload = {
            "model": self.valves.OPENAI_MODEL,
            "messages": messages,
            "max_tokens": self.valves.OPENAI_MAX_TOKENS,
            "temperature": self.valves.OPENAI_TEMPERATURE,
            "stream": True
        }
        
        output_content = ""
        
        try:
            response = requests.post(url, headers=headers, json=payload, stream=True, timeout=self.valves.OPENAI_TIMEOUT)
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            delta = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                            if delta:
                                output_content += delta
                                yield delta
                        except json.JSONDecodeError:
                            pass
            
            # ç»Ÿè®¡tokenä½¿ç”¨
            input_text = system_prompt + user_prompt
            self._add_token_stats(input_text, output_content)
            
        except Exception as e:
            yield f"OpenAIæµå¼APIè°ƒç”¨é”™è¯¯: {str(e)}"

    def _call_lightrag(self, query: str, mode: str = None) -> dict:
        """è°ƒç”¨LightRAGè¿›è¡Œæ£€ç´¢ï¼ˆéæµå¼ç‰ˆæœ¬ï¼‰"""
        if not mode:
            mode = self.valves.LIGHTRAG_DEFAULT_MODE

        url = f"{self.valves.LIGHTRAG_BASE_URL}/query"
        payload = {
            "query": query,
            "mode": mode
        }

        headers = {"Content-Type": "application/json"}

        # ç»Ÿè®¡è¾“å…¥token
        self._add_token_stats(query, "")

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.LIGHTRAG_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            # ç»Ÿè®¡è¾“å‡ºtoken
            if "response" in result:
                self._add_token_stats("", result["response"])
                
            return result
        except Exception as e:
            return {"error": f"LightRAGæŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def _call_lightrag_stream(self, query: str, mode: str = None, stage: str = "action") -> Generator[tuple, None, None]:
        """è°ƒç”¨LightRAGè¿›è¡Œæµå¼æ£€ç´¢ï¼Œæ”¯æŒå®æ—¶æµå¼è¾“å‡ºï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if not mode:
            mode = self.valves.LIGHTRAG_DEFAULT_MODE

        url = f"{self.valves.LIGHTRAG_BASE_URL}/query/stream"
        payload = {
            "query": query,
            "mode": mode
        }

        headers = {
            "Content-Type": "application/json",
            "Accept": "application/x-ndjson",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }

        # ç»Ÿè®¡è¾“å…¥token
        self._add_token_stats(query, "")

        try:
            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.valves.LIGHTRAG_TIMEOUT,
                stream=True
            )
            response.raise_for_status()
            
            # è®¾ç½®è¾ƒå°çš„ç¼“å†²åŒºä»¥ç¡®ä¿å®æ—¶æ€§
            response.raw.decode_content = True
            
            collected_content = ""
            buffer = ""
            
            # ä½¿ç”¨ç¼“å†²åŒºå¤„ç†NDJSONæµå¼å“åº”
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    # ç¡®ä¿chunkæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if isinstance(chunk, bytes):
                        chunk = chunk.decode('utf-8', errors='ignore')
                    buffer += chunk
                    
                    # æŒ‰è¡Œåˆ†å‰²å¤„ç†NDJSON
                    while '\n' in buffer:
                        line, buffer = buffer.split('\n', 1)
                        line = line.strip()
                        
                        if line:
                            try:
                                json_data = json.loads(line)
                                
                                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                                if "error" in json_data:
                                    yield ("error", f"LightRAGæŸ¥è¯¢å¤±è´¥: {json_data['error']}")
                                    return
                                
                                # å¤„ç†æ­£å¸¸å“åº”
                                if "response" in json_data:
                                    response_content = json_data["response"]
                                    collected_content += response_content
                                    
                                    # ä½¿ç”¨_emit_processingå‡½æ•°è¿›è¡Œæµå¼è¾“å‡º
                                    for chunk_emit in self._emit_processing(response_content, stage, next_line=False):
                                        yield ("stream_data", f"data: {json.dumps(chunk_emit)}\n\n")
                                    
                            except json.JSONDecodeError as e:
                                if self.valves.DEBUG_MODE:
                                    logger.warning(f"JSONè§£æé”™è¯¯: {line} - {str(e)}")
                                continue
            
            # å¤„ç†æœ€åçš„ç¼“å†²åŒºå†…å®¹
            if buffer.strip():
                try:
                    json_data = json.loads(buffer.strip())
                    if "response" in json_data:
                        response_content = json_data["response"]
                        collected_content += response_content
                        
                        # ä½¿ç”¨_emit_processingå‡½æ•°è¿›è¡Œæœ€åçš„æµå¼è¾“å‡º
                        for chunk_emit in self._emit_processing(response_content, stage, next_line=False):
                            yield ("stream_data", f"data: {json.dumps(chunk_emit)}\n\n")
                    elif "error" in json_data:
                        yield ("error", f"LightRAGæŸ¥è¯¢å¤±è´¥: {json_data['error']}")
                        return
                except json.JSONDecodeError:
                    if self.valves.DEBUG_MODE:
                        logger.warning(f"æ— æ³•è§£ææœ€åçš„å“åº”ç‰‡æ®µ: {buffer}")
                        
            # ç»Ÿè®¡è¾“å‡ºtoken
            if collected_content:
                self._add_token_stats("", collected_content)
                
            # è¿”å›æœ€ç»ˆæ”¶é›†çš„å†…å®¹
            yield ("result", {"response": collected_content} if collected_content else {"error": "æœªè·å–åˆ°æ£€ç´¢ç»“æœ"})
                
        except Exception as e:
            yield ("error", f"LightRAGæµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}")

    async def _reasoning_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActæ¨ç†é˜¶æ®µ"""
        if stream_mode:
            for chunk in self._emit_processing("åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢çŸ¥è¯†...", "reasoning"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        conversation_context = self._build_conversation_context(user_message, messages)
        used_queries = list(self.react_state['used_queries'])
        collected_info_summary = self._summarize_collected_info()
        
        reasoning_prompt = REASONING_PROMPT.format(
            user_message=user_message,
            conversation_context=conversation_context,
            used_queries=used_queries,
            collected_info_summary=collected_info_summary
        )
        
        decision = self._call_openai_api("", reasoning_prompt, json_mode=True)
        
        # æ£€æŸ¥OpenAI APIæ˜¯å¦è¿”å›é”™è¯¯
        if decision.startswith("é”™è¯¯:") or decision.startswith("OpenAI APIè°ƒç”¨é”™è¯¯:"):
            error_msg = f"âŒ æ¨ç†åˆ†æå¤±è´¥ï¼š{decision}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "reasoning"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            # è¿”å›é»˜è®¤å†³ç­–ï¼Œè¡¨ç¤ºä¸éœ€è¦æœç´¢
            yield ("decision", {"need_search": False, "sufficient_info": True, "reasoning": "æ¨ç†åˆ†æå¤±è´¥", "error": decision})
            return
        
        try:
            decision_data = json.loads(decision)
            if stream_mode:
                reasoning_content = f"æ¨ç†åˆ†æï¼š{decision_data.get('reasoning', 'æ— åˆ†æ')}"
                for chunk in self._emit_processing(reasoning_content, "reasoning"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("decision", decision_data)
        except json.JSONDecodeError:
            error_msg = "âŒ æ¨ç†åˆ†æç»“æœè§£æå¤±è´¥ï¼šæ— æ³•è§£æJSONæ ¼å¼"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "reasoning"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            yield ("decision", {"need_search": False, "sufficient_info": True, "reasoning": "è§£æå¤±è´¥"})

    async def _action_phase(self, query: str, mode: str = "mix", stream_mode: bool = False) -> AsyncGenerator[tuple, None]:
        """ReActåŠ¨ä½œé˜¶æ®µ - è°ƒç”¨LightRAGï¼ˆä½¿ç”¨æµå¼è°ƒç”¨ï¼‰"""
        if stream_mode:
            action_msg = f"æ‰§è¡ŒLightRAGæ£€ç´¢ï¼š{query} (æ¨¡å¼: {mode})"
            for chunk in self._emit_processing(action_msg, "action"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # è°ƒç”¨LightRAGæµå¼æ£€ç´¢
        search_result = None
        collected_content = ""
        
        for stream_result in self._call_lightrag_stream(query, mode, "action"):
            result_type, content = stream_result
            
            if result_type == "stream_data" and stream_mode:
                # ç›´æ¥è¾“å‡ºæµå¼æ•°æ®
                yield ("processing", content)
            elif result_type == "result":
                # æ”¶é›†æœ€ç»ˆç»“æœ
                search_result = content
            elif result_type == "error":
                # å¤„ç†é”™è¯¯ï¼Œä½¿ç”¨_emit_processingè¾“å‡ºé”™è¯¯ä¿¡æ¯
                if stream_mode:
                    error_msg = f"âŒ LightRAGæ£€ç´¢å¤±è´¥ï¼š{content}"
                    for chunk in self._emit_processing(error_msg, "action"):
                        yield ("processing", f'data: {json.dumps(chunk)}\n\n')
                search_result = {"error": content}
                break
        
        # å¦‚æœæ²¡æœ‰è·å¾—ç»“æœï¼Œè®¾ç½®é»˜è®¤é”™è¯¯
        if search_result is None:
            error_msg = "âŒ æœªè·å–åˆ°æ£€ç´¢ç»“æœ"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "action"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            search_result = {"error": "æœªè·å–åˆ°æ£€ç´¢ç»“æœ"}
           
        # è®°å½•æŸ¥è¯¢å†å²
        self.react_state['query_history'].append({
            "query": query,
            "mode": mode,
            "result": search_result
        })
        self.react_state['used_queries'].add(query.lower())
        self.react_state['search_modes_used'].add(mode)
        
        yield ("result", search_result)

    async def _observation_phase(self, search_result: dict, current_query: str, search_mode: str, 
                         user_message: str, stream_mode: bool) -> AsyncGenerator[tuple, None]:
        """ReActè§‚å¯Ÿé˜¶æ®µ"""
        if stream_mode:
            for chunk in self._emit_processing("è§‚å¯Ÿæ£€ç´¢ç»“æœï¼Œåˆ†æä¿¡æ¯è´¨é‡ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨...", "observation"):
                yield ("processing", f'data: {json.dumps(chunk)}\n\n')
        
        # æ£€æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦åŒ…å«é”™è¯¯
        if isinstance(search_result, dict) and "error" in search_result:
            error_msg = f"âŒ è§‚å¯Ÿåˆ°æ£€ç´¢é”™è¯¯ï¼š{search_result['error']}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            # è¿”å›é»˜è®¤çš„è§‚å¯Ÿç»“æœï¼Œè¡¨ç¤ºéœ€è¦åœæ­¢æœç´¢
            yield ("observation", {"sufficient_info": False, "need_more_search": False, "error": search_result["error"]})
            return
        
        collected_info = self._get_collected_info_text()
        
        observation_prompt = OBSERVATION_PROMPT.format(
            user_message=user_message,
            current_query=current_query,
            search_mode=search_mode,
            search_result=json.dumps(search_result, ensure_ascii=False, indent=2),
            collected_info=collected_info
        )
        
        observation = self._call_openai_api("", observation_prompt, json_mode=True)
        
        # æ£€æŸ¥OpenAI APIæ˜¯å¦è¿”å›é”™è¯¯
        if observation.startswith("é”™è¯¯:") or observation.startswith("OpenAI APIè°ƒç”¨é”™è¯¯:"):
            error_msg = f"âŒ è§‚å¯Ÿåˆ†æå¤±è´¥ï¼š{observation}"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            # è¿”å›é»˜è®¤çš„è§‚å¯Ÿç»“æœï¼Œè¡¨ç¤ºéœ€è¦åœæ­¢æœç´¢
            yield ("observation", {"sufficient_info": False, "need_more_search": False, "error": observation})
            return
        
        try:
            observation_data = json.loads(observation)
            
            # å­˜å‚¨å…³é”®ä¿¡æ¯åˆ°æ”¶é›†åˆ—è¡¨
            key_info = observation_data.get('key_information', [])
            if key_info:
                info_entry = {
                    "query": current_query,
                    "mode": search_mode,
                    "key_information": key_info,
                    "relevance_score": observation_data.get('relevance_score', 5),
                    "raw_result": search_result
                }
                self.react_state['collected_information'].append(info_entry)
            
            if stream_mode:
                obs_content = f"è§‚å¯Ÿåˆ†æï¼š{observation_data.get('observation', 'æ— è§‚å¯Ÿ')}"
                
                if key_info:
                    obs_content += f"\næå–å…³é”®ä¿¡æ¯ï¼š{', '.join(key_info)}"
                
                relevance_score = observation_data.get('relevance_score', 0)
                obs_content += f"\nç›¸å…³æ€§è¯„åˆ†ï¼š{relevance_score}/10"
                
                for chunk in self._emit_processing(obs_content, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            
            yield ("observation", observation_data)
        except json.JSONDecodeError:
            error_msg = "âŒ è§‚å¯Ÿåˆ†æç»“æœè§£æå¤±è´¥ï¼šæ— æ³•è§£æJSONæ ¼å¼"
            if stream_mode:
                for chunk in self._emit_processing(error_msg, "observation"):
                    yield ("processing", f'data: {json.dumps(chunk)}\n\n')
            yield ("observation", {"sufficient_info": True, "need_more_search": False})

    async def _answer_generation_phase(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ"""
        conversation_context = self._build_conversation_context(user_message, messages)
        collected_information = self._get_collected_info_text()
        
        answer_prompt = ANSWER_GENERATION_PROMPT.format(
            user_message=user_message,
            conversation_context=conversation_context,
            collected_information=collected_information
        )
        
        system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. å……åˆ†åˆ©ç”¨æ‰€æœ‰é€šè¿‡LightRAGæ£€ç´¢åˆ°çš„ä¿¡æ¯
2. æä¾›å‡†ç¡®ã€å…¨é¢ã€æœ‰é€»è¾‘çš„å›ç­”
3. ç¡®ä¿ç­”æ¡ˆç»“æ„æ¸…æ™°ï¼Œä¾¿äºç†è§£
4. å¦‚æœ‰ä¸ç¡®å®šæ€§ï¼Œè¯šå®æŒ‡å‡º
5. çªå‡ºæœ€é‡è¦çš„æ´å¯Ÿå’Œç»“è®º"""
        
        if stream_mode:
            for chunk in self._stream_openai_response(system_prompt, answer_prompt):
                chunk_data = {
                    'choices': [{
                        'delta': {'content': chunk},
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
            
            # æ·»åŠ tokenç»Ÿè®¡ä¿¡æ¯
            stats_text = self._get_token_stats_text()
            stats_chunk_data = {
                'choices': [{
                    'delta': {'content': stats_text},
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(stats_chunk_data)}\n\n"
        else:
            answer = self._call_openai_api(system_prompt, answer_prompt)
            stats_text = self._get_token_stats_text()
            yield answer + stats_text

    def _summarize_collected_info(self) -> str:
        """æ€»ç»“å·²æ”¶é›†çš„ä¿¡æ¯"""
        if not self.react_state['collected_information']:
            return "æš‚æ— æ”¶é›†ä¿¡æ¯"
        
        summary = f"å·²æ”¶é›† {len(self.react_state['collected_information'])} æ¡ä¿¡æ¯ï¼š"
        for i, info in enumerate(self.react_state['collected_information'], 1):
            key_points = ', '.join(info.get('key_information', []))[:100]
            summary += f"\n{i}. {key_points} (ç›¸å…³æ€§: {info.get('relevance_score', 0)}/10)"
        
        return summary

    def _get_collected_info_text(self) -> str:
        """è·å–æ”¶é›†ä¿¡æ¯çš„å®Œæ•´æ–‡æœ¬"""
        if not self.react_state['collected_information']:
            return "æš‚æ— æ”¶é›†ä¿¡æ¯"
        
        info_text = f"å…±æ”¶é›† {len(self.react_state['collected_information'])} æ¡æ£€ç´¢ä¿¡æ¯ï¼š\n\n"
        
        for i, info in enumerate(self.react_state['collected_information'], 1):
            info_text += f"=== ä¿¡æ¯æ¡ç›® {i} ===\n"
            info_text += f"æ£€ç´¢æŸ¥è¯¢ï¼š{info.get('query', 'æœªçŸ¥')}\n"
            info_text += f"æ£€ç´¢æ¨¡å¼ï¼š{info.get('mode', 'æœªçŸ¥')}\n"
            info_text += f"ç›¸å…³æ€§è¯„åˆ†ï¼š{info.get('relevance_score', 0)}/10\n"
            
            key_info = info.get('key_information', [])
            if key_info:
                info_text += f"å…³é”®ä¿¡æ¯ï¼š{', '.join(key_info)}\n"
            
            raw_result = info.get('raw_result', {})
            if isinstance(raw_result, dict) and 'response' in raw_result:
                response_text = raw_result['response']
                if len(response_text) > 800:
                    response_text = response_text[:800] + "..."
                info_text += f"æ£€ç´¢ç»“æœï¼š{response_text}\n"
            
            info_text += "\n"
        
        return info_text

    def _get_token_stats_text(self) -> str:
        """æ ¼å¼åŒ–tokenç»Ÿè®¡ä¿¡æ¯"""
        stats = self.token_stats
        stats_text = f"""

---

**ğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡**
- è¾“å…¥Tokenæ•°: {stats['input_tokens']:,} å­—ç¬¦
- è¾“å‡ºTokenæ•°: {stats['output_tokens']:,} å­—ç¬¦  
- æ€»Tokenæ•°: {stats['total_tokens']:,} å­—ç¬¦
- APIè°ƒç”¨æ¬¡æ•°: {stats['api_calls']} æ¬¡
- å¹³å‡æ¯æ¬¡è°ƒç”¨: {stats['total_tokens']//max(stats['api_calls'], 1):,} å­—ç¬¦

*æ³¨: Tokenæ•°é‡åŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼Œå®é™…ä½¿ç”¨é‡å¯èƒ½ç•¥æœ‰å·®å¼‚*
"""
        return stats_text

    async def _react_loop(self, user_message: str, messages: List[dict], stream_mode: bool) -> AsyncGenerator[str, None]:
        """ReActä¸»å¾ªç¯"""
        # é‡ç½®çŠ¶æ€
        self.react_state = {
            "collected_information": [],
            "query_history": [],
            "used_queries": set(),
            "current_iteration": 0,
            "search_modes_used": set(),
        }
        
        self.token_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0
        }
        
        # å¼€å§‹æµå¼å“åº”
        if stream_mode:
            yield f'data: {json.dumps({"choices": [{"delta": {}, "finish_reason": None}]})}\n\n'
        
        # 1. Reasoningé˜¶æ®µ
        initial_decision = None
        async for phase_result in self._reasoning_phase(user_message, messages, stream_mode):
            result_type, content = phase_result
            if result_type == "processing":
                yield content
            elif result_type == "decision":
                initial_decision = content
                break
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢
        if not initial_decision.get("need_search", False) or initial_decision.get("sufficient_info", False):
            # ç›´æ¥è¿›å…¥ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
            async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
                yield answer_chunk
            return
        
        # 2. Action-Observationå¾ªç¯
        max_iterations = self.valves.MAX_REACT_ITERATIONS
        current_query = initial_decision.get("search_query", "")
        search_mode = initial_decision.get("search_mode", self.valves.LIGHTRAG_DEFAULT_MODE)
        
        while self.react_state['current_iteration'] < max_iterations and current_query:
            self.react_state['current_iteration'] += 1
            
            # Actioné˜¶æ®µ
            search_result = None
            async for phase_result in self._action_phase(current_query, search_mode, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "result":
                    search_result = content
                    break
            
            if search_result is None:
                break
            
            # Observationé˜¶æ®µ
            observation = None
            async for phase_result in self._observation_phase(search_result, current_query, search_mode, user_message, stream_mode):
                result_type, content = phase_result
                if result_type == "processing":
                    yield content
                elif result_type == "observation":
                    observation = content
                    break
            
            # æ£€æŸ¥æ”¶é›†ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ
            collected_count = len(self.react_state['collected_information'])
            if collected_count >= self.valves.MIN_INFO_THRESHOLD:
                if stream_mode:
                    stop_content = f"\nâœ… å·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯({collected_count}æ¡ >= {self.valves.MIN_INFO_THRESHOLD}æ¡é˜ˆå€¼)ï¼Œåœæ­¢æœç´¢"
                    for chunk in self._emit_processing(stop_content, "observation"):
                        yield f'data: {json.dumps(chunk)}\n\n'
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢
            if not observation or not observation.get("need_more_search", False) or observation.get("sufficient_info", False):
                break
            
            # è·å–ä¸‹ä¸€è½®æŸ¥è¯¢
            current_query = observation.get("next_query", "")
            search_mode = observation.get("next_mode", search_mode)
            
            # é¿å…é‡å¤æŸ¥è¯¢
            if current_query and current_query.lower() in self.react_state['used_queries']:
                break
        
        # 3. ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ
        async for answer_chunk in self._answer_generation_phase(user_message, messages, stream_mode):
            yield answer_chunk

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Union[str, Generator, Iterator]:
        """ä¸»ç®¡é“å‡½æ•°"""
        if not user_message or not user_message.strip():
            yield "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜"
            return
        
        stream_mode = self.valves.ENABLE_STREAMING
        
        try:
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ReActå¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async_gen = self._react_loop(user_message, messages, stream_mode)
                
                while True:
                    try:
                        result = loop.run_until_complete(async_gen.__anext__())
                        yield result
                    except StopAsyncIteration:
                        break
                
                # æµå¼æ¨¡å¼ç»“æŸæ ‡è®°
                if stream_mode:
                    done_msg = {
                        'choices': [{
                            'delta': {},
                            'finish_reason': 'stop'
                        }]
                    }
                    yield f"data: {json.dumps(done_msg)}\n\n"
                    yield "data: [DONE]\n\n"
                    
            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ ReAct Pipelineæ‰§è¡Œé”™è¯¯: {str(e)}"
            if stream_mode:
                error_chunk = {
                    'choices': [{
                        'delta': {'content': error_msg},
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield error_msg
